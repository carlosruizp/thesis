% Appendix B

\chapter{Appendix for Chapter \ref{Chapter4}}
\label{AppendixB}
\lhead{Appendix \ref{AppendixB}. \emph{Appendix for Chapter \ref{Chapter4}}}

We provide, in this Appendix, additional material that complements the exposition of the PESMOC approach introduced in Chapter \ref{Chapter4}. Concretely, we show the exact computations done by the expectation propagation algorithm to approximate the intractable factors of the conditional predictive distribution involved in the PESMOC acquisition function approximation. We also include a sensitivity analysis of the number of Monte Carlo iterations of the Slice sampling algorithm. Additionaly, we include another sensitivity analysis of the sampled Pareto sets and an analysis of the infeasible solutions in benchmark experiments.


\section{Vector-Valued Reproducing Kernel Hilbert Spaces}%Single-Task Learning with \acrshort{mtl} Kernels}
To give different definitions of vector-valued kernels, we will follow the work of~\citet{MicchelliP05}.
Consider $\Yspace$ a Hilbert space with inner product $\ydotp{.}{.}$, and $\mathcal{L}(\Yspace)$ the space of linear operators from $\Yspace$ to $\Yspace$; then we can study the Hilbert spaces $\hilbertspace$ of functions
\begin{equation*}
    \begin{aligned}
        f: &  & \Xspace &  &  & \to &  &  &  & \Yspace &  & \\
           &  & x       &  &  & \to &  &  &  & f(x)    &  &
    \end{aligned}
\end{equation*}
with inner product $\dotp{\cdot}{\cdot}$.
The kernels in such spaces are operator-valued functions $K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)$.
We look at these spaces from three different and equivalent perspectives: continuous evaluation functionals, positive-definite kernels and feature maps.

%\paragraph*{Continuous evaluation functionals.}
The first approach considered is that of continuous evaluation functionals.
% Given a Hilbert space with continuous evaluation functionals, which are defined later, we can find the reproducing kernel operator.
Consider the vector-valued Hilbert space $\mathcal{H}$ with inner product $\dotp{\cdot}{\cdot}$ of functions defined in $\Xspace$ and values in $\Yspace$, and the functionals
%$L_{x, y} f = \ydotp{y}{f(x)}$,
\begin{equation*}
    \begin{aligned}
        L_{x, y}: &  & \hilbertspace &  & \to &  &  & \Yspace &  &  & \to &  &  & \reals          &  & \\
                  &  & f             &  & \to &  &  & f(x)    &  &  & \to &  &  & \ydotp{y}{f(x)} &  &
    \end{aligned}.
\end{equation*}
If these functionals are continuous, we can apply Riesz representation theorem~\citep{riesz2012functional}. That is, for every $x \in \Xspace, y \in \Yspace$ we can find an unique $g_{x, y} \in \hilbertspace$ such that for all $f \in \hilbertspace$,
\begin{equation}\label{eq:vector_riesz}
    L_{x, y} f = \ydotp{y}{f(x)} = \dotp{g_{x,y}}{f}_{\hilbertspace} .
\end{equation}
We can now give the definition of vector-valued Hilbert space from the point of view of continuous functionals as in~\citet[Definition 2.1]{MicchelliP05}.
\begin{definition}[vector-valued RKHS]
    We say that $\hilbertspace$ is a vector-valued RKHS when for any $x \in \Xspace$ and $y \in \Yspace$, the functional $L_{x, y} f = \ydotp{y}{f(x)}$ is continuous.
\end{definition}
Note that this is a definition similar to the scalar case but we use the inner product of $\Yspace$ to construct the scalar-valued functionals $L_{x, y}$. The price we pay is that it is necessary to express the Riesz representation as dependent of the elements $y \in \Yspace$. To get rid of this dependence, for every $x \in \Xspace$ we can define the linear operator
\begin{equation}
    \label{eq:vector_riesz}
    \begin{aligned}
        g_x: &  & \Yspace &  &  & \to &  &  &       & \hilbertspace &  & \\
             &  & y       &  &  & \to &  &  & g_x y & = g_{x, y}    &  &
    \end{aligned}.
\end{equation}
This operator is well defined because $g_{x, y}$ is unique for every $x \in \Xspace, y \in \Yspace$ and its linearity is easy to check from the linearity of the inner product $\ydotp{.}{.}$.
%
Using these results, we can now define the operator
\begin{equation}\label{eq:vector_kernel}
    \begin{aligned}
        K(x, \hat{x}) : &  & \Yspace &  &  & \to &  &  &                 & \Yspace               &  & \\
                        &  & y       &  &  & \to &  &  & K(x, \hat{x}) y & = (g_{\hat{x}} y) (x) &  &
    \end{aligned}
\end{equation}
for every $x, \hat{x} \in \Xspace$.
%Observe that $K(x, \hat{x})$ is linear since $\forall x,\;  g_x$ is linear, i.e. $g_x(\lambda_1 y_1 + \lambda_2 y_2) = \lambda_1 g_x(y_1) + \lambda_2 g_x(y_2)$.
It is possible then to prove that $K(x, \hat{x})$ is a reproducing kernel in $\mathcal{H}$ as seen in~\citet[Propositon 2.1]{MicchelliP05}.

\begin{proposition}
    If $K(x, \tilde{x})$ is defined for every $x, \hat{x} \in \Xspace$ as in ~\eqref{eq:vector_kernel} and $g_x$ is defined as in ~\eqref{eq:vector_riesz}, then
    \begin{equation}
        \nonumber
        K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)
    \end{equation}
    is a kernel that for every $x, \hat{x} \in \Xspace$ satisfies:
    \begin{enumerate}
        \item For every $y, \hat{y} \in \Yspace$, we have
              \begin{equation}
                  \nonumber
                  \ydotp{y}{K(x, \tilde{x}) \hat{y}} = \dotp{g_x \hat{y}}{g_{\hat{x}} y} .
              \end{equation}
        \item $K(x, \tilde{x}) = \adj{K(\tilde{x}, x)}$ and $K(x, x)$ is positive definite for every $x \in \Xspace$.
        \item Given $n \in \naturals$, for any  $x_1, \ldots, x_n \in \Xspace,  y_1, \ldots, y_n \in \Yspace$,
              \begin{equation}
                  \nonumber
                  \sum_{i, j =1}^n \ydotp{y_i}{K(x_i, x_j) y_j} \geq 0.
              \end{equation}
        \item $\norm{g_x} = \norm{K(x, x)}^{\frac{1}{2}}$.
        \item $\norm{K(x, t)} \leq \norm{K(x, x)}^{\frac{1}{2}} \norm{K(t, t)}^{\frac{1}{2}}$.
        \item For every $f \in \hypspace$ and $x \in \Xspace$, we have that
              $$ \norm{f(x)} \leq \norm{f} \norm{K(x, x)}^{\frac{1}{2}} .$$
    \end{enumerate}
\end{proposition}

% To do that, first we have to define a vector-valued kernel and the corresponding reproducing property.
% \begin{definition}[operator-valued Kernel]
%     Given a non-empty set $\Xspace$, if $\Yspace$ is a finite-dimensional Hilbert space
%     % , and $\hilbertspace$ is a Hilbert space of $\Yspace$-valued functions with domain in $\Xspace$
%      an operator-valued kernel is a function
%     \begin{equation}
%         \nonumber
%         K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)
%     \end{equation}
%     which is symmetric and positive definite.
% \end{definition}
% For the clarity of the text an operator-valued $K$ will be referred just as a kernel unless an explicit distinction is needed.
% \begin{definition}[Reproducing Property of operator-valued operators]
%     If $\Yspace$ is a finite-dimensional Hilbert space, and $\hilbertspace$ is a Hilbert space of $\Yspace$-valued functions with domain in $\Xspace$, a function
%     \begin{equation}
%         \nonumber
%         K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)
%     \end{equation}
%     has the reproducing property if $\forall x \in \Xspace, y \in \Yspace, \forall f \in \hilbertspace,$ 
%     $$ \ydotp{y}{f(x)} = \dotp{K(\cdot, x) y}{f}_\hilbertspace.$$ 
% \end{definition}
% A kernel with the reproducing property is also called a reproducing kernel. The next proposition shows how we can build a reproducing kernel for a space $\hilbertspace$ in which the evaluation functionals are continuous.
% \begin{proposition}
%     If for every $x, \hat{x} \in \Xspace$, the function $K(x, \hat{x})$ is defined as in Equation~\eqref{eq:vector_kernel}, then the function
%     \begin{equation}
%         \nonumber
%         K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)
%     \end{equation}
%     is a reproducing kernel.
% \end{proposition}
% \begin{proof}
%     To prove that $K$ is a reproducing kernel we need to check: 
%     \begin{enumerate}
%         \item\label{item:vvkernel_bounded} $K(x, \hat{x})$ is bounded for every $x, \hat{x} \in \Xspace$ so $K$ is well defined.
%         \item\label{item:vvkernel_symmetric} $K$ is symmetric: $\adj{K(x, \hat{x})} = K(\hat{x}, x)$, where $\adj{A} \in \mathcal{L}(\Yspace)$ is the adjoint of $A \in \mathcal{L}(\Yspace)$.
%         \item\label{item:vvkernel_positive}  $K$ is positive definite: given $n \in \naturals$, for any  $x_1, \ldots, x_n \in \Xspace,  y_1, \ldots, y_n \in \Yspace$,
%         \begin{equation}
%             \nonumber
%             \sum_{i, j =1}^n \ydotp{y_i}{K(x_i, x_j) y_j} \geq 0.
%         \end{equation}
%         \item\label{item:vvkernel_repr} It has the reproducing property: $\forall x \in \Xspace, y \in \Yspace, \forall f \in \hilbertspace,$ $ \ydotp{y}{f(x)} = \dotp{K(\cdot, x) y}{f}.$
%     \end{enumerate}
%     To prove~\ref{item:vvkernel_bounded} and~\ref{item:vvkernel_symmetric}, observe that by
%     applying~\eqref{eq:vector_riesz} to $f = g_{\hat{x}}\hat{y}$, for every $x \in \Xspace, y \in \Yspace$ there exists an unique $g_{x, y} = g_{x}y$ such that
%     \begin{equation}
%         \ydotp{y}{(g_{\hat{x}} \hat{y}) (x)} = \dotp{g_{x}y}{g_{\hat{x}}\hat{y}} ,
%     \end{equation}
%     and combining this result with~\eqref{eq:vector_kernel} we get
%     \begin{equation}
%         \nonumber
%         \ydotp{y}{K(x, \hat{x}) \hat{y}} = \ydotp{y}{(g_{\hat{x}} \hat{y})(x) }= \dotp{g_{x}y}{g_{\hat{x}}\hat{y}}.
%     \end{equation}
%     Also, using the definitions,
%     \begin{equation}
%         \nonumber
%         \ydotp{K(\hat{x}, x) y}{ \hat{y}} = \ydotp{(g_{x} y)(\hat{x})}{\hat{y}} = \ydotp{\hat{y}}{(g_{x} y)(\hat{x})} = \dotp{g_{\hat{x}} \hat{y}}{g_x y}.
%     \end{equation}
%     Since both operators $K(x, \hat{x}), K(\hat{x}, x)$ are bilinear, by the Uniform Boundness Principle~\citep{Akhiezer1961TheoryOL}, $K(x, \hat{x}), K(\hat{x}, x)$ are bounded (hence continuous) and $K(x, \hat{x})^* = K(\hat{x}, x)$.
%     \\
%     To prove~\ref{item:vvkernel_positive} we write
%     \begin{equation}
%         \nonumber
%         \sum_{i, j =1}^n \ydotp{y_i}{K(x_i, x_j) y_j} = \sum_{i, j =1}^n \dotp{g_{x_i} y_i}{g_{x_j} y_j}  = \norm{\sum_{i=1}^n g_{x_i} y_i}^2 \geq 0 .
%     \end{equation}
%     Finally, to prove~\ref{item:vvkernel_repr} we use that 
%     $\forall x \in \Xspace, y \in \Yspace, \forall f \in \hilbertspace, \exists g_{x, y} \in \hilbertspace$ such that 
%     $$ \ydotp{y}{f(x)} = \dotp{g_{x, y}}{f} = \dotp{g_{x} y}{f} =\dotp{K(\cdot, x) y}{f}.$$
% \end{proof}

%\paragraph*{Positive semi-definite kernels.}
The second approach of~\citet{MicchelliP05} to define vector-valued kernels changes the point of view. Given a kernel $K$, the Hilbert space from which $K$ is the reproducing kernel is built.
To do this, we use~\citet[Theorem 2.1]{MicchelliP05}, which extends the Moore-Aronszanj Theorem:
\begin{theorem}\label{th:moore-arons_vector}
    If $K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)$ is a kernel, then there exists a unique (up to an isometry) RKHS which admits $K$ as its reproducing kernel.
\end{theorem}
The proof is similar to that of the Moore-Aronszanj Theorem, considering the space of the completion of the span of $\left\lbrace K_x = K(\cdot, x) , x \in \Xspace \right\rbrace$.

%\paragraph*{Feature map.}
The last approach is based on feature maps, which provide a very simple way of generating kernels.
\begin{lemma}
    Given some Hilbert space $\mathcal{W}$, any continuous feature map $\Phi: \Xspace \to \mathcal{L}(\mathcal{W}, \mathcal{Y})$ defines a kernel as
    \begin{equation}
        \label{eq:kernel_featmap}
        K(x, \hat{x}) = \Phi(x) \circ \adj{\Phi(\hat{x})}: \Yspace \to \Yspace .
    \end{equation}
\end{lemma}
\begin{proof} We need to prove that it is bounded, symmetric and positive definite:
    \begin{enumerate}
        \item Since $\Phi(x)$ is continuous, its adjoint $\adj{\Phi(x)}$ is continuous and the composition $\Phi(x) \circ \adj{\Phi(\hat{x})}$ is also continuous.
        \item It is symmetric since  $\adj{(\Phi(x) \circ \adj{\Phi(\hat{x})})} = (\adj{(\adj{\Phi(x)})} \circ \adj{\Phi(\hat{x})}) = (\Phi(x) \circ \adj{\Phi(\hat{x})}) $.
        \item It is semipositive definite since, given any $x_1, \ldots, x_n \in \Xspace,  y_1, \ldots, y_n \in \Yspace$
              \begin{equation}
                  \nonumber
                  \begin{aligned}
                      \sum_{i, j =1}^n \ydotp{y_i}{K(x_i, x_j) y_j} & = \sum_{i, j =1}^n \ydotp{y_i}{ \Phi(x_i) \circ \adj{\Phi(x_j)} y_j}                                                      \\
                                                                    & = \sum_{i, j =1}^n \ydotp{\adj{\Phi(x_i)} y_i}{ \adj{\Phi(x_j)} y_j} = \norm{\sum_{i=1}^n \adj{\Phi(x_i)} y_i}^2 \geq 0 .
                  \end{aligned}
              \end{equation}
    \end{enumerate}
\end{proof}
% Since $K$ as defined in~\eqref{eq:kernel_featmap} is a kernel, according to Theorem~\ref{th:moore-arons_vector} we can find its corresponding vector-valued Hilbert space $\hilbertspace$.

%\subsubsection*{Representer Theorem for Operator-Valued Kernels}
These definitions related to vector-valued \acrshort{rkhss} are useful to get a better understanding of how to apply operator-valued kernels for learning multi-target or multi-task functions. Concretely, we would like to develop some result similar to representer theorem~\cite{ScholkopfHS01}, which is a crucial result in optimization and \acrshort{ml}. Given a regularized empirical risk, under some assumptions, the theorem gives a precise description of the minimizer $f^*$ as a finite linear combination of functions $K(\cdot, x_i)$ where $x_i$ are part of the empirical sample.
This result is extended in~\citet[Theorem 4.2]{MicchelliP05} for operator-valued kernels through the next theorem.
\begin{theorem}
    Let $\mathcal{Y}$ be a Hilbert space and let $\mathcal{H}$ be the Hilbert space of $\mathcal{Y}$-valued functions with an operator-valued reproducing kernel $K$. Let $V: \Yspace^n \times \reals_+ \to \reals$ be a function strictly increasing in its second variable and consider the problem of minimizing the functional
    \begin{equation}\label{eq:representer_functional}
        \begin{aligned}
            E(f) = V((f(x_1), \ldots, f(x_n)), \norm{f}^2)
        \end{aligned}
    \end{equation}
    in $\hilbertspace$.
    If
    % for every $(f_1, \ldots, f_n) \in \Yspace^n$ the function $h: \reals_+ \to \reals_+$ defined as $h(t) = V((f_1, \ldots, f_n)), t)$ is strictly increasing and
    $f_0$ minimizes $E$, then $f_0 = \sum_{j=1}^n K(\cdot, x_j) c_j$ where $c_j \in \mathcal{Y}$. In addition, if $V$ is strictly convex, the minimizer is unique.
\end{theorem}
% The proof can be found in~\citet{MicchelliP05}.

%\subsubsection*{Bijection between Scalar and Vector-Valued Kernels}
The scalar-valued kernels are well known and studied but this is not the case for operator-valued kernels. However, as shown in~\cite{Hein04kernels,BaldassarreRBV12}, we can find a bijection between operator-valued kernels and scalar-valued ones.
\begin{lemma}\label{lemma:kernel_bijection}
    Let $\mathcal{Y}$ be a finite-dimensional Hilbert space and
    $K: \Xspace \times \Xspace \to \mathcal{L}(\mathcal{Y})$
    be an operator-valued kernel. Consider also the scalar-valued kernel
    $L: (\Xspace, \mathcal{Y}) \times (\Xspace, \mathcal{Y}) \to \reals$
    such that $L((x, z), (\hat{x}, \hat{z})) = \ydotp{z}{K(x, \hat{x}) \hat{z}}$. Then the map $K \to L$ is a bijection.
\end{lemma}
Moreover, if we focus on finite dimensional Hilbert spaces, that is, isomorphic to $\reals^d$, given an operator-valued kernel $K$, the corresponding scalar-valued kernel $L$ is defined using the normal basis as
$$ L((x, e_r), (\hat{x}, e_s)) = \ydotp{e_r}{K(x, \hat{x}) e_s} = K(x, \hat{x})_{rs} ; \; r,s=1, \ldots, \dimx.$$
That is, each pair $(x, \hat{x})$ defines a matrix which contains the information of how the different outputs, or tasks, are related.

% \begin{figure}[t]
%     \centering
% \begin{tikzpicture}[node distance=1cm, auto,]
%     %nodes
%     \node[punkt] (market) {Market (b)};
%     % We make a dummy figure to make everything look nice.
%     \node[above=of market] (dummy) {};
%     \node[right=of dummy] (t) {Ultimate borrower}
%       edge[pil,bend left=45] (market.east) % edges are used to connect two nodes
%     \node[left=of dummy] (g) {Ultimate lender}
%       edge[pil, bend right=45] (market.west)
%       %edge[pil,<->, bend left=45] node[auto] {Direct (a)} (t);
% \end{tikzpicture}
% \caption{Bijections between kernel functions.}
% \end{figure}

% \begin{figure}[t]
%     \centering
% \begin{tikzpicture}[node distance=1cm, auto,]
%     %nodes
%     \node[] (w) {};
% \end{tikzpicture}
% \caption{Bijections between kernel functions.}
% \end{figure}

\section{Examples of MTL Kernels}

%\subsubsection*{Examples of \acrshort{mt} Kernels}
Using the framework for \acrshort{mtl} with kernel methods just discussed we can choose different regularizations, induced by the matrix $\mymat{E}$, which lead to different \acrshort{mt} approaches.

One trivial example that we can illustrate using this framework is that of independent tasks. This is the case when $E = I_{\ntasks}$ and therefore $B =  I_{\ntasks}$ , that is
$$B_r^\intercal =  (\overbrace{0}^1, \ldots, \overbrace{1}^{r}, \ldots, \overbrace{0}^T), $$
and the kernel is
\begin{equation}
    \nonumber
    \hat{k}(x_i^r, x_j^s) = \dotp{B_r}{B_s} k(x_i^r, x_j^s) = (\delta_{rs}) k(x_i^r, x_j^s).
\end{equation}
This approach is not a proper \acrshort{mtl} method because each task is learned separately, and no coupling is being enforced among tasks.

%\paragraph*{Independent Parts with Shared Common Model:}
Other important example that we can model using this framework is that of a combination-based \acrshort{mtl}, where we use the combination of a common part and task-specific parts. For this case, we select the matrix $B$ such that its columns are
$$B_r^\intercal =  \left(\overbrace{0}^1, \ldots, \overbrace{1}^{r}, \ldots,  \overbrace{0}^T, \overbrace{\frac{1}{\mu}}^{T+1} \right), $$
the corresponding multi-task kernel is:
\begin{equation}
    \nonumber
    \hat{k}(x_i^r, x_j^s) = \dotp{B_r}{B_s} k(x_i^r, x_j^s) = \left( \frac{1}{\mu} + \delta_{rs} \right) k(x_i^r, x_j^s) .
\end{equation}
% Evgeniou
This is equivalent to the approach presented in the work of~\cite{EvgeniouP04}, that we have presented in~\eqref{eq:svmmtl_primal_add}.
Here, for each task we use the vector
$$w_r = w + v_r,$$
to define the model of the $r-th$ task, where $w$ is common to all tasks and $v_r$ is task-specific.

% % Leveraging common and specific information
% Note that $\mu$ is a parameter that controls the tradeoff between the relevance of common and specific models. That is, when $\mu$ tends to infinite, the resulting model approaches a common-task standard \acrshort{svm}; when $\mu$ tends to zero, an independent task approach is taken, with one standard \acrshort{svm} problem for each task.


% This is also reflected in the corresponding dual problem
% \begin{equation}\label{eq:regmtlsvm_dual}
%     \begin{aligned}
%         & \argmin_{\alpha_i} 
%         & & \frac{1}{2} \sum_{r, s=1}^\ntasks \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s \dotp{x_i^r}{x_j^s} + \frac{1}{2 \mu} \sum_{r, s=1}^\ntasks  \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s \delta_{rs} \dotp{x_i^r}{x_j^s} \\
%         & & & \qquad - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} p_i^r \alpha_i^r \\
%         & \text{s.t.}
%         & & 0 \leq \alpha_i^r \leq C \\
%         & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
%         \end{aligned}
% \end{equation}
% In this dual form, as $\mu$ grows, the task-specific part goes to zero, and the most important term is the first one, corresponding to the common part. The opposite effect is obtained when $\mu$ shrinks.
% Common + specific model which is equivalent to penalizing individual norm and variance

Moreover, in~\cite{EvgeniouP04} it is shown that this is a problem equivalent to
\begin{equation}
    \nonumber
    \begin{aligned}
         & \argmin_{w, w_r, \xi_i^r}
         &                           & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \xi_i^r +  \frac{1}{2} \sum_{r=1}^\ntasks \norm{w_r}^2 + \frac{\mu}{2} \sum_{r=1}^\ntasks  \norm{w_r - \sum_{s=1}^\ntasks w_s}^2                                                      \\
         & \text{s.t.}
         &                           & y_{i}^r ( \dotp{w_r}{x_{i}^r}) \geq p_i^r - \xi_i^r ,                                                                                                                                                                               \\
         &                           &                                                                                                                                                                                & \xi_i^r \geq 0,                                    \\
         & \text{for }               &                                                                                                                                                                                & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
    \end{aligned}
\end{equation}
Now, only the $w_r$ variables are included, and it is clear that $\mu$ penalizes the variance of the $w_r$ vectors, so all models $w_r$ will tend to a common model as $\mu$ grows.

This is a very interesting approach because, as it was pointed out in~\cite{LiangC08}, this approach has connections with the \acrshort{svm}+ of the \acrshort{lupi} approach from~\cite{VapnikI15a}.


Finally, we can take the case where the tasks are considered as nodes in a graph, and the non-negative weights of the edges of this graph capture the relation between each pair of tasks.
Here, the matrix $\mymat{E}$ can be chosen as a Laplacian matrix $\mymat{L} = \mymat{D} - \mymat{A}$, where $\mymat{A}$ is the adjacency matrix indicating the weights of the edges between each pair of tasks, and $\mymat{D}$ is the degree matrix, a diagonal matrix where each diagonal term is the sum of the corresponding row of $\mymat{A}$. Observe that using this matrix, the regularization term is
\begin{align*}
    \myvec{u}^\intercal (L \otimes I) \myvec{u}
     & =\sum_{r=1}^T \sum_{s=1}^T u_r^\intercal L_{rs} u_s                                                                                                       \\
     & = \sum_{r=1}^T \sum_{s=1}^T u_r^\intercal (D - A)_{rs} u_s                                                                                                \\
     & = \sum_{r=1}^T  \sum_{s=1}^T u_r^\intercal \left(\delta_{rs} \sum_{q} A_{rq} \right) u_s - \sum_{r=1}^T  \sum_{s=1}^T u_r^\intercal A_{rs} u_s            \\
     & =  \sum_{r=1}^T u_r^\intercal \sum_{q} A_{rq} u_r + \sum_{s=1}^T u_s^\intercal \sum_{q} A_{sq} u_s  - \sum_{r=1}^T  \sum_{s=1}^T u_r^\intercal A_{rs} u_s \\
     & =  \sum_{r=1}^T  \sum_{s=1}^T u_r^\intercal \left( A_{rs} u_s + u_s^\intercal A_{rs} u_s - u_r^\intercal A_{rs} u_s \right)                               \\
     & = \sum_{r=1}^T \sum_{s=1}^T A_{rs} \norm{u_r - u_s}^2 \; .
\end{align*}
That is, the distance between task models is penalized, weighted by the degree of similatiy between the tasks as indicated by the graph.
