% Chapter 2

\chapter{Multi-Task Learning} % Write in your own chapter title
\label{Chapter2}
\lhead{Chapter \ref{Chapter2}. 
\emph{Gaussian Processes And Approximate Inference}} % Write in your own chapter title to set the page header

{\bf \small{
This chapter presents\dots
}}

\section{Introduction}
% What is MTL

% Examples and Motivation

% Some important references

\section{Why does Multi-Task Learning work?}
% First works in Learning to Learn
% Overview of section
 % Ruder and Caruana
% 
\subsection{Inductive Bias Learning Problem} % Baxter
% The first theoretical work on MTL... MTL as a Bias Learning Problem
Tipically in Machine Learning the goal is to find the best hypothesis $\hyp{x}{\param_0}$ from a space of hypothesis $\hypspace = \set{\hyp{x}{\param}, \param \in \paramspace}$, where $\paramspace$ is any set of parameters. This best candidate can be selected according to different inductive principles, which define a method of approximating a global function $f(x)$ from a training set:
$ \sample \defeq \set{(x_i, y_i),\; \idotsn} $
where $(x_i, y_i)$ are sampled from a distribution $\distf$.
%
In the classical statistics we find the Maximum Likelihood approach, where the goal is to estimate the density $\fun{x} = \cond{y}{x}$ and the hypothesis space is parametric, i.e. $\hypspace = \set{\hyp{x}{\param}, \param \in \paramspace \subset \reals^m}$. The learner select the parameter $\param$ that maximizes the probability of the data given the hypothesis.
%
Another more direct inductive principle is Empirical Risk Minimization (ERM), which is the most common one. In ERM the densities are ignored and an empirical error $\emprisk$ is minimized with the hope of minimizing the true expected error $\exprisk$, which would result in a good generalization. 
%
Several models use the ERM principle to generalize from data such as Neural Networks or Support Vector Machines. These methods are designed to find a good hypothesis $\hyp{x}{\alpha}$ from a given space $\hypspace$. The definition of such space $\hypspace$ define the bias for these problems. If $\mathcal{H}$ does not contain any good hypothesis, the learner will not be able to learn.
%

The best hypothesis space we can provide is the one containing only the optimal hypothesis, but this is the original problem that we want to solve. Therefore, in the single task scenario, there is no difference between bias learning and ordinary learning.
Instead, we focus on the situation where we want to solve multiple related tasks. In that case, we can obtain a good space $\hypspace$ that contains good solutions for the different tasks.
%
In~\cite{baxter2000model} an effort is made to define the concepts needed to construct the theory about inductive bias learning, which can be seen as a generalization of strict multi-task learning. This is done by defining an environment of tasks and extending the work of~\cite{vapnik2013nature}, which defines the capacity of space of hypothesis, Baxter defines the capacity of a family of spaces of hypothesis.

% Review of concepts for STL in Supervised Learning
Before presenting the concepts defined for Bias Learning, and to establish an analogy to those of ordinary learning, we briefly review some statistical learning concepts.
\subsubsection*{Ordinary Learning}
In the ordinary statistical learning, some theoretical concepts are used:
\begin{itemize}
    \item an \emph{input space} $\Xspace$ and an \emph{output space} $\Yspace$,
    \item a \emph{probability distribution} $\distf$, which is unknown, defined over $\Xspace \times \Yspace$,
    \item a \emph{loss function} $\loss{\cdot}{\cdot}:\Yspace \times \Yspace \to \reals$, and
    \item a \emph{hypothesis space} $\hypspace = \set{\hyp{x}{\param}, \param \in \paramspace \subset \reals^m}$ with hypothesis $\hyp{\cdot}{\param}: \Xspace \to \Yspace$.
\end{itemize}
The goal for the learner is to select a hypothesis $\hyp{x}{\alpha} \in \mathcal{H}$, or equivalently $\param \in \paramspace$, that minimizes the expected risk
$$ \exprisk(\param) =  \int_{\Xspace \times \Yspace} \loss{\hyp{x}{\alpha}}{y} d\distf(x, y) .$$
The distribution $\distf$ is unknown, but we have a training set $\sample = \{(x_1, y_1), \ldots, (x_\nsamples, y_\nsamples)\}$ of samples drawn from $\distf$. 
The approach is then is to apply the ERM inductive principle, that is to minimize the empirical risk
$$ \emprisk(\param) = \frac{1}{\nsamples} \sum_{i=1}^\nsamples l(h(x_i), y_i).$$
Thus, a learner $\mathcal{A}$ maps the set of training samples to a set of hypothesis:
\begin{equation}
    \nonumber
    \mathcal{A} : \bigcup {(\Xspace \times \Yspace)^\nsamples} \to \hypspace.
\end{equation}
Although $\emprisk(\param)$ is an unbiased estimator of $\exprisk(\param)$, it has been shown~\cite{vapnik2013nature} that this approach, despite being the most evident one, is not the best principle that can be followed.
This has relation with two facts: the first one is that the unbiased property is an asymptotical one, the second one has to do with overfitting.
Vapnik answers to the question of what can be said about $\exprisk$ when $\param$ minimizes $\emprisk(\param)$, and moreover, his results are valid also for small number of training samples $n$.
More specifically, Vapnik sets the sufficient and necessary conditions for the consistency of an inductive learning process, i.e. for $\emprisk(\param) \toprob \exprisk(\param) $ uniformly. Vapnik also defines the capacity of a hypothesis space and use it to derive bounds on the rate of this convergence for any $\param \in \paramspace$ and, more importantly, bounds on the difference $\inf_{\param \in \paramspace} \emprisk(\param) - \inf_{\param \in \paramspace} \exprisk(\param)$.
Under some general conditions, he proves that
\begin{equation}\label{eq:ordinary_generalization_bound}
    \inf_{\param \in \paramspace} \emprisk(\param) - \inf_{\param \in \paramspace} \exprisk(\param) \leq B(\nsamples/\vcdim{\hypspace})
\end{equation}
where $B$ is some non-decreasing function and $\vcdim{\hypspace}$ is the capacity of the space $\hypspace$, also named the VC-dimension $\hypspace$. This means that the generalization ability of a learning process can be controlled in terms of two factors:
\begin{itemize}
    \item The number of training samples $\nsamples$. A greater number of training samples assures a better generalization of the learning process.This looks intuitive and could be already inferred from the asymptotical properties. 
    \item The VC-dimension $\vcdim{\hypspace}$ of the hypothesis space $\hypspace$, which is desirable to be small. This term is not intuitive and is the most important term in Vapnik theory.
\end{itemize}
The VC-dimension measures the capacity of a set of hypothesis $\hypspace$. 
%In the case of a set of indicator functions, it is the maximum number of vectors $x_1, \ldots, x_\vcdim{\hypspace}$ that can be shattered (in two classes) by functions of this set. In the case of real functions, it is defined as the VC-dimension of the following set of indicator functions $ I(x, \alpha, \beta) = \myvec{1}_{\{\hyp{x}{\param} - \beta\}} $.
If the capacity of the set $\hypspace$ is too large, we may find a
hypothesis $\hyp{x}{\opt{\param}}$ that minimizes $\emprisk$ but does not 
generalize well and therefore, does not minimize $\exprisk$. This is the 
overfitting problem. 
On the other side, if we use a simple $\hypspace$, 
with low capacity, we could be in a situation where there is not a good hypothesis $\hyp{x}{\alpha} \in \hypspace$, so the empirical risk $\inf_{\param \in \paramspace} \exprisk$ is too large. This is the underfitting problem.

% The Structural Risk Minimization (SRM) as an inductive principle, proposed by Vapnik (as opposed to the ERM), tries to find a tradeoff between minimizing $\emprisk$ and minimizing $\vcdim{\hypspace}$. The idea is to define an admissible structure, that is a sequence of hypothesis spaces:
% $$ \mathcal{H}_1 \subset \mathcal{H}_2 \subset \ldots \subset \mathcal{H}_k \subset \ldots $$ 
% where their VC-dimensions are ordered:
% $$ d_1 < d_2 < \ldots < d_k < \ldots$$
% where $d_i$ is the VC-dimension of $\mathcal{H}_i$.
% SRM selects the hypothesis $\hyp{x}{\alpha^*} = \hyp{x}{\alpha_i^*} \in \hypspace_i$ that obtains the best bound for the actual risk $\exprisk$.
% This admissible structure can be built in various ways. In Neural Networks in can be the built by increasing the number of hidden layers. In other methods, such as SVM or Ridge Regression, this is done by decreasing the regularization.
% However, this SRM principle is usually replaced by a cross-validation (CV) procedure.

% Support Vector Machines, which are the most representative models of this theory, use the VC-dimension also in other way (apart from the SRM principle or the CV procedure). The goal of finding the optimal hyperplane, that is, that with the maximum margin between the classes, has its motivation in the fact the set of such type of hypothesis have a lower VC-dimension that the set of all hyperplanes do.


% Extension to MTL
\subsubsection*{Bias Learning: Concept and Components}
In ~\cite{baxter2000model} two main concepts are presented: the \emph{family of hypothesis spaces} and an \emph{environment} of related tasks. 
For simplicity we write $\hypf(x)$  instead of $\hypf(x, \param)$, and since $\param$ completely defines $\hypf$, we also substitute $\param$ by $\hypf$ for an easier notation.
Using these concepts, the bias learning problem has the following components:
\begin{itemize}
    \item an \emph{input space} $\Xspace$ and an \emph{output space} $\Yspace$,
    \item an \emph{environment} $(\bprobspace, \bdistf)$ where $\bprobspace$ is a set of distributions $P$ defined over $\Xspace \times \Yspace$, and we can sample from $\bprobspace$ according to a distribution $\bdistf$,
    \item a \emph{loss function} $\loss{\cdot}{\cdot}:\Yspace \times \Yspace \to \reals$, and
    \item a \emph{family of hypothesis spaces} $\hypspacef = \set{\hypspace_\delta, \delta \in \Delta}$, where each element $\hypspace_\delta$ is a set of hypothesis.
\end{itemize}
Analogous to ordinary learning, the goal is to minimize the expected risk, defined as
\begin{equation}\label{eq:biaslearn_exprisk}
    \bexprisk(\delta) = \int_{\bprobspace} \inf_{\hypf \in \hypspace_\delta} \risk_P(\hypf) d\bdistf(P) = \int_{\bprobspace} \inf_{\hypf \in \hypspace_\delta} \int_{\Xspace \times Y} l(h(x), y) dP(x, y) d\bdistf(P).
\end{equation}
Again, we do not know $\bprobspace$ nor $\bdistf$, but we have a training set samples from the environment $(\bprobspace, \bdistf)$ obtained in the following way:
\begin{enumerate}
    \item Sample $T$ times from $\bdistf$ obtaining $P_1, \ldots, P_T \in \bprobspace$
    \item For $r=1, \ldots, T$ sample $m$ pairs $z_r = \{(x_1^r, y_1^r), \ldots, (x_m^r, y_m^r)\}$ according to $P_r$ where $(x_i^r, y_i^r) \in X \times Y$ .
\end{enumerate}
We obtain a sample $z=\{(x_i^r, y_i^r), r=1,\; i=1, \ldots, \;m=1, \ldots, T\}$, with $\npertask$ examples from $\ntasks$ different learning tasks, and
\begin{equation}
    \nonumber
    \bsample \defeq 
    \begin{matrix}
        (x_1^1, y_1^1) & \ldots & (x_m^1, y_m^1) \\
        \vdots & \ddots & \vdots \\
        (x_1^\ntasks, y_1^\ntasks) & \ldots & (x_m^\ntasks, y_m^\ntasks) \\
    \end{matrix}
\end{equation}
is named as a $(\ntasks, \npertask)$-sample.
Using $\bsample$ we can define the empirical loss as
\begin{equation}\label{eq:biaslearn_emprisk}
    \bemprisk(\delta) = \sum_{r=1}^\ntasks \inf_{\hypf \in \hypspace_\delta} \hat{\risk}_{\sample_r}(\hypf) = \sum_{r=1}^\ntasks \inf_{\hypf \in \hypspace_\delta} \sum_{i=1}^m l(\hypf(x_i^r), y_i^r),
\end{equation}
which is an average of the empirical losses of each task. 
Note, however, that in the case of the bias learner, this estimate is biased, since $\risk_{P_r}(\hypf)$ does not coincide with $\hat{\risk}_{z_r}(\hypf)$. 
Putting all together, a bias learner $\mathcal{A}$ maps the set of all $(\ntasks, \npertask)$-samples to a family of hypothesis spaces:
\begin{equation}
    \nonumber
    \mathcal{A} : \bigcup {(\Xspace \times \Yspace)^{(\ntasks, \npertask)}} \to \hypspacef.
\end{equation}
%

To follow an analogous path to that of ordinary learning, the milestones in bias learning theory should include:
\begin{itemize}
    \item Checking the consistency of the Bias Learning methods, i.e. proving that $\bemprisk(\delta)$ converges uniformly in probability to $\bexprisk(\delta)$.
    \item Defining a notion of capacity of hypothesis space families $\hypspacef$.
    \item Finding a bound of $\bemprisk(\delta) - \bexprisk(\delta)$ for any $\delta$ using the capacity of the hypothesis space family. If possible, finding also a bound for $\inf_{\delta \in \Delta}\bemprisk(\delta) - \inf_{\delta \in \Delta} \bexprisk(\delta)$.
\end{itemize}
To achieve these goals some previous definitions are needed. From this point, since any $\hypspace$ is defined by a $\delta \in \Delta$, we omit $\delta$ and write just $\hypspace$ for simplicity.
%
\subsubsection*{Bias Learning: Capacities and Uniform Convergence}
% Pseudo-metrics, Covering numbers and Capacities
In first place, a \emph{sample-driven} pseudo-metric of $(\ntasks, 1)$-empirical risks is defined.
Consider a sequence of $\ntasks$ probabilities $\bprobseq = (P_1, \ldots, P_\ntasks)$ sampled from $\bprobspace$ according the the distribution $\bdistf$. 
Consider also the set of sequences of $\ntasks$ hypothesis 
$$\hypspace^\ntasks \defeq \set{ \myvec{\hypf} = (\hypf_1, \ldots, \hypf_\ntasks) , \hypf_1, \ldots, \hypf_\ntasks \in \hypspace} .$$
We can define then the set of $(\ntasks, 1)$-empirical risks as 
$$\hypspace^\ntasks_\lossf \defeq \set{ \myvec{\hypf}_\lossf(x_1, y_1, \ldots, x_\ntasks, y_\ntasks) = \sum_{r=1}^\ntasks \lossf(\hypf(x_i), y_i) , \hypf_1, \ldots, \hypf_\ntasks \in \hypspace} $$
The family of the set of $\ntasks$-risks of hypothesis is then $\bsetsample = \bigcup_{\hypspace \in \hypspacef} \hypspace^\ntasks$. Now we can define
\begin{equation}
    \nonumber
    \begin{aligned}
        \dist{\bprobseq}(\myvec{\hypf}_\lossf, \myvec{\hypf'}_\lossf) = \int_{(\Xspace \times \Yspace)^{\ntasks}} &\abs{\myvec{\hypf}_\lossf(x_1, y_1, \ldots, x_\ntasks, y_\ntasks) - \myvec{\hypf'}_\lossf(x_1, y_1, \ldots, x_\ntasks, y_\ntasks)} \\ 
        & d{P_1}(x_1, y_1) \ldots d{P_\ntasks}(x_\ntasks, y_\ntasks)
    \end{aligned}
\end{equation}
for $\myvec{\hypf}_\lossf, \myvec{\hypf'}_\lossf \in \hypspace_\lossf, \hypspace'_\lossf$ as a pseudo-metric in $\hypspacef^T$.
%

Then, a \emph{distribution-driven} pseudo-metric is defined. Given a distribution $P$ on $\Xspace \times \Yspace$. Consider the set of infimum expected risk for each $\hypspace$:
\begin{equation}
    \nonumber
    \hypspace^* \defeq \inf_{\hypf \in \hypspace} \risk_P(\hypf).
\end{equation}
The family of such sets is defined as 
$\bsetdist = \set{\hypspace^*, \hypspace \in \hypspacef}$.
The pseudo-metric in this space is given by $Q$:
\begin{equation}
    \nonumber
    \dist{\bdistf} = \int_\bprobspace \abs{\hypspace_1^* - \hypspace_2^*} d\bdistf
\end{equation}
With these two pseudo-metrics, two capacities for families of hypothesis spaces are defined. For that the definition of $\epsilon$-cover is needed. Given a pseudo-metric $\dist{S}$ in a space $\mathcal{S}$, 
a set of $l$ elements $s_1, \ldots, s_l \in \mathcal{S}$ is an $\epsilon$-cover of $\mathcal{S}$ if 
$ \dist{S}(s, s_i) \leq \epsilon $
for some $i=1, \ldots, l$.  Let $\mathcal{N}(\epsilon, \mathcal{S}, \dist{S})$ denote the size of the smallest $\epsilon$-cover.
Then, we can define the following capacities of a family space $\hypspacef$:
\begin{itemize}
    \item The \emph{sample-driven capacity} $\capacity{\epsilon}{\bsetsample} \defeq \sup_{\bprobseq} \mathcal{N}(\epsilon, \hypspacef^T, \dist{\bprobseq})$.
    \item The \emph{distribution-driven capacity} $\capacity{\epsilon}{\bsetdist} \defeq \sup_Q \mathcal{N}(\epsilon, \bsetdist, \dist{\bdistf})$.
\end{itemize}

% Uniform Convergence for bias learners (Comparison with Vapnik)
Using these capacities, the convergence (uniformly over all $\hypspace \in \hypspacef$) of bias learners can be proved~\cite[Theorem~2]{baxter2000model}. Moreover, the bias expected risk is bounded
\begin{equation}
    \nonumber
    \bemprisk(\hypspace) \leq \bexprisk(\hypspace) + \epsilon
\end{equation}
with probability $1 - \delta$, given sufficiently large $\ntasks$ and $\npertask$, 
\begin{equation}
    \nonumber
    \ntasks \geq \max \left( \frac{256}{\ntasks \epsilon^2} \log\frac{8\capacity{\frac{\epsilon}{32}}{\bsetdist}}{\delta} , \frac{64}{\epsilon^2}\right)  , \; \npertask \geq \max \left( \frac{256}{\ntasks \epsilon^2} \log\frac{8\capacity{\frac{\epsilon}{32}}{\bsetsample}}{\delta} , \frac{64}{\epsilon^2}\right) .
\end{equation}
It should be noted that the bound for $\npertask$ is inversely proportional to $\ntasks$, that is, the more tasks we have, the less samples we need for each task. 


% Multi-Task Learning (strictly speaking, with fixed tasks)
\subsubsection*{Multi-Task Learning}
The previous result is a result for pure Bias Learning, where we have an $(\bsetdist, \bdistf)$-environment of tasks. In Multi-Task Learning, we have a fixed number of tasks $\ntasks$ and a fixed sequence of distributions $\bprobseq = (P_1, \ldots, P_\ntasks)$, where $P_i$ is a distribution over $(\Xspace \times \Yspace)^\npertask$. The goal is not learning a hypothesis space $\hypspace$ but a sequence of hypothesis $\myvec{\hypf} = (\hypf_1, \ldots, \hypf_\ntasks), \hypf_1, \ldots, \hypf_\ntasks \in \hypspace $. Thus, the Multi-Task expected risk is
\begin{equation}\label{eq:mtlearn_exprisk}
    \risk_{\bprobseq}(\myvec{\hypf}) = \sum_{r=1}^\ntasks \risk_{P_r}(\hypf_r)  = \sum_{r=1}^\ntasks \int_{\Xspace \times Y} l(\hypf_r(x), y) d{P_r}(x, y),
\end{equation}
and the empirical risk is defined as
\begin{equation}\label{eq:mtlearn_emprisk}
    \bemprisk(\myvec{\hypf}) = \sum_{r=1}^\ntasks \hat{\risk}_{\sample_r}(\hypf_r) = \sum_{r=1}^\ntasks \sum_{i=1}^m l(\hypf_r(x_i^r), y_i^r).
\end{equation}
A similar result to that of Bias Learning is given for Multi-Task Learning~\cite[Theorem~4]{baxter2000model}:
\begin{equation}
    \nonumber
    \bemprisk(\myvec{\hypf}) \leq \risk_{\bprobseq}(\myvec{\hypf}) + \epsilon,
\end{equation}
with probability $1 - \delta$ given that the number of samples per task
\begin{equation}
    \nonumber
    \npertask \geq \max \left( \frac{64}{\ntasks \epsilon^2} \log\frac{4\capacity{\frac{\epsilon}{16}}{\bsetsample}}{\delta} , \frac{16}{\epsilon^2}\right).
\end{equation}
Observe that we do not need the \emph{distribution-driven} capacity in this case, just the \emph{sample-driven} capacity.
% Feature Learning
\subsubsection*{Feature Learning}
Feature Learning is a common way to encode bias. The most popular example are Neural Networks, where all the hidden layers can be seen as a Feature Learning engine that learns a mapping from the original space to a space with ''strong'' features.
In general, a set of ''strong'' feature maps is defined as $\mathcal{F} = \set{f, f: \Xspace \to \Fspace}$. Using these features, functions $g \in \mathcal{G}$ (which are tipically simple) are built:
$\Xspace \to_f \Fspace \to_g \Yspace$.
Thus, for each map $f$, the hypothesis space can be expressed as 
$\hypspace_f = \set{h = \mathcal{G} \circ f, g \in \mathcal{G}}$
, and the family of hypothesis spaces is 
$\hypspacef = \set{\hypspace_f, f \in \mathcal{F}}$.
Now, the Bias Learning problem is the problem of finding a good mapping $f$.
It is proved~\cite[Theorem~6]{baxter2000model} that in the Feature Learning case the capacities of $\hypspacef$ can be bounded by the capacities of $\mathcal{F}$ and $\mathcal{G}$ as
\begin{align*}
    \capacity{\epsilon}{\bsetsample} &\leq \capacity{\epsilon_1}{\mathcal{G}^\ntasks}^T \capf_{\mathcal{G}_\lossf}(\epsilon_2, \mathcal{F}) , \\
    \capacity{\epsilon}{\bsetdist} &\leq \capf_{\mathcal{G}_\lossf}(\epsilon, \mathcal{F})
\end{align*}
with $\epsilon = \epsilon_1 + \epsilon_2 $. Here, $\capf_{\mathcal{G}_\lossf}(\epsilon, \mathcal{F})$ is defined as 
$\capf_{\mathcal{G}_\lossf}(\epsilon, \mathcal{F}) \defeq \sup_P \mathcal{N}(\epsilon, \mathcal{F}, \dist{[P, \mathcal{G}_\lossf]})$, where
$$ \dist{[P, \mathcal{G}_\lossf]}(f, f') = \int_{\Xspace \times \Yspace} sup_{g \in \mathcal{G}} \abs{\loss{g \circ f(x)}{y} - \loss{g \circ f'(x)}{y}} dP(x, y)$$
is a pseudo-metric. Using these results alongside those presented for Bias Learning is useful to establish bounds for Feature Learning models like Neural Networks.

% Generalized VC-dim , Theorems 12, 13 and Corollary 13
\subsubsection*{Generalized VC-Dimension for Multi-Task Learning}
The concepts presented until now rely on the concepts of two capacities of a family of hypothesis spaces $\hypspacef$ to establish bounds in the difference $\bemprisk(\myvec{\hypf}) - \bexprisk(\myvec{\hypf})$, that is, the probability of large deviations between the empirical and expected risks for a given hypothesis sequence. However, it would be more interesting to establish some bounds between the empirical error and the \emph{best expected error}.
To overcome this limitations, a generalized VC-dimension is developed in~\cite{baxter2000model} for Multi-Task Learning with Boolean hypothesis.
%

Let $\hypspace$ be a space of boolean functions and $\hypspacef$ a boolean hypothesis space family. Denote the set of $\ntasks \times \npertask$ matrices in $\Xspace$ as $\Xspace^{\ntasks \times \npertask}$
For each $\mymat{x} \in \Xspace^{\ntasks \times \npertask}$ and each $\hypspace \in \hypspacef$ define the set of binary $\ntasks \times \npertask$ matrices
\begin{equation}
    \nonumber
    \hypspace_{\vert \mymat{x}} \defeq \set{\begin{pmatrix}
        \hypfun{x_1^1} & \ldots & \hypfun{x_\npertask^1} \\
        \vdots & \ddots & \vdots \\
        \hypfun{x_1^\ntasks} & \ldots & \hypfun{x_\npertask^\ntasks} \\
    \end{pmatrix}, \hypf \in \hypspace} ,
\end{equation}
and the corresponding family of such sets as
\begin{equation}
    \nonumber
    \hypspacef_{\vert \mymat{x}} = \bigcup_{\hypspace \in \hypspacef}  \hypspace_{\vert \mymat{x}}.
\end{equation}
For each $\ntasks, \npertask \geq 0$ define the number of binary matrices obtainable with $\hypspacef$ as
\begin{equation}
    \nonumber
    \Pi_\hypspacef(\ntasks, \npertask) \defeq \max_{\mymat{x} \in \Xspace^{\ntasks \times \npertask}} \abs{ \hypspacef_{\vert \mymat{x}} }.
\end{equation}
Note that $\Pi_\hypspacef(\ntasks, \npertask) \leq 2^{\ntasks \npertask}$ and if $\Pi_\hypspacef(\ntasks, \npertask) \leq 2^{\ntasks \npertask}$ we say that $\hypspacef$ shatters $\Xspace^{\ntasks \times \npertask}$.
For each $n > 0$ define
\begin{align}
    d_\hypspacef(\ntasks) &\defeq \max_{m: \Pi_\hypspacef(\ntasks, \npertask) = 2^{\ntasks \npertask}} m , \nonumber \\
    \overline{d}(\hypspacef) &\defeq \vcdim{\hypspacef^1} = \vcdim{\bigcup_{\hypspace \in \hypspacef} \hypspace}  ,  \nonumber \\
    \underline{d}(\hypspacef) &\defeq \max_{\hypspace \in \hypspacef} \vcdim{\hypspace}  . \nonumber
\end{align}
Here, $d_\hypspacef(\ntasks)$ is the generalized VC-dimension, and
\begin{equation}
    \nonumber
    d_\hypspacef(\ntasks) \geq \max\left(\floor*{\frac{\overline{d}(\hypspacef)}{\ntasks}}, \underline{d}(\hypspacef) \right)
\end{equation}
where is trivial to see that $\overline{d}(\hypspacef) \geq  d_\hypspacef(\ntasks)\geq \underline{d}(\hypspacef)$. 
% Look at Ben-David version
Now we can present the relevant result expressed in~\cite[Corollary~13]{baxter2000model}.
\begin{theorem}\label{th:baxter_vcdim}
    Given a sequence $\bprobseq = (P_1, \ldots, P_\ntasks)$ on $(\Xspace \times \set{0, 1})^\ntasks$, and a sample $\bsample$ from this distribution. Consider also a sequence $\myvec{\hypf} = (\hypf_1, \ldots, \hypf_\ntasks)$ of boolean hypothesis $\hypf_i \in \hypspace$, then
\begin{equation}
    \nonumber
    \bexprisk(\myvec{\hypf}) \leq \bemprisk(\myvec{\hypf}) + \epsilon,
\end{equation}
with probability $1 - \delta$ given that the number of samples per task
\begin{equation}
    \label{eq:bound_npertask_genvcdim}
    \npertask \geq \frac{88}{\epsilon^2} \left[2 d_\hypspacef(\ntasks) \log \frac{22}{\epsilon} + \frac{1}{\ntasks}\log\frac{4}{\delta} \right] .
\end{equation}
\end{theorem}
Here, since $d_\hypspacef(\ntasks) \geq d_\hypspacef(\ntasks+1)$, it is easy to see that as the number of task $\ntasks$ increases, the number of examples needed per task can decrease. 
Moreover, as shown in~\cite[Theorem~14]{baxter2000model}, if this bound on $\npertask$ is not fulfilled, then we can always find a sequence of distributions $\bprobseq$ such that
\begin{equation}
    \nonumber
    \inf_{\hypf \in \hypspace} \bemprisk(\myvec{\hypf}) > \inf_{\hypf \in \hypspace} \risk_{\bprobseq}(\myvec{\hypf}) + \epsilon .
\end{equation}
With this results we can see that the condition~\eqref{eq:bound_npertask_genvcdim} has some important properties:
\begin{itemize}
    \item It is a computable bound, given that we know how to compute $d_\hypspacef(\ntasks)$.
    \item It provides a sufficient condition for the uniform convergence (in probability) of the empirical risk to the expected risk.
    \item It provides a necessary condition for the consistency of Multi-Task Learners, i.e. uniform convergence of the best empirical risk to the best expected risk.
\end{itemize}

\subsubsection*{Conclusion}
In~\cite{baxter2000model} several new concepts are developed. The $(\bprobspace, \bdistf)$-environment of tasks is useful to characterize the concept of related tasks. Moreover, using this definition, Baxter is able to give some important results of uniform convergence in the Bias Learning paradigm. From this general view, Multi-Task Learning
is a particular case and the uniform convergence results are also valid. The Feature Learning approach, which can be seen as a more particular method of Multi-Task Learning has some interesting results splitting the analysis into the feature learning process and the construction of models over these features. Finally, the most important result is the definition of a generalized VC-dimension and the uniform convergence of Multi-Task Learning models using this concept. Although this is a result only valid for boolean hypothesis, it helps to shed some light on Multi-Task Learning and the reasons of its effectiveness.

\subsection{Learning with Related Tasks} % Ben-David
% Baxter gives the foundation for a theoretical work of a framework where the tasks share a common learning bias...
Using the work of~\cite{baxter2000model} as the foundation, several important notions and results are presented in~\cite{Ben-DavidB08} for boolean hypothesis functions defined over $\Xspace \times \set{0, 1}$.
% task relatedness
One of the main contributions of this work is a notion of task relatedness. In~\cite{baxter2000model} the tasks are related by sharing a common inductive bias that can be learned. In~\cite{Ben-DavidB08} a precise mathematical definition for task relatedness is given.
% task individual risks
The other important contribution is the focus on the individual risk of each task. In~\cite{baxter2000model} all the results are given for the Multi-Task empirical and expected risks, which are an average of the risks of each task. However, bounding this average does not bound the risk of each particular task. This is specially relevant if we are in a Transfer Learning scenario, where there is a target task that we want to solve and the remaining tasks can be seen as an aid to improve the performance in the target one.

% F-related tasks
\subsubsection*{A Notion of Task Relatedness: $\frelset$-Related Tasks}
The main concept for the theory developed in~\cite{Ben-DavidB08} is a set of $\frelset$ of transformations $\frelf: \Xspace \to \Xspace$. Given a probability distribution $\distf$ over $\Xspace \times \set{0, 1}$, a set of tasks with distributions $P_1, \ldots, P_\ntasks$ are $\frelset$-related if, for each task there exists some $\frelf_i \in \frelset$ such that $P_i = \frelf_i(\distf)$.

\begin{definition}[$\frelset$-related task]
    Consider a measurable space $(\Xspace, \mathcal{A})$ and the corresponding measurable product space $(\Xspace \times \set{0, 1}, \mathcal{A} \times \powerset{\set{0, 1}})$. Consider $P$ a probability distribution over this product space and a function $\frelf: \Xspace \to \Xspace$, then we define the distribution $\frel{P}$ such that for any $S \in \mathcal{A}$,
    $$ \frel{P}(S) \defeq P(\set{(f(x), b), (x, b) \in S }).$$
    %
    Let $\frelset$ be a set of transformations $\frelf: \Xspace \to \Xspace$, and let $P_1, P_2$ be distributions over $(\Xspace \times \set{0, 1}, \mathcal{A} \times \powerset{\set{0, 1}})$, then the distributions $P_1, P_2$ are $\frelset$-related if $\frel{P_1}= P_2$ or $\frel{P_2} = P_1$ for some $\frelf \in \frelset$.
    % \begin{itemize}
    %     \item The distributions $P_1, P_2$ are $\frelset$-related if $\frel{P_1}= P_2$ or $\frel{P_2} = P_1$ for some $\frelf \in \frelset$.
    %     \item Two samples $z_1, z_2$ sampled from $P_1, P_2$ respectively are $\frelset$-related if $P_1, P_2$ are $\frelset$-related.
    % \end{itemize}
\end{definition}
This notion establishes a clear definition of related tasks but we are interested in how a learner can use this relatedness to improve the learning process.
For that, considering that $\frelset$ is a group under function composition, we regard at the action of the group $\frelset$ over the set of hypothesis $\hypspace$. This action defines the following equivalence relation in $\hypspace$:
$$ \hypf_1 \sim_\frelset \hypf_2 \iff \exists \frelf \in \frelset,  \hypf_1 \circ \frelf = \hypf_2 .$$
%
This equivalence relation defines equivalence classes $\equivclass{\hypf}$, that is let $h' \in \hypspace$ be an hypothesis, then $h' \in \equivclass{\hypf}$ iff $h' \sim_\frelset h$. 
We consider the quotient space $\hypspace_\frelset \defeq \hypspace / \sim_\frelset = \set{\equivclass{\hypf}, \hypf \in \hypspace}$.
%
This equivalence classes are useful to divide the learning process in two stages: a first one to determine the best equivalence class, and a second one to find the best representative hypothesis of such equivalence class.
%

For example, consider the handwritten digits recognition problem, we might integrate $T$ different datasets designed in different conditions. Each dataset have been created using certain conditions of light and some specific scanner for getting the images. Even different pens or pencils might be influential in the stroke of the numbers. All these conditions are the $\frelset$ transformations, and each $\frelf \in \frelset$ generate a different bias for the dataset. However, there exists a probability for ''pure'' digits, e.g. the pixels of digit one have higher probability around a line in the middle of the picture than in the sides. This ''pure'' probability distribution $P_0$ and all the distributions $P_1, \ldots, P_T$ from which our datasets have been sampled might be $\frelset$-related among them and with $P_0$. If we first determine the $\frelset$-equivalent class of hypothesis $\equivclass{\hypf}$ suited for digit recognition in the first stage, then it will be easier to select $\hypf_1, \ldots, \hypf_\ntasks \in \equivclass{\hypf}$ for each dataset in the second one.


\subsubsection*{Bounds for $\frelset$-Related Tasks}
% Relation with Baxter work
The results of Theorem~\ref{th:baxter_vcdim} can be applied to the hypothesis space of equivalent classes $\hypspace_\frelset$. However the following results is needed first.
%
Let $P_1, P_2$ be $\frelset$-related distributions, then this statement can be proved~\cite[Lemma~2]{Ben-DavidB08}:
\begin{equation}
    \nonumber
    \inf_{\hypf \in \hypspace} \risk_{P_1}(\hypf) = \inf_{\hypf \in \hypspace} \risk_{P_2}(\hypf).
\end{equation}
This indicates that the the expected risk is invariant under transformations of $\frelset$.
Now, one of the main results~\cite[Theorem~2]{baxter2000model} can be given.
\begin{theorem}
    
\end{theorem}


% Results for F-related tasks (Theorems 2, 3)

% Analysis of generalized VCdim
\subsubsection*{Analysis of generalized VC-dimension}




\subsection{Learning Under Privileged Information}



\section{Multi-Task Learning Methods: An Overview}

\subsection{Bias Learning Approach}
% Joint Learning
% Leveraging common and specific information + LUPI
% Common + specific model which is equivalent to penalizing individual norm and variance
% Evgeniou, T. and Pontil, M. (2004). Regularized multi-task learning.
% Evgeniou, T., Micchelli, C. A., and Pontil, M. (2005).  Learning multiple tasks with kernel methods.
% Connection with LUPI!
% Generalized SMO
% Eigenfunction-Based Multitask Learning in a Reproducing Kernel Hilbert Space ?
% Learning multiple tasks with kernel methods

%\subsection{Feature Learning}
% Feature learning or
%   Feature transformation (relacion con Deep Learning)
% Sparse coding and MTL
% K-Dimensional Coding Schemes in Hilbert Spaces
% Sparse coding for multitask and transfer learning
% Learning task grouping and overlap in multi-task learning.
%%%%%%%% Relacion en A Survey on Multi-Task Learning
% Multi-Task Feature Learning.
% A spectral regularization framework for multi-task structure learning?
% Infinite latent SVM for classificationand multi-task learning


\subsection{Low-Rank Approach}
%   Feature selection or block sparse regularization

% Argyriou, A. and Pontil, M. (2007). Multi-Task Feature Learning.

%  A Dirty Model for Multi-task Learning

% Low-rank

% .  K.  Ando  and  T.  Zhang,  “A  framework  for  learning  predictivestructures  from  multiple  tasks  and  unlabeled  data,”

% A convex formulation for learning shared structures from multiple tasks

% Learning  multiple  tasks using manifold regularization

% Learning multiple related tasks using latent independent component analysis


 % Learning task relations
\subsection{Learning Task Relations}
% Task relation learning

% Bonilla

% A Convex Formulation for Learning Task Relationships in Multi-Task Learning

% A regularization approach to learning task relationships in multitask learning

% Convex learning of multiple tasks and their structure

% Tasks Clustering

% Discovering structure in multiple learning tasks: The TC algorithm?

% Learning  multiple  tasks  using  shared hypothesis?

% Flexible modeling of latent task structures in multitask learning

% Clustered multi-task learning: A convex formulation

% Learning  with  whom  to  share  in multi-task feature learning

% Learning task grouping and overlap in multi-task learning


\subsection{Decomposition Approach}
% learning to multitask

% .  Chen,  J.  Liu,  and  J.  Ye,  “Learning  incoherent  sparse  and  low-rankpatterns from multiple tasks,” inKDD, 2010.[100]  J.  Chen,  J.  Zhou,  and  J.  Ye,  “Integrating  low-rank  and  group-sparsestructures for robust multi-task learning,” inKDD, 2011.[101]  P. Gong, J. Ye, and C. Zhang, “Robust multi-task feature learning,” inKDD, 2012.[102]  W. Zhong and J. T. Kwok, “Convex multitask learning with flexible taskclusters,” inICML, 2012.



\section{Deep Multi-Task Learning}
\subsection{Hard Parameter Sharing}
\subsection{Soft Parameter Sharing}

% Deep multi-task representation learning: A tensor factorisation approach

% Trace norm regularised deep multi-task learning



\section{Multi-Task Learning with Kernel Methods} % Evgeniou
% Most multi-task methods are linear models, which may not be flexible enough to capture certain dependencies.
% Deep Learning is a very popular and cost-effective way of overcoming this problem. The final linear models are substituted by the neural network output and the parameters are learned together using back propagation.
% However, one of the main problems of deep learning is the lack of theoretical results and the non-convexity of the problems.
% Other alternative to extend the MTL models non-linearly is by using the kernels.
% Kernel trick.

% Joint Learning
% Leveraging common and specific information
% Common + specific model which is equivalent to penalizing individual norm and variance
% Evgeniou, T. and Pontil, M. (2004). Regularized multi-task learning.
% Evgeniou, T., Micchelli, C. A., and Pontil, M. (2005).  Learning 

% Teorema de Evgeniou

% When is there a representer theorem? Vector versus matrix regularizers.

% Multi-task least-squares support vector machines. Shuo

% Multi-task Gaussian process prediction. Bonilla

% Sparse coding for multitask and transfer learning

\section{Conclusions}\label{sec-conclusions-2}

In this chapter, we covered\dots
