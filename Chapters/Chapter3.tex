% Chapter 2

\chapter{Multi-Task Learning} % Write in your own chapter title
\label{Chapter3}
\lhead{Chapter \ref{Chapter2}. 
\emph{Multi-Task Learning}} % Write in your own chapter title to set the page header

{\bf \small{
This chapter presents\dots
}}

\section{Introduction}
% What is MTL
% Notation

% Examples and Motivation

% Some important references
% Caruana, Baxter... (ver review de Caruana para ver más)



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%%%%%%%%%             SECTION         %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Multi-Task Learning Methods: An Overview}\label{sec:ch3_overview}

\subsection{Feature-Based MTL}
% Feature learning or
%   Feature transformation (relacion con Deep Learning)
The feature-based methods try to find a set of features that are useful for all tasks. Two main approaches are taken: Feature Learning, which tries to learn new features from the original ones, and Feature Selection which selects a subset of the original features.
Deep Learning is a particular case of Feature Learning that has had a tremendous relevance in Machine Learning and that is treated separately. 

\subsubsection*{Feature Learning approach}


% Caruana R. Multitask learning. 1997 (multi-task feedforward NN)
% Multi-Task feature Learning 2006
% Convex multi-task feature learning 2008
% A spectral regularization framework for multi-task structure learning 2007
Apart from the Multi-Task feedforward Neural Network first shown in~\cite{Caruana97}, which will be described later, the first work of Multi-Task Feature Learning is presented in~\cite{ArgyriouEP06}. Argyriou et al. assume a multi-task linear model in some RKHS $\rkhs$, where that the task parameters $w_t$ lie in a linear subspace, i.e. $$\underset{\dimx \times \ntasks}{\mymat{w}} = \underset{\dimx \times \dimx}{\mymat{u}}\underset{\dimx \times \ntasks}{\mymat{a}}$$
< where
$w_t$ are the columns of $\mymat{w}$, $\mymat{u}$ is an orthogonal matrix and $\mymat{a}$ is a row-sparse matrix. The minimization problem is
\begin{equation}
    \label{eq:mtl_feat_learning}
    \argmin_{\mymat{u} \in \reals^{d \times d}, \mymat{a} \in \reals^{d \times \ntasks}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{\mymat{u} a_r}{x_i^r}) + \lambda \norm{\mymat{a}}_{2, 1}^2 \text{ s.t. } \mymat{u}^\intercal \mymat{u} = \mymat{i} .
\end{equation}
Here, the $L_{2, 1}$ regularizer is used to impose row-sparsity across tasks, i.e. forcing some rows of $\mymat{a}$ to be zero while the matrix $\mymat{u}$ is restricted to be orthonormal.
Although problem~\eqref{eq:mtl_feat_learning} is not jointly convex in $\mymat{u}$ and $\mymat{a}$, in~\cite{ArgyriouEP06} and~\cite{ArgyriouEP08} is shown to be equivalent to the convex problem
\begin{equation}
    \label{eq:convmtl_feat_learning}   
    \begin{aligned}
        &\argmin_{\mymat{w} \in \reals^{d \times \ntasks}, \mymat{d}  \in \reals^{d \times d}}  \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{w_r}{x_i^r}) 
        + \lambda \sum_{r=1}^\ntasks \dotp{w_r}{D^{-1}w_r} 
        %+ \lambda \trace{\mymat{w}^\intercal \mymat{d}^{-1} \mymat{w}}
        + \mu \trace{\mymat{D}^{-1}} \\ &\text{s.t.} \; \mymat{d} \succeq 0,\; \trace{\mymat{d}} \leq 1 .
    \end{aligned}
\end{equation}
Here $\fm{D} = \fm{U} \Diag{\left(\frac{\norm{\fv{a}^1}_2}{\norm{\fm{A}_{2, 1}}}\right), \ldots, \left(\frac{\norm{\fv{a}^\dimx}_2}{\norm{\fm{A}_{2, 1}}}\right) } \fm{U}$, $\fv{a}^i$ are the rows of $\fm{A}$, $\mymat{d} \succeq 0$ restricts the $\mymat{d}$ to the semidefinite positive matrices and the trace of $\mymat{D}^{+}$ is added for numerical stability and convenient convergence properties. Problem~\eqref{eq:convmtl_feat_learning} is solved using an alternating two-step optimization algorithm in $\mymat{W}$ and $\mymat{D}$.
First they fix $\mymat{d}$ and solve for $\mymat{w}$; while in the second step, with fixed $\mymat{w}$, there exists a closed solution for $\mymat{d}^* = \left(\mymat{w}^\intercal \mymat{w} + \mu \fm{I} \right)^\frac{1}{2} / \Tr\left( \left(\mymat{w}^\intercal \mymat{w} + \mu \fm{I} \right)^\frac{1}{2} \right)$.
When $\mu=0$, the regularizer of~\eqref{eq:convmtl_feat_learning} can be expressed as $\Tr \left( \mymat{w}^\intercal \mymat{d}^+ \mymat{w} \right)$ and by plugging $\mymat{d}^*$ in this formula we obtain the squared-trace norm regularizer for $\mymat{w}$:
%When $\mu=0$ in~\eqref{eq:convmtl_feat_learning} the optimization problem can be reformulated as
\begin{equation}
    \label{eq:mtl_feat_learning_tracenorm}
    \argmin_{\mymat{w} \in \reals^{\dimx \times \ntasks}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{w_r}{x_i^r}) + \lambda \norm{\mymat{w}}_{*}^2.
\end{equation}
%
Here, $\norm{W}_* = \trace{\left(W^\intercal W \right)^{\frac{1}{2}}}$ denotes the trace norm (also known as Frobenius or nuclear norm), which can be seen as the continuous envelope of the rank, thus favouring low-rank solutions of $\mymat{w}$.
%
This idea can be extended into a kernel setting in the standard way, by replacing the original features by the implicit transformations $\phi(x_i^r)$ in the primal problem.
%
In~\cite{ArgyriouMPY07} this idea is extended to any spectral funcion $F: \mathbb{S}^d_{++} \to \mathbb{S}^d_{++}$ where $\mathbb{S}^d_{++}$ is the set of matrices $A \in \mathbb{R}^{d \times d}$ symmetric and positive definite. The definition for the spectral function $F(\mymat{A})$, where we can diagonalize the matrix $\mymat{A}$ as $\mymat{V}^\intercal \Diag (\lambda_1, \ldots, \lambda_d)  \mymat{V}$ is
$$ F(\mymat{A}) = \mymat{u}^\intercal \Diag (f(\lambda_1), \ldots, f(\lambda_d)) \mymat{u} \; .$$
Then, a generalized regularizer for problem~\eqref{eq:convmtl_feat_learning} can be expressed as
$$ \sum_t \langle w_t, F(\mymat{D}) w_t\rangle = \trace{\mymat{w}^\intercal F(\mymat{D}) \mymat{w}} = \trace{F(\mymat{D}) \mymat{w}  \mymat{w}^\intercal} \; .$$
It is easy to see that problem~\eqref{eq:convmtl_feat_learning} is a particular case where $f(\lambda) = \lambda^{-1}$.
In~\cite{Maurer09} some bounds on the excess risks are given for this Multi-Task Feature Learning method.

%  Learning multiple tasks using manifold regularization. Agarwal
Another relevant extension is shown in~\cite{AgarwalDG10}, where instead of assuming that the task parameters $w_r$ lie in a linear subspace, the authors generalize this idea by assuming that $w_r$ lies in a manifold $\mathcal{M} \in \rkhs$.
\begin{equation}
    \label{eq:mtl_feat_learning_manifold}   
    \begin{aligned}
        &\argmin_{\mymat{w}, \mathcal{M}, \myvec{b}}  \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{w_r}{x_i^r} + b_r) + \lambda \sum_{r=1}^\ntasks \mathcal{P}_\mathcal{M}(w_r) ,
    \end{aligned}
\end{equation}
where $\mathcal{P}_\mathcal{M}(w_t)$ represents the distance between $w_t$ and its projection on the manifold $\mathcal{M}$. Again, an approximation of~\eqref{eq:mtl_feat_learning_manifold} is used to obtain a convex problem and it is solved using a two-step optimization algorithm.

% Sparse coding and MTL. 
% K-Dimensional Coding Schemes in Hilbert Spaces
% Sparse coding for multitask and transfer learning Maurer 2013
% Learning task grouping and overlap in multi-task learning.
Other distinct, relevant approach for Feature Learning is the one described in~\cite{MaurerPR13}, where a sparse-coding method~\cite{MaurerP10} is used for MTL. Maurer et al. present the problem
    \begin{equation}
        \label{eq:mtl_sparse_coding}
        \argmin_{\mymat{d} \in \mathcal{D}_k, \mymat{a} \in \reals^{k \times \ntasks}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{D a_r}{x_i^r}) + \lambda \norm{\mymat{d}}_{2, \infty} +\mu \norm{\mymat{a}}_{1, \infty} .
    \end{equation}
Here, $\mathcal{D}_k$ is the set of $k$-dimensional dictionaries and every $\mymat{d} \in \mathcal{D}_k$ is a linear map $\mymat{D}: \reals^k \to \rkhs$; in the linear case, where $\rkhs = \reals^d$, the set $\mathcal{D}_k$ is the set of matrices $\reals^{d \times k}$, such that 
$$\underset{d \times T}{\mymat{w}} = \underset{d \times k}{\mymat{D}} \underset{k \times T}{\mymat{a}} .$$
Although~\eqref{eq:mtl_feat_learning} and~\eqref{eq:mtl_sparse_coding} share a similar form, there are crucial differences. The matrix $\mymat{u}$ in~\eqref{eq:mtl_feat_learning} is an orthogonal square matrix, while the matrix $\mymat{d}$ of~\eqref{eq:mtl_sparse_coding} is overcomplete with $k > d$ columns of bounded norm.
A problem very similar to~\eqref{eq:mtl_sparse_coding} is presented in~\cite{KumarD12} where the idea is the same but the regularizers are the $L_{2, 2}$ (Frobenius) norm for $\mymat{d}$ and the $L_{1, 1}$ norm for $\mymat{a}$:
\begin{equation}
    \label{eq:mtl_go}
    \argmin_{\mymat{d} \in \mathcal{D}_k, \mymat{a} \in \reals^{k \times \ntasks}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{D a_r}{x_i^r}) + \lambda \norm{\mymat{d}}_{2, 2} +\mu \norm{\mymat{a}}_{1, 1} .
\end{equation}
Here, we can interpret this model as a linear sparse combination, encoded in $\fm{a}$, of some features encoded in $\fm{D}$: $\fv{w}_r^\intercal \cdot \fv{x}_i^r = \sum_{i=1}^k a_r^i \left(\fv{D}_i^\intercal \cdot \fv{x}_i^r \right)$.
Unlike the Multi-Task Feature Learning approach of Argyriou et al, this sparse coding formulation is only presented in the linear setting.

\subsubsection*{Feature Selection approach}
%   Feature selection or block sparse regularization
The feature selection is also driven by learning a good set of features for all tasks, however it focuses on subsets of the original features. Due to their nature, the works following this strategy are based on linear models. This is a more rigid approach than that of Feature Learning but is also more interpretable.

% G. Obozinski, B. Taskar, and M. Jordan, “Multi-task feature selection,” (2006)
% H. Liu, M. Palatucci, and J. Zhang, “Blockwise coordinate descent  procedures for the multi-task lasso, with applications to neural semantic basis discovery,” in ICML, 2009.
Most works on Multi-Task Feature Selection uses an $L_{p, q}$ regularization of the weights matrix $\mymat{w}$. The first work~\cite{obozinski2006multi} solves the problem
\begin{equation}
    \label{eq:mtl_feat_selection}   
    \begin{aligned}
        &\argmin_{\mymat{w}}  \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{w_r}{x_i^r}) + \lambda \norm{\mymat{w}}_{2, 1}^2 ,
    \end{aligned}
\end{equation}
where the $L_{2, 1}$ regularization enforces row sparsity and forces different tasks to share a subset of features $\fv{w}^i$, which are the rows of $\mymat{w}$. In~\cite{LiuPZ09} the $L_{\infty, 1}$ regularization is used for the same goal. 
Then, in~\cite{GongYZ12} this idea is generalized with a capped-$L_{p, 1}$ penalty of $\mymat{W}$, which is defined as
$ \sum_{i=1}^d min(\theta, \norm{\fv{w}^i}_p).$
That is, the parameter $\theta$ enables a more flexible regularization, with small values of $\theta$ the smallest rows are pushed towards zero since the rows with norms larger than $\theta$ will not dominate the sum, as $\theta$ grows this penalty will degenerate to the standard $L_{p, 1}$ norm.

%
In~\cite{LozanoS12} a multi-level lasso selection is presented where the main idea is to decompose each $w_r^i$, that is, the $i$-th feature of the $r$-th task as 
$w_r^i = \theta^i a_r^i$ 
and then, using the matrix $\fm{\Theta} = \Diag \left( \theta^1, \ldots, \theta^\dimx \right)$, the y define the problem
\begin{equation}
    \label{eq:mtl_multilevel_lasso}   
    \begin{aligned}
        &\argmin_{\mymat{\Theta}, \fv{a}_1, \ldots, \fv{a}_\ntasks}  \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{\fm{\Theta} \fv{a}_r}{x_i^r})  + \mu \Tr \Theta + \nu \norm{\fm{a}}_{1, 1},
    \end{aligned}
\end{equation}
By doing this, the features $i$ such that $\theta^i = 0$, are discarded for all tasks, but the rest may be shared among tasks or not depending on the values of $a_r^i$.
Observe that this is similar to the sparse coding problem shown in~\eqref{eq:mtl_sparse_coding} with $\fm{d}$, the ``feature building'' matrix, being limited to diagonal matrices since it is acting here as a selection matrix: $\fv{w}_r^\intercal \cdot \fv{x}_i^r = \sum_{i=1}^k a_r^i \left({\theta}_i \cdot \fv{x}_i^r \right)$.

% Bayesian approaches
% 15. Zhang Y, Yeung DY and Xu Q. Probabilistic multi-task feature selection. In:
% Advances in Neural Information Processing Systems 23. 2010, 2559–67.
% 16. Hern ́andez-Lobato D and Hern ́andez-Lobato JM. Learning feature selection
% dependencies in multi-task learning. In: Advances in Neural Information Pro-
% cessing Systems 26. 2013, 746–54.
% 17. Hern ́andez-Lobato D, Hern ́andez-Lobato JM and Ghahramani Z. A probabilis-
% tic model for dirty multi-task feature selection. In: Proceedings of the 32nd
% International Conference on Machine Learning. 2015, 1073–82.
The feature selection methods based on $L_{p, 1}$ regularization are shown to be equivalent to a Bayesian approximation with a generalized Gaussian prior in~\cite{ZhangYX10}. Moreover, this approach also allows to find the relationship among tasks and to identify outliers. In~\cite{Hernandez-LobatoH13} a horseshoe prior is used instead to learn feature covariance; and in~\cite{Hernandez-Lobato15} this prior is also used to identify outlier tasks.


\subsubsection*{Deep Learning approach}

% Hard parameter sharing
% Soft parameter sharing

% Linear algorithms for online multitask classification 2010

% cross-stitch networks for multi-task learning 2016

% An Overview of Multi-Task Learning in Deep Neural Networks 2017 (incluye los de 2017?)

% sluice-networks 2017

% Deep multi-task representation learning: A tensor factorisation approach 2017

% Trace norm regularised deep multi-task learning 2017

% Latent Multi-Task Architecture Learning 2019

% adashare 2020


\subsection{Parameter-Based MTL}
The parameter-based MTL does not focus on shared sets of features across tasks, instead other types of dependencies among the task like the task-parameters $w_r$, are taken into account. Some approaches rely on the assumption that the Multi-Task weight matrix $\mymat{w}$ has a low rank, others try to learn the pairwise task relations or to cluster the tasks. A different approach is the decomposition one, where the assumption is that the matrix $\mymat{w}$ can be expressed as the summation of multiple matrices. We summarize each approach below.

\subsubsection*{Low-Rank approach}
% Low-rank

% .  K.  Ando  and  T.  Zhang,  “A  framework  for  learning  predictive structures  from  multiple  tasks  and  unlabeled  data,” 2005

% A convex formulation for learning shared structures from multiple tasks 2009

% Trace Norm Regularization: Reformulations, Algorithms, and Multi-Task Learning

% Multi-Stage Multi-Task Learning with Reduced Rank
In the low-rank approach the assumption is that task parameters $w_r$ share a low-dimensional space, or, at least, are close to this subspace. This is similar to the Feature Learning approach, but it is not that rigid, since it allows for some flexibility.
The idea in~\cite{AndoZ05} is that the task parameters can be decomposed as
$$ w_r = u_r + \mymat{\Theta}^\intercal v_r,$$
where $\mymat{\Theta} \in \reals^{k \times \dimx}$ spans a shared low dimensional space, that is $\mymat{\Theta} \mymat{\Theta}^\intercal = \mymat{i}_k$ with $k < \dimx$, and $d$ is the dimension of the data. Under this consideration, the proposed model is
\begin{equation}
    \label{eq:mtl_struct_learn}
    \argmin_{\mymat{\Theta} \in \reals^{k \times \dimx}, \myvec{u}, \myvec{v}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{u_r + \mymat{\Theta}^\intercal v_r}{x_i^r}) + \lambda \sum_{r=1}^\ntasks \norm{u_r}^2 \text{ s.t. } \mymat{\Theta} \mymat{\Theta}^\intercal = \mymat{i}_k .
\end{equation}
Observe that this problem shares some similarities with~\eqref{eq:mtl_feat_learning}. However, this is a more flexible approach, since the vectors $u_r$ allow for deviations of the task parameters from the shared subspace.
Problem~\eqref{eq:mtl_struct_learn} is solved using a two-step optimization iterating between minimizing in $\{\mymat{\Theta}, \myvec{v}\}$ and minimizing in $\myvec{u}$. Observe that problem~\eqref{eq:mtl_struct_learn} is not convex but it can be reformulated as
\begin{equation}
    \label{eq:mtl_struct_learn_ref}
    \argmin_{\mymat{\Theta} \in \reals^{k \times \dimx}, \myvec{w}, \myvec{v}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{w_r}{x_i^r}) + \lambda \sum_{r=1}^\ntasks \norm{w_r - \mymat{\Theta}^\intercal v_r}^2 \text{ s.t. } \mymat{\Theta} \mymat{\Theta}^\intercal = \mymat{i}_k ,
\end{equation}
where the terms $\norm{w_r - \mymat{\Theta}^\intercal v_r}^2$ enforces the similarity across tasks by bringing them closer to the shared subspace.
In~\cite{ChenTLY09} the following extension is proposed:
\begin{equation}
    \label{eq:mtl_struct_learn_frob}
    \argmin_{\mymat{\Theta} \in \reals^{k \times \dimx}, \myvec{w}, \myvec{v}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{w_r}{x_i^r}) + \lambda \sum_{r=1}^\ntasks \norm{w_r - \mymat{\Theta}^\intercal v_r}^2 + \mu \sum_{r=1}^\ntasks \norm{w_r}^2 \text{ s.t. } \mymat{\Theta} \mymat{\Theta}^\intercal  = \mymat{i}_k .
\end{equation}
And it is shown that~\eqref{eq:mtl_struct_learn_frob}, when relaxing the orthogonality constraint, can be expressed as a convex minimization problem.
%

A different approach relies on the use of the trace norm of matrix $\mymat{w}$. This norm penalizes $\sum_{i=1}^d {\lambda_i(\mymat{w})}^2$, where $\lambda_i(\mymat{w})$ are the eigenvalues of $\mymat{w}$, and thus, forcing $\mymat{w}$ to be low-rank.
In the work of~\cite{PongTJY10} new formulations for problems with this trace norm penalty and a primal-dual method for solving the problem is developed.
% ?
A modification of the trace norm can be found in~\cite{HanZ16}, where a capped-Frobenius norm is defined as $\sum_{i=1}^d \min(\theta, {\lambda_i(\mymat{w})}^2)$. This capped norm, as in the capped-$L_{p, q}$ norm, can enforce a lower rank matrix for small $\theta$ and also degenerates to the trace norm for large enough $\theta$. 


\subsubsection*{Task-Relation Learning approach}
% Task relation learning
In other approaches, like the Feature Learning approach or the Low-Rank approach, the assumption is that all task parameters share the same subspace, which may be detrimental when there exists a negative or neutral transfer. The Task-Relation Learning approach aims to find the pairwise dependencies among tasks and to possibly model positive, neutral and negative transfers between tasks.

% GP approach
% Multi-task Gaussian Process Prediction. Bonilla. 2007
% A Convex Formulation for Learning Task Relationships in Multi-Task Learning 2010
% A regularization approach to learning task relationships in multitask learning 2013
One of the first works with the goal of explicitly modelling the pairwise task-relations is~\cite{BonillaCW07}, where a Multi-Task Gaussian Process (GP) formulation is presented. Assuming a Gaussian noise model 
$y_i^r \sim \normal{f_r(x_i^r), \sigma_r^2}$, 
Bonilla et al. place a GP prior over the latent functions $\myvec{f}_r$ to induce correlation among tasks:
$$ \myvec{f} \sim \normal{\myvec{0}_{d \ntasks}, \mymat{K}^f \otimes K_\theta^\Xspace}$$ 
where $\myvec{f} = (\myvec{f}_1, \ldots, \myvec{f}_\ntasks)$ and $\mymat{a} \otimes \mymat{b}$ is the Kronecker product of two matrices, so $\Cov(f_r(x), f_s(x')) = \mymat{K}^f_{rs} k^\Xspace_\theta(x, x')$. That is, instead of assuming a block-diagonal covariance matrix for $\myvec{f}$ as in previous works of Joint Learning~\cite{LawrenceP04}, the authors in~\cite{BonillaCW07} model the covariance as a product of inter-task covariance and inter-feature covariance. The inference of this model can be done using the standard GP inference for the mean and the variance of the prediction distribution. The mean prediction for a new data point $x_*$ of task $s$ is:
\begin{equation}
    \nonumber
    f_s(x_*) = (\mymat{K}^f_{:,l} \otimes \mymat{K}^\Xspace(:, x_*))^\intercal \Sigma^{-1} \myvec{y}, \;
     \Sigma = \mymat{K}^f \otimes \mymat{K}^\Xspace_\theta + \Diag(\sigma_1, \ldots, \sigma_\ntasks) \otimes \mymat{i}_{\nsamples}.
\end{equation}
However the interest resides in learning the task-covariance matrix $\mymat{K}^f$, but this leads to a non-convex problem. The authors propose a low-rank approximation of $\mymat{K}^f$, which weaken its expressive power.
To overcome this disadvantage, in~\cite{ZhangY10,ZhangY13a}, using the idea of the Multi-Task GP they consider linear models $f(x_i^r) = \dotp{w_r}{x_i^r} + b_r$ and the prior on matrix $\mymat{w} = (w_1 \ldots, w_\ntasks)$ is defined as
\begin{equation}
    \nonumber
    \mymat{W} \vert \sigma_r \sim \left(\prod_{r=1}^\ntasks \normal{\myvec{0}_d, \theta_i^2 I_d}  \right) \multinormal{\mymat{0}_{d \times m}, \mymat{i}_d \otimes \mymat{\Omega}}
\end{equation}
where $\multinormal{0, \mymat{A} \otimes \mymat{B}}$ denotes the matrix-variate normal distribution with mean $\mymat{M}$, row covariance matrix $\mymat{a}$ and column covariance matrix $\mymat{b}$. It is shown that the problem of selecting the maximum a posteriori estimation of $\mymat{w}$ and the maximum Likelihood estimations of $\mymat{\Omega}$ and $\myvec{b}$ has a regularized minimization problem that, when relaxing the restrictions on $\Omega$, can be expressed as
% \begin{equation}
%     \label{eq:mtl_relation_learn_trace}
%     \argmin_{\mymat{\Omega} \in \reals^{d \times \ntasks}, \mymat{w}, \myvec{b}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{w_r}{x_i^r}) + \lambda \Tr\left({\mymat{w} \mymat{w}^\intercal}\right) + \mu \Tr \left(\mymat{w} \mymat{\Omega}^{-1} \mymat{w}^\intercal \right)\text{ s.t. } \mymat{\mymat{\Omega}} \succeq 0, \Tr\mymat{\Omega} = 1 .
% \end{equation}
\begin{equation}
    \label{eq:mtl_relation_learn}
    \begin{aligned}
        &\argmin_{\mymat{\Omega} \in \reals^{d \times \ntasks}, \mymat{w}, \myvec{b}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{w_r}{x_i^r} + b_r) + \lambda \sum_{r=1}^\ntasks \norm{w_r}^2 + \mu \sum_{r,s=1}^\ntasks \left(\Omega^{-1}\right)_{rs} \norm{w_r - w_s}^2 \\
        &\text{ s.t. } \mymat{\mymat{\Omega}} \succeq 0, \Tr\mymat{\Omega} = 1 .
    \end{aligned}    
\end{equation}
This is a convex formulation and a two-step procedure is developed to find the solution. 

% Laplacian approach
% Learning the Graph of Relations Among Multiple Tasks. 2013
% Learning Output Kernels for Multi-TaskProblems. 2013
% Convex learning of multiple tasks and their structure 2015
Other approaches like~\cite{argyriou2013learning} reach a similar problem from other perspective. Argyriou et al. assume a representation of the structure of the tasks as a graph, then the graph Laplacian in the optimization problem can incorporate the knowledge about the task structure as shown in~\cite{EvgeniouMP05}:
\begin{equation}
    \label{eq:mtl_laplacian}
    \begin{aligned}
        &\argmin_{\mymat{w}, \myvec{b}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{w_r}{x_i^r} + b_r) + \mu \sum_{r,s=1}^\ntasks \left(\mymat{L}^{+}\right)_{rs} \dotp{w_r}{w_s} .\\
        %&\text{ s.t. } \mymat{\mymat{\Omega}} \succeq 0, \Tr\mymat{\Omega} = 1 .
    \end{aligned}    
\end{equation}
Here the regularization can be also written using the adjacency matrix $\mymat{A}$ as
\begin{equation}
    \nonumber
    \sum_{r,s=1}^\ntasks \left(\mymat{L}^{+}\right)_{rs} \dotp{w_r}{w_s} = \sum_{r,s=1}^\ntasks \left(\mymat{A}^{+}\right)_{rs} \norm{w_r - w_s}^2,
\end{equation}
and $\mu$ is a tuning parameter.
The goal of~\cite{argyriou2013learning} is to jointly learn the task parameters and the graph Laplacian $\mymat{L}$. For that, they add the penalty some small perturbation to the diagonal of $\mymat{L}$ so they obtain a convenient positive kernel; and then, according to~\cite{EvgeniouMP05}, the problem~\eqref{eq:mtl_laplacian} can be solved in the dual space using
$$ k_\mymat{L}(x_i^r, x_j^s) = \left(\mymat{L}+ \lambda \mymat{I} \right)^{-1} _{rs} k(x_i^r, x_j^s). $$
Argyriou et al. propose the joint learning in this dual space by solving
\begin{equation}
    \label{eq:mtl_laplacian_dual}
    \begin{aligned}
        &\argmin_{\myvec{\alpha}, \mymat{L}} \myvec{\alpha}^\intercal \mymat{K}_L \myvec{\alpha} + \nu \myvec{\alpha}^\intercal \bm{y}  + \Tr \left(\mymat{L}+ \lambda \mymat{I} \right)^{-1} \\
        &\text{ s.t. } \mymat{0} \preceq \left(\mymat{L}+ \lambda \mymat{I} \right)^{-1} \preceq \frac{1}{\lambda} I,\; \left(\mymat{L}+ \lambda \mymat{I} \right)^{-1}_\text{off} \leq 0, \; \left(\mymat{L}+ \lambda \mymat{I} \right)^{-1} \myvec{1}_\nsamples = \frac{1}{\lambda} \myvec{1}_\nsamples ,
    \end{aligned}
\end{equation}
where $\mymat{A}_\text{off}$ denotes the off diagonal entries of $\mymat{A}$. The restrictions of problem~\eqref{eq:mtl_laplacian_dual} are to ensure that $\mymat{L}$ is a valid graph Laplacian, and the trace term in the objective function is to force $\mymat{L}$ to be low-rank so it is easier to find clusters of tasks. The objective function is not jointly convex but is convex in each $\myvec{\param}$ and $\mymat{L}$ when we fix the other. The step to optimize the Laplacian matrix requires two projection steps using proximal operators.
Another work focused on learning the task-relations is~\cite{Dinuzzo13}, where the approach the problem a learning problem in an RKHS of vector-valued functions $g: \Xspace \to \reals^\ntasks$, where the associated reproducing kernel is:
\begin{equation*}
    H(x_1, x_2) = K_\Xspace(x_1, x_2) \cdot \mymat{L}
\end{equation*}
and $\mymat{L}$ is a symmetric positive matrix called \emph{output} kernel.

In~\cite{CilibertoMPR15} some results are developed for the convergence of alternating minimization algorithms in convex problems as that used in~\cite{ZhangY13a}. However, since the problems presented in~\cite{argyriou2013learning,Dinuzzo13} are not convex, this result of convergence does not hold.

% learning to multitask ?

\subsubsection*{Task Clustering approach}
The Task Clustering approach tries to find $K$ clusters or groups among the original set of $T$ tasks. Usually, the goal is to learn jointly only the tasks in the same cluster, so no negative transfer takes place.
% Discovering structure in multiple learning tasks: The TC algorithm. 1996
% Task clustering and gating for Bayesian multitask learning. 1999
% Clustered multi-task learning: A convex formulation. 2008
% Learning  with  whom  to  share  in multi-task feature learning. 2011
% Learning task grouping and overlap in multi-task learning. 2012
% Learning Multiple Tasks using Shared Hypotheses 2012
% Convex Multi-Task Learning by Clustering. 2015
The first clustering approach~\cite{ThrunO96} divides the optimization process in two separate steps: independently learning the task-parameters and jointly learning the clusters of tasks.
%It assumes that the models involves $f(x)$ needs a definition of distance, e.g. kernel methods, such that
Using models that involves distances among points, e.g. kernel methods, we define for each task the distance
$$ \text{dist}_{\phi_r}(x, x') = \sqrt{\sum_{i=1}^\dimx \phi_r^i \left( x^i - x'^i \right)^2}. $$
That is, $\myvec{\phi}_r$ parametrizes a distance with a different weight for each feature.
Then, for each task $r=1, \ldots, \ntasks$ an optimal $\myvec{\phi}_r^*$ is computed minimizing the distance between examples of the same class and maximizing the distance among different classes:
\begin{equation}
    \nonumber
    A_r(\phi) = \sum_{i, j}^{\npertask_r} (y_i^r  y_j^r) \text{dist}_\phi(x_i^r, x_j^r).
\end{equation}
Then, the procedure is the following. First the empirical loss on task $r$ of a model fitted on data of task $r$ using a distance parametrized by $\phi_s^*$ is defined as $e_{rs}$. Then, for $K = 1, 2,  \ldots, \ntasks$, the following functional is minimized
\begin{equation}
    \nonumber
    J(K) = \sum_{\kappa = 1}^K \sum_{r \in B_\kappa} \frac{1}{\abs{B_r}} \sum_{s \in B_\kappa} e_{r, s} ,
\end{equation}
and the number of clusters $K$ with minimum $J(K)$ is selected. That is, the clusters are selected using the results of independently trained tasks and the transfer is done through the parameters $\phi$.
%

A different approach is the Bayesian proposal of~\cite{BakkerH03} where a Multi-Task Neural Network~\cite{Caruana97} with one shared hidden layer is used. The task-dependent parameters can be modeled together using a prior that is a mixture of $K$ Gaussians $\myvec{a}_r \sim \sum_{\kappa = 1}^K \alpha_\kappa \normal{\myvec{\mu}_\kappa, \mymat{\Sigma}_\kappa}$; and by learning the variables $\alpha_i$ using Bayesian inference, the tasks can be clustered. In this model, unlike in~\cite{ThrunO96} the clusters and task parameters are jointly learned.
%

From the regularization approach some proposals have also been made. In~\cite{JacobBV08} a problem based on~\cite{EvgeniouP04} is proposed. Considering $\mymat{U} = \frac{1}{\ntasks} \myvec{1} \myvec{1}^\intercal$, $\mymat{E}$ the $\ntasks \times K$ cluster assignment binary matrix, and defining the adjacency matrix $M = E (E^\intercal E)^{-1} E^\intercal$ the problem is
\begin{equation}
    \label{eq:mtl_clustered}
    \begin{aligned}
        &\argmin_{\mymat{w}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{w_r}{x_i^r}) + \lambda (\mu_\text{m} \Omega_\text{m}(\mymat{w}) + \mu_\text{b} \Omega_\text{b}(\mymat{w}) + \mu_\text{w} \Omega_\text{w}(\mymat{w})) ,
    \end{aligned}    
\end{equation}
where $\Omega_\text{m} = \Tr\left( \mymat{W} \mymat{U} \mymat{W}^\intercal \right)$ is the mean regularization, $\Omega_\text{b} = \Tr\left( \mymat{W} (\mymat{M} - \mymat{U}) \mymat{W}^\intercal \right)$ is the inter-cluster variance regularization and $\Omega_\text{w} = \Tr\left( \mymat{W} (\mymat{I} - \mymat{M}) \mymat{W}^\intercal \right)$ is the intra-cluster variance regularization.
This problem cannot be solved using the results of~\cite{EvgeniouMP05} because the regularization used is not convex, so a convex relaxation is needed.
%

A similar approach is presented in~\cite{KangGS11} where, using the results from~\cite{ArgyriouEP08}, they propose a trace norm regularizer of the matrices $\mymat{W}_\kappa = \mymat{W} Q_\kappa$, where $Q_\kappa$ are diagonal matrices and the $r$-th element of the diagonal indicates if task $r$ corresponds to cluster $\kappa$. They consider the problem
\begin{equation}
    \label{eq:mtl_clustered_featlearn}   
    \begin{aligned}
        &\argmin_{\mymat{w}, \mymat{Q}_1, \ldots, \mymat{Q}_T, \myvec{b}} & &\sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{w_r}{x_i^r} + b_r) + \lambda \sum_{\kappa=1}^K \norm{\mymat{W}\mymat{Q}_\kappa}_*^2 \\
        &\text{s.t.} & & \sum_{\kappa= 1}^K \mymat{Q}_\kappa = \mymat{I} \text{ with } 0 \leq \mymat{Q}_{\kappa t} \leq 1
    \end{aligned}
\end{equation}
This can be seen as a clusterized version of Multi-Task Feature Learning~\cite{ArgyriouEP06}, that is, instead of assuming that all tasks share the same subspace, only the tasks in the same cluster do, however the explicit learned features cannot be recovered.
%

Another approximation to clusterized MTL is provided in~\cite{CrammerM12}, where a two-step procedure if described as follows. Considering that $K$ initial clusters are fixed containing the $T$ tasks, then two steps are repeated:
First $K$ single task models $f_\kappa$ are fitted using the pooled data from tasks in cluster $\kappa$.
Secondly each task $r$ is assigned to the cluster $\kappa$ whose function $f_\kappa$ obtains the lowest error in task $r$.
%
The proposal of~\cite{BarzilaiC15} takes the idea of the cluster assignation step from~\cite{CrammerM12} and is also inspired by the work of~\cite{KumarD12}. In this work a model is presented where $\mymat{w} = \mymat{D} \mymat{A}$, where $\mymat{D} \in \reals^{\dimx \times K}$ contains as columns the hypothesis for each cluster and $\mymat{G} \in \reals^{K \times \ntasks}$ is the task assignment matrix, that is ${g}_r = G_r \in \set{0, 1}$ and $\norm{{g}_r}^2 = 1$. The corresponding optimization problem is
\begin{equation}
    \label{eq:mtl_clustering_convex}
    \begin{aligned}
        &\argmin_{\mymat{d} \in \reals^{\dimx \times K}, \mymat{G} \in \reals^{K \times \ntasks}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{D g_r}{x_i^r}) + \lambda \norm{\mymat{d}}_{2, 1} \\
        & \text{s.t. } {g}_r \in [0, 1], \; \norm{{g}_r}^2 = 1 ,
    \end{aligned}
\end{equation}
where the constraints on $g_r$ have been relaxed to be in the $[0,1]$ interval in order to make problem~\eqref{eq:mtl_clustering_convex} convex. Observe that~\eqref{eq:mtl_clustering_convex} is similar to~\eqref{eq:mtl_go} but different restrictions are used to ensure that $\mymat{G}$ can be seen as a clustering assignment matrix.

\subsubsection*{Decomposition approach}
The Decomposition approach considers that assuming that the task parameters resides in the same subspace or that the parameter matrix $\mymat{w}$ is too restrictive for real world scenarios. The proposition is then to decompose the parameter matrix in the sum of two matrices, i.e. $\mymat{W} = \mymat{U} + \mymat{V}$ where usually $\mymat{U}$  captures the shared properties of the tasks and $V$ accounts for the information that cannot be shared among tasks.
This models also receive the name of \emph{dirty models} because they assume that the data is \emph{dirty} and does not place perfectly in constrained subspaces.
% A Dirty Model for Multi-task Learning. 2010
% Learning incoherent sparse and low-rank patterns from multiple tasks. 2010
% Robust multi-task feature learning 2012
% Integrating low-rank and group-sparse structures for robust multi-task learning. 2012
% A convex feature learning formulation for latent task structure discovery. 2012
% Hierarchical regularization cascade for joint learning 2013
% Learning tree structure in multi-task learning. 2015
The optimization problem is
\begin{equation}
    \label{eq:mtl_dirty}
    \argmin_{\mymat{u}, \mymat{v} \in \reals^{\dimx \times \ntasks}} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{u_r + v_r}{x_i^r}) + \lambda g(\mymat{U}) +\mu h(\mymat{V}),
\end{equation}
where $g(\mymat{U})$ and $h(\mymat{V})$ are different regularizers for $\mymat{U}$ and $\mymat{V}$, respectively.
In~\cite{JalaliRSR10} $g(\mymat{U}) = \norm{\mymat{U}^\intercal}_{1, \infty}$ to enforce block-sparsity and $h(\mymat{V}) = \norm{\mymat{V}}_{1, 1}$ to enforce element-sparsity. 
In~\cite{ChenLY10} $g(\mymat{U}) = \norm{\mymat{U}}_{2, 2}$ to enforce low-rank while maintaining $h(\mymat{V}) = \norm{\mymat{V}}_{1, 1}$. 
In~\cite{ChenZY11} both regularizers seek properties shared among all tasks, $g(\mymat{U}) = \norm{\mymat{U}}_{2, 2}$ to enforce a low-rank and $h(\mymat{V}) = \norm{\mymat{V}^\intercal}_{1, 2}$ for row-sparsity.
In~\cite{GongYZ12rmfl} they propose $g(\mymat{U}) = \norm{\mymat{U}^\intercal}_{1, 2}$ to enforce row-sparsity, i.e. the tasks share a common subspace; and $h(\mymat{V}) = \norm{\mymat{V}}_{1, 2}$ which penalizes the orthogonal parts to the common subspace of task-parameter, the authors state that it penalizes outlier tasks.

Other approaches generalize the decomposition method by assuming that the parameter matrix can be expressed as $\mymat{W} = \sum_{l=1}^L \mymat{W}_l$, then the problem to solve has the form
\begin{equation}
    \label{eq:mtl_decomposition_gen}
    \argmin_{\mymat{W}_1, \ldots, \mymat{W}_L \in \reals^{\dimx \times \ntasks}} 
    \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} 
    \lossf(y_i^r, \dotp{\sum_{l=1}^L {(W_l)}_r}{x_i^r}) 
    + \sum_{l=1}^L \lambda_l r(\mymat{W}_l) ,
\end{equation}
In~\cite{ZweigW13} the regularizer used is $r(\mymat{W}_l) = \norm{\mymat{W}_l^\intercal}_{2, 1} + \norm{\mymat{W}_l}_{1, 1}$ to enforce the row and element-sparsity. 
In~\cite{HanZ15} the regularizer is $r(\mymat{W}_l) = \sum_{r,s=1}^T \norm{(W_l)_r - (W_l)_s}^2$, which alongside some constraints it allows to build a tree of task groups, where the root contains all the tasks and the leafs only correspond to one task.


\subsection{Joint MTL}
% Joint Learning
% Sometimes in task clustering approaches, sometimes in task relations learning
The Joint Learning methods for MTL can be divided into the frequentist and the Bayesian approaches.
The frequentist approaches uses a combination of task-specific models and models that are common to all tasks. These two models are learned simultaneously with the goal of leveraging the common and specific information.
In the Bayesian approaches common prior is shared for all the tasks models, and the diverse sources of data define different posterior distributions for each task.

\subsubsection*{Bayesian Approach}
% Bayesian approach
% Learning to learn with the informative vector machine (2004)
The work of~\cite{LawrenceP04} presents a GP model where all the tasks share a common prior.
That is, given $T$ tasks, a noise model with latent variables $\myvec{f}_r$ is considered, i.e. $y_i^r = f_i^r + \epsilon$, and $\myvec{f}_r$ follows a GP prior
$$ p(\myvec{f}_r \vert \mymat{x}, \myvec{\theta}) = \normal{0, \mymat{k}_{\myvec{\theta}}} $$
where $\mymat{k}$ is a kernel matrix parametrized by $\myvec{\theta}$ and evaluated at the points $\mymat{x}$. Note that a single $\myvec{\theta}$ parameter is used to model a prior shared for all tasks.
Using this idea the posterior probability can be expressed as
\begin{equation}
    \nonumber
    p (\myvec{y}^1, \ldots, \myvec{y}^\ntasks \vert \myvec{f}_1, \ldots, \myvec{f}_\ntasks, \myvec{\theta},  \mymat{x}^1, \ldots, \mymat{x}^\ntasks) \propto  p (\myvec{f}_1, \ldots, \myvec{f}_\ntasks, \myvec{\theta} \vert \mymat{x}^1, \ldots, \mymat{x}^\ntasks) \prod_{r=1}^\ntasks p(\myvec{y}^r \vert \myvec{f}_r, \myvec{\theta}) %p(\myvec{\theta} | \myvec{f}_r)
\end{equation}
where it is assumed the distribution for the latent parameters factorizes as
\begin{equation}
    \nonumber
    p (\myvec{f}_1, \ldots, \myvec{f}_\ntasks, \myvec{\theta} \vert \mymat{x}^1, \ldots, \mymat{x}^\ntasks) \propto \prod_{r=1}^\ntasks p(\myvec{f}_r \vert \mymat{x}^r, \myvec{\theta}) , %p(\myvec{\theta} | \myvec{f}_r).
\end{equation}
where $p(\myvec{f}_r \vert \mymat{x}^r, \myvec{\theta}) = \normal{0, K_\theta}$, that is 
$\Cov(f^r(x), f^r(x')) = k_\theta(x, x') .$
Although this idea is interesting for MTL, it presents a rigid framework since we use a fixed model for the prior $ p(\myvec{f}_r \vert \mymat{x}, \myvec{\theta})$. To use Bayesian induction over the prior too, the hierarchical Bayesian model is considered. That is, we consider a different prior $ p(\myvec{f}_r \vert \mymat{x}, \myvec{\theta}^r)$ for each task and a hyperprior for $\myvec{\theta}^r$, $p(\myvec{\theta}^r \vert \myvec{\phi})$. Then, the distribution for latent parameters is expressed as
\begin{equation}
    \nonumber
    p (\myvec{f}_1, \ldots, \myvec{f}_\ntasks, \myvec{\theta}^1, \ldots, \myvec{\theta}^\ntasks, \myvec{\phi} \vert \mymat{x}^1, \ldots, \mymat{x}^\ntasks) \propto \prod_{r=1}^\ntasks p(\myvec{f}_r \vert \mymat{x}^r, \myvec{\theta}^r, \myvec{\phi}) p(\myvec{\theta}^r | \myvec{\phi}).
\end{equation}
In~\cite{YuTS05}, a Gaussian hyperprior $p(\myvec{\theta}^r | \myvec{\phi})$ is considered. Note that in this formulation, each task parameter $\myvec{\theta}^r$ is an independent from the rest of parameters $\myvec{\theta}^s, s \neq r$ given $\myvec{\phi}$ .
That is, the joint distribution factorizes as
\begin{equation}
    \nonumber
    p(\myvec{\theta}^1, \ldots, \myvec{\theta}^\ntasks | \myvec{\phi}) = \prod_{r=1}^\ntasks p(\myvec{\theta}^r | \myvec{\phi}) .
\end{equation} 

%
% Learning Gaussian processes from multiple tasks (2005)
% Multi-Task Learning for Classification with Dirichlet Process Priors (2007)
% Bayesian multitask learning with latent hierarchies (2009)
Then, in~\cite{XueLCK07} a Dirichlet Process Prior is considered for modelling the task parameters. An explicit dependence is then defined over the task parameters
\begin{equation}
    \nonumber
    p(\myvec{\theta}^1, \ldots, \myvec{\theta}^\ntasks | \myvec{\phi}) = \prod_{r=1}^\ntasks p(\myvec{\theta}^r |\myvec{\theta}^{-r} , \myvec{\phi}) .
\end{equation} 
where $\myvec{\theta}^{-r}  = \set{\myvec{\theta}^s, s \neq r }$.
This formulation converts this model in a task-clustering approach, where the clusters are learned jointly with the task parameters $\myvec{\theta}$.
Following this approach of hierarchical Bayes,~\cite{Daume09} uses a prior for the task parameters $\myvec{\theta}^r$ that learns backwards a genealogy tree. That is, beginning at the leafs, which are the task parameters $\myvec{\theta}^r$, the branches merge until a common root to all the tasks. Thus, by selecting different thresholds or levels of this tree, we can obtain different clusters.

\subsubsection*{Frequentist Approach}
% Frequentist approach
% Evgeniou, T. and Pontil, M. (2004). Regularized multi-task learning.
The first proposal of the frequentist approach, which uses the SVM as the foundation, is found in~\cite{EvgeniouP04} where the \emph{regularized MTL} SVM is presented. The goal is to find a decision function for each task, each being defined by a vector
$$w_r = w + v_r,$$
where $w$ is common to all tasks and $v_r$ is task-specific.
Instead of imposing some restrictions such as low-rank or inter-task regularization the idea is to impose the coupling by directly placing a model $w$ that is common to all tasks. The $v_r$ part is added so each model can be adapted to the specific task.
%

Multiple extensions of the work of~\cite{EvgeniouP04} have been presented: in~\cite{XuAQZ14, LiTST15} the method is extended to the Proximal SVM~\cite{FungM01} and Least Squares SVM~\cite{SuykensV99}, respectively. Also, in~\cite{ParameswaranW10} the idea is adapted for the Large Margin Nearest Neighbor model~\cite{WeinbergerS09}.
%
% Evgeniou, T., Micchelli, C. A., and Pontil, M. (2005).  Learning multiple tasks with kernel methods.
% MTLSVM and Generalized SMO
However, in this work we are interested mainly in two extensions: one is the work of~\cite{EvgeniouMP05} and the other is developed in~\cite{LiangC08, CaiC09}, both will be described in detail in Section~\ref{sec:ch3_mtl_kernelmethods}.
Using the unified formulation we can express the more general primal problem as
\begin{equation}
    \nonumber
    \begin{aligned}
        & \argmin_{w, b, v_r, b_r, \xi_i^r}
        & & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \xi_i^r + \frac{1}{2} \dotp{w}{w} + \frac{\mu}{2} \sum_{r=1}^\ntasks \dotp{v_r}{v_r} \\
        & \text{s.t.}
        & & y_{i} ( \dotp{w}{\phi(x_{i}^r)} + b + \dotp{v_r}{\phi_r(x_{i}^r)} + b_r) \geq p_i^r - \xi_i^r ,\\
        & & &\xi_i^r \geq 0, \\
        & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
    \end{aligned}
\end{equation}
where $\mu$ is a tradeoff parameter to leverage the common and task-specific information.

% Eigenfunction-Based Multitask Learning in a Reproducing Kernel Hilbert Space ?


\section{Multi-Task Problems}
% Explanation of an MTL problem, with its characteristics
In Table~\ref{tab:mtl_problems} we show some of the most used multi-task learning problems, exposing its characteristics and the papers that use them.

% Explanation of datasets
\newpage
%\begin{center}
    \begin{longtable}{llllll}
        
    \hline \multicolumn{1}{c}{\textbf{Name}} & \multicolumn{1}{c}{\textbf{$\nsamples$}} & \multicolumn{1}{c}{\textbf{$\dimx$}} & \multicolumn{1}{c}{\textbf{$T$}} & \multicolumn{1}{c}{\textbf{Goal}} & \multicolumn{1}{c}{\textbf{References}} \\ \hline 
    \endfirsthead
    
    \multicolumn{6}{c}%
    {{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
    \hline \multicolumn{1}{c}{\textbf{Name}} & \multicolumn{1}{c}{\textbf{$\nsamples$}} & \multicolumn{1}{c}{\textbf{$\dimx$}} & \multicolumn{1}{c}{\textbf{$T$}} & \multicolumn{1}{c}{\textbf{Goal}} & \multicolumn{1}{c}{\textbf{References}} \\ \hline 
    \endhead
    
    \hline \multicolumn{6}{|r|}{{Continued on next page}} \\ \hline
    \endfoot
    
    \hline \hline
    \endlastfoot    
    \multirow{13}{*}{\fdata{school}} & \multirow{13}{*}{15362} & \multirow{13}{*}{27} & \multirow{13}{*}{139} & \multirow{13}{*}{single-reg}  & \multirow{9}{*}{}~\cite{EvgeniouP04} \\ &&&&&~\cite{EvgeniouMP05} \\ &&&&&~\cite{ArgyriouEP06,ArgyriouMPY07,ArgyriouEP08} \\ &&&&&~\cite{BonillaCW07} \\ &&&&&~\cite{ZhangY10}  \\ &&&&&~\cite{AgarwalDG10}  \\ &&&&&~\cite{ChenZY11}   \\ &&&&&~\cite{ZhouCY11} \\ &&&&&~\cite{GongYZ12rmfl} \\ &&&&&~\cite{KumarD12}  \\ &&&&&~\cite{ZhangY13a}  \\ &&&&&~\cite{HanZ16} \\ &&&&&~\cite{JeongJ18}  \\ [3.0ex]
    \multirow{2}{*}{\fdata{20-newsgroup}} & \multirow{2}{*}{18000} & \multirow{2}{*}{t.v.} & \multirow{2}{*}{20} & \multirow{2}{*}{multi-clas}  & \multirow{2}{*}{}~\cite{AndoZ05} \\ &&&&&~\cite{Daume09} \\ [3.0ex]
    \multirow{2}{*}{\fdata{Reuters-RCV1}} & \multirow{2}{*}{800000} & \multirow{2}{*}{t.v.} & \multirow{2}{*}{103} & \multirow{2}{*}{multi-clas}  & \multirow{2}{*}{}~\cite{YuTS05}  \\ &&&&&~\cite{AndoZ05} \\ [3.0ex]
    \multirow{6}{*}{\fdata{computer}} & \multirow{6}{*}{3600} & \multirow{6}{*}{13} & \multirow{6}{*}{180} & \multirow{6}{*}{single-reg}  & \multirow{6}{*}{}~\cite{ArgyriouEP06,ArgyriouMPY07,ArgyriouEP08} \\ &&&&&~\cite{EvgeniouMP05}  \\ &&&&&~\cite{AgarwalDG10} \\ &&&&&~\cite{KumarD12} \\ &&&&&~\cite{JeongJ18} \\ [3.0ex]
    \multirow{4}{*}{\fdata{landmine}} & \multirow{4}{*}{14820} & \multirow{4}{*}{10} & \multirow{4}{*}{29} & \multirow{4}{*}{bin-clas}  & \multirow{4}{*}{}~\cite{XueLCK07} \\ &&&&&~\cite{Jebara11} \\ &&&&&~\cite{Daume09}  \\ &&&&&~\cite{JawanpuriaN12}  \\ [3.0ex]
    \multirow{2}{*}{\fdata{MHC-I}} & \multirow{2}{*}{32302} & \multirow{2}{*}{184} & \multirow{2}{*}{47} & \multirow{2}{*}{bin-clas}  & \multirow{2}{*}{}~\cite{JacobBV08} \\ &&&&&~\cite{JawanpuriaN12} \\ [3.0ex]
    \multirow{2}{*}{\fdata{dermatology}} & \multirow{2}{*}{366} & \multirow{2}{*}{33} & \multirow{2}{*}{6} & \multirow{2}{*}{multi-clas}  & \multirow{2}{*}{}~\cite{Jebara04} \\ &&&&&~\cite{ArgyriouEP08} \\ [3.0ex]
    \multirow{5}{*}{\fdata{sentiment}} & \multirow{5}{*}{2000} & \multirow{5}{*}{473856} & \multirow{5}{*}{4} & \multirow{5}{*}{bin-clas}  & \multirow{5}{*}{}~\cite{Daume09} \\ &&&&&~\cite{ZhangY10} \\ &&&&&~\cite{CrammerM12} \\ &&&&&~\cite{ZhangY13a} \\ &&&&&~\cite{BarzilaiC15} \\ [3.0ex]
    \\ \\ 
    \multirow{6}{*}{\fdata{sarcos}} & \multirow{6}{*}{44484} & \multirow{6}{*}{21} & \multirow{6}{*}{7} & \multirow{6}{*}{multi-reg}  & \multirow{6}{*}{}~\cite{ZhangY10} \\ &&&&&~\cite{ChenZY11} \\ &&&&&~\cite{ZhouCY11} \\ &&&&&~\cite{JawanpuriaN12} \\ &&&&&~\cite{ZhangY13a} \\ &&&&&~\cite{CilibertoMPR15} \\ [3.0ex]
    \multirow{2}{*}{\fdata{isolet}} & \multirow{2}{*}{7797} & \multirow{2}{*}{617} & \multirow{2}{*}{5} & \multirow{2}{*}{MT multi-clas}  & \multirow{2}{*}{}~\cite{ParameswaranW10} \\ &&&&&~\cite{GongYZ12} \\ [3.0ex]
%    \multirow{2}{*}{\fdata{coil}} & \multirow{2}{*}{340} & \multirow{2}{*}{17} & \multirow{2}{*}{7} & \multirow{2}{*}{single-reg}  & \multirow{2}{*}{}~\cite{ParameswaranW10} \\ &&&&& - \\ [3.0ex]
    \multirow{4}{*}{\fdata{mnist}} & \multirow{4}{*}{70000} & \multirow{4}{*}{400} & \multirow{4}{*}{10} & \multirow{4}{*}{multi-clas}  & \multirow{4}{*}{}~\cite{KangGS11} \\ &&&&&~\cite{KumarD12} \\ &&&&&~\cite{ZweigW13} \\ &&&&&~\cite{JeongJ18} \\ [3.0ex]
    \multirow{4}{*}{\fdata{usps}} & \multirow{4}{*}{9298} & \multirow{4}{*}{256} & \multirow{4}{*}{10} & \multirow{4}{*}{multi-clas}  & \multirow{4}{*}{}~\cite{KangGS11} \\ &&&&&~\cite{KumarD12} \\ &&&&&~\cite{ZweigW13} \\ &&&&&~\cite{JeongJ18} \\ [3.0ex]
    \multirow{2}{*}{\fdata{adni}} & \multirow{2}{*}{675} & \multirow{2}{*}{306} & \multirow{2}{*}{6} & \multirow{2}{*}{single-reg}  & \multirow{2}{*}{}~\cite{GongYZ12} \\ &&&&&~\cite{GongYZ12rmfl} \\ [3.0ex]
    \multirow{2}{*}{\fdata{microarray}} & \multirow{2}{*}{131} & \multirow{2}{*}{21} & \multirow{2}{*}{19} & \multirow{2}{*}{multi-reg}  & \multirow{2}{*}{}~\cite{LozanoS12} \\ &&&&&~\cite{HanZ16} \\ [3.0ex]
    \multirow{2}{*}{\fdata{cifar10}} & \multirow{2}{*}{50000} & \multirow{2}{*}{1024} & \multirow{2}{*}{10} & \multirow{2}{*}{multi-clas}  & \multirow{2}{*}{}~\cite{ZweigW13} \\ &&&&&~\cite{HanZ16} \\ [3.0ex]
    \multirow{2}{*}{\fdata{parkinson}} & \multirow{2}{*}{5875} & \multirow{2}{*}{19} & \multirow{2}{*}{42} & \multirow{2}{*}{single-reg}  & \multirow{2}{*}{}~\cite{JawanpuriaN12} \\ &&&&&~\cite{JeongJ18} \\ [3.0ex]
    \caption{Multi-Task Learning Problems. The columns show the number of samples $n$, the dimension $d$, the number of tasks $T$ and the goal. The goal can be: single-reg, that is regression of a single target, multi-reg, that is multi-output regression, bin-clas, that is binary classification, multi-clas, that is multi-class classification that is transformed into a binary classification problem for each task, and MT multi-task classification, proper multi-task classification with multiple classes.} \label{tab:mtl_problems}
    \end{longtable}
    %\end{center}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%%%%%%%%%             SECTION         %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Why does Multi-Task Learning work?}
\comm{TODO: Repasar sección entera después escribir capítulo 2}
% First works in Learning to Learn
% Overview of section
 % Ruder and Caruana
% 
\subsection{Learning to Learn: A notion of environment of tasks} % Baxter
% The first theoretical work on MTL... MTL as a Bias Learning Problem
Tipically in Machine Learning the goal is to find the best hypothesis $\hyp{x}{\param_0}$ from a space of hypothesis $\hypspace = \set{\hyp{x}{\param}, \param \in \paramspace}$, where $\paramspace$ is any set of parameters. This best candidate can be selected according to different inductive principles, which define a method of approximating a global function $f(x)$ from a training set:
$ \sample \defeq \set{(x_i, y_i),\; \idotsn} $
where $(x_i, y_i)$ are sampled from a distribution $\distf$.
%
In the classical statistics we find the Maximum Likelihood approach, where the goal is to estimate the density $\fun{x} = \cond{y}{x}$ and the hypothesis space is parametric, i.e. $\hypspace = \set{\hyp{x}{\param}, \param \in \paramspace \subset \reals^m}$. The learner select the parameter $\param$ that maximizes the probability of the data given the hypothesis.
%
Another more direct inductive principle is Empirical Risk Minimization (ERM), which is the most common one. In ERM the densities are ignored and an empirical error $\emprisk$ is minimized with the hope of minimizing the true expected error $\exprisk$, which would result in a good generalization. 
%
Several models use the ERM principle to generalize from data such as Neural Networks or Support Vector Machines. These methods are designed to find a good hypothesis $\hyp{x}{\param}$ from a given space $\hypspace$. The definition of such space $\hypspace$ define the bias for these problems. If $\mathcal{H}$ does not contain any good hypothesis, the learner will not be able to learn.
The best hypothesis space we can provide is the one containing only the optimal hypothesis, but this is equivalent the original problem. When we only want to estimate a single function $f(x)$, a single-task scenario, there is no difference between learning the optimal hypothesis space (bias learning) and ordinary learning of the optimal hypothesis function.
Instead, we focus on the situation where we want to solve multiple related tasks, that is, estimating multiple functions $f_1(x), \ldots, f_\ntasks(x)$. In that case, we can obtain a good space $\hypspace$ that contains good solutions for the different tasks.
%
In~\cite{baxter2000model} an effort is made to define the concepts needed to construct the theory about inductive bias learning or Learning to Learn, which can be seen as a generalization of strict Multi-Task Learning. This is done by defining an environment of tasks and extending the work of~\cite{Vapnik00}, which defines the capacity of space of hypothesis, Baxter defines the capacity of a family of spaces of hypothesis.

% Review of concepts for STL in Supervised Learning
Before presenting the concepts defined for Bias Learning, and to establish an analogy to those of ordinary learning, we briefly review some statistical learning concepts.
\subsubsection*{Ordinary Learning}
In the ordinary statistical learning, some theoretical concepts are used:
\begin{itemize}
    \item an \emph{input space} $\Xspace$ and an \emph{output space} $\Yspace$,
    \item a \emph{probability distribution} $\distf$, which is unknown, defined over $\Xspace \times \Yspace$,
    \item a \emph{loss function} $\loss{\cdot}{\cdot}:\Yspace \times \Yspace \to \reals$, and
    \item a \emph{hypothesis space} $\hypspace = \set{\hyp{x}{\param}, \param \in \paramspace \subset \reals^m}$ with hypothesis $\hyp{\cdot}{\param}: \Xspace \to \Yspace$.
\end{itemize}
The goal for the learner is to select a hypothesis $\hyp{x}{\param} \in \mathcal{H}$, or equivalently $\param \in \paramspace$, that minimizes the expected risk
$$ \exprisk(\param) =  \int_{\Xspace \times \Yspace} \loss{\hyp{x}{\param}}{y} d\distf(x, y) .$$
The distribution $\distf$ is unknown, but we have a training set $\sample = \{(x_1, y_1), \ldots, (x_\nsamples, y_\nsamples)\}$ of samples drawn from $\distf$. 
The approach is then is to apply the ERM inductive principle, that is to minimize the empirical risk
$$ \emprisk(\param) = \frac{1}{\nsamples} \sum_{i=1}^\nsamples l(h(x_i), y_i).$$
Thus, a learner $\mathcal{A}$ maps the set of training samples to a set of hypothesis:
\begin{equation}
    \nonumber
    \mathcal{A} : \bigcup {(\Xspace \times \Yspace)^\nsamples} \to \hypspace.
\end{equation}
Although $\emprisk(\param)$ is an unbiased estimator of $\exprisk(\param)$, it has been shown~\cite{Vapnik00} that this approach, despite being the most evident one, is not the best principle that can be followed.
This has relation with two facts: the first one is that the unbiased property is an asymptotical one, the second one has to do with overfitting.
Vapnik answers to the question of what can be said about $\exprisk$ when $\param$ minimizes $\emprisk(\param)$, and moreover, his results are valid also for small number of training samples $n$.
More specifically, Vapnik sets the sufficient and necessary conditions for the consistency of an inductive learning process, i.e. for $\emprisk(\param) \toprob \exprisk(\param) $ uniformly. Vapnik also defines the capacity of a hypothesis space and use it to derive bounds on the rate of this convergence for any $\param \in \paramspace$ and, more importantly, bounds on the difference $\inf_{\param \in \paramspace} \emprisk(\param) - \inf_{\param \in \paramspace} \exprisk(\param)$.
Under some general conditions, he proves that
\begin{equation}\label{eq:ordinary_generalization_bound}
    \inf_{\param \in \paramspace} \emprisk(\param) - \inf_{\param \in \paramspace} \exprisk(\param) \leq B(\nsamples/\vcdim{\hypspace})
\end{equation}
where $B$ is some non-decreasing function and $\vcdim{\hypspace}$ is the capacity of the space $\hypspace$, also named the VC-dimension $\hypspace$. This means that the generalization ability of a learning process can be controlled in terms of two factors:
\begin{itemize}
    \item The number of training samples $\nsamples$. A greater number of training samples assures a better generalization of the learning process.This looks intuitive and could be already inferred from the asymptotical properties. 
    \item The VC-dimension $\vcdim{\hypspace}$ of the hypothesis space $\hypspace$, which is desirable to be small. This term is not intuitive and is the most important term in Vapnik theory.
\end{itemize}
The VC-dimension measures the capacity of a set of hypothesis $\hypspace$. 
%In the case of a set of indicator functions, it is the maximum number of vectors $x_1, \ldots, x_\vcdim{\hypspace}$ that can be shattered (in two classes) by functions of this set. In the case of real functions, it is defined as the VC-dimension of the following set of indicator functions $ I(x, \param, \beta) = \myvec{1}_{\{\hyp{x}{\param} - \beta\}} $.
If the capacity of the set $\hypspace$ is too large, we may find a
hypothesis $\hyp{x}{\opt{\param}}$ that minimizes $\emprisk$ but does not 
generalize well and therefore, does not minimize $\exprisk$. This is the 
overfitting problem. 
On the other side, if we use a simple $\hypspace$, 
with low capacity, we could be in a situation where there is not a good hypothesis $\hyp{x}{\param} \in \hypspace$, so the empirical risk $\inf_{\param \in \paramspace} \exprisk$ is too large. This is the underfitting problem.

% The Structural Risk Minimization (SRM) as an inductive principle, proposed by Vapnik (as opposed to the ERM), tries to find a tradeoff between minimizing $\emprisk$ and minimizing $\vcdim{\hypspace}$. The idea is to define an admissible structure, that is a sequence of hypothesis spaces:
% $$ \mathcal{H}_1 \subset \mathcal{H}_2 \subset \ldots \subset \mathcal{H}_k \subset \ldots $$ 
% where their VC-dimensions are ordered:
% $$ d_1 < d_2 < \ldots < d_k < \ldots$$
% where $d_i$ is the VC-dimension of $\mathcal{H}_i$.
% SRM selects the hypothesis $\hyp{x}{\param^*} = \hyp{x}{\param_i^*} \in \hypspace_i$ that obtains the best bound for the actual risk $\exprisk$.
% This admissible structure can be built in various ways. In Neural Networks in can be the built by increasing the number of hidden layers. In other methods, such as SVM or Ridge Regression, this is done by decreasing the regularization.
% However, this SRM principle is usually replaced by a cross-validation (CV) procedure.

% Support Vector Machines, which are the most representative models of this theory, use the VC-dimension also in other way (apart from the SRM principle or the CV procedure). The goal of finding the optimal hyperplane, that is, that with the maximum margin between the classes, has its motivation in the fact the set of such type of hypothesis have a lower VC-dimension that the set of all hyperplanes do.


% Extension to MTL
\subsubsection*{Bias Learning: Concept and Components}
In ~\cite{baxter2000model} two main concepts are presented: the \emph{family of hypothesis spaces} and an \emph{environment} of related tasks. 
For simplicity we write $\hypf(x)$  instead of $\hypf(x, \param)$, and since $\param$ completely defines $\hypf$, we also substitute $\param$ by $\hypf$ for an easier notation.
Using these concepts, the bias learning problem has the following components:
\begin{itemize}
    \item an \emph{input space} $\Xspace$ and an \emph{output space} $\Yspace$,
    \item an \emph{environment} $(\bprobspace, \bdistf)$ where $\bprobspace$ is a set of distributions $P$ defined over $\Xspace \times \Yspace$, and we can sample from $\bprobspace$ according to a distribution $\bdistf$,
    \item a \emph{loss function} $\loss{\cdot}{\cdot}:\Yspace \times \Yspace \to \reals$, and
    \item a \emph{family of hypothesis spaces} $\hypspacef = \set{\hypspace_\param, \param \in \paramspace}$, where each element $\hypspace_\param$ is a set of hypothesis.
\end{itemize}
Analogous to ordinary learning, the goal is to minimize the expected risk, defined as
\begin{equation}\label{eq:biaslearn_exprisk}
    \bexprisk(\param) = \int_{\bprobspace} \inf_{\hypf \in \hypspace_\param} \risk_P(\hypf) d\bdistf(P) = \int_{\bprobspace} \inf_{\hypf \in \hypspace_\param} \int_{\Xspace \times Y} l(h(x), y) dP(x, y) d\bdistf(P).
\end{equation}
Again, we do not know $\bprobspace$ nor $\bdistf$, but we have a training set samples from the environment $(\bprobspace, \bdistf)$ obtained in the following way:
\begin{enumerate}
    \item Sample $T$ times from $\bdistf$ obtaining $P_1, \ldots, P_T \in \bprobspace$
    \item For $r=1, \ldots, T$ sample $m$ pairs $\sample_r = \{(x_1^r, y_1^r), \ldots, (x_m^r, y_m^r)\}$ according to $P_r$ where $(x_i^r, y_i^r) \in X \times Y$ .
\end{enumerate}
We obtain a sample $z=\{(x_i^r, y_i^r), r=1,\; i=1, \ldots, \;m=1, \ldots, T\}$, with $\npertask$ examples from $\ntasks$ different learning tasks, and
\begin{equation}
    \nonumber
    \bsample \defeq 
    \begin{matrix}
        (x_1^1, y_1^1) & \ldots & (x_m^1, y_m^1) \\
        \vdots & \ddots & \vdots \\
        (x_1^\ntasks, y_1^\ntasks) & \ldots & (x_m^\ntasks, y_m^\ntasks) \\
    \end{matrix}
\end{equation}
is named as a $(\ntasks, \npertask)$-sample.
Using $\bsample$ we can define the empirical loss as
\begin{equation}\label{eq:biaslearn_emprisk}
    \bemprisk(\param) = \sum_{r=1}^\ntasks \inf_{\hypf \in \hypspace_\param} \hat{\risk}_{\sample_r}(\hypf) = \sum_{r=1}^\ntasks \inf_{\hypf \in \hypspace_\param} \sum_{i=1}^m l(\hypf(x_i^r), y_i^r),
\end{equation}
which is an average of the empirical losses of each task. 
Note, however, that in the case of the bias learner, this estimate is biased, since $\risk_{P_r}(\hypf)$ does not coincide with $\hat{\risk}_{\sample_r}(\hypf)$. 
Putting all together, a bias learner $\mathcal{A}$ maps the set of all $(\ntasks, \npertask)$-samples to a family of hypothesis spaces:
\begin{equation}
    \nonumber
    \mathcal{A} : \bigcup {(\Xspace \times \Yspace)^{(\ntasks, \npertask)}} \to \hypspacef.
\end{equation}
%

To follow an analogous path to that of ordinary learning, the milestones in bias learning theory should include:
\begin{itemize}
    \item Checking the consistency of the Bias Learning methods, i.e. proving that $\bemprisk(\param)$ converges uniformly in probability to $\bexprisk(\param)$.
    \item Defining a notion of capacity of hypothesis space families $\hypspacef$.
    \item Finding a bound of $\bemprisk(\param) - \bexprisk(\param)$ for any $\param$ using the capacity of the hypothesis space family. If possible, finding also a bound for $\inf_{\param \in \paramspace}\bemprisk(\param) - \inf_{\param \in \paramspace} \bexprisk(\param)$.
\end{itemize}
To achieve these goals some previous definitions are needed. From this point, since any $\hypspace$ is defined by a $\param \in \paramspace$, we omit $\param$ and write just $\hypspace$ for simplicity.
%
\subsubsection*{Bias Learning: Capacities and Uniform Convergence}
% Pseudo-metrics, Covering numbers and Capacities
In first place, a \emph{sample-driven} pseudo-metric of $(\ntasks, 1)$-empirical risks is defined.
Consider a sequence of $\ntasks$ probabilities $\bprobseq = (P_1, \ldots, P_\ntasks)$ sampled from $\bprobspace$ according the the distribution $\bdistf$. 
\begin{definition}[\emph{sample-driven} pseudometric]
    \label{def:sample_pseudometric}
    Consider also the set of sequences of $\ntasks$ hypothesis 
$$\hypspace^\ntasks \defeq \set{ \myvec{\hypf} = (\hypf_1, \ldots, \hypf_\ntasks) , \hypf_1, \ldots, \hypf_\ntasks \in \hypspace} .$$
We can define then the set of $(\ntasks, 1)$-empirical risks as 
$$\hypspace^\ntasks_\lossf \defeq \set{ \myvec{\hypf}_\lossf(x_1, y_1, \ldots, x_\ntasks, y_\ntasks) = \sum_{r=1}^\ntasks \lossf(\hypf(x_i), y_i) , \hypf_1, \ldots, \hypf_\ntasks \in \hypspace} $$
The family of the set of $\ntasks$-risks of hypothesis is then $\bsetsample = \bigcup_{\hypspace \in \hypspacef} \hypspace^\ntasks$. Now we can define
\begin{equation}
    \nonumber
    \begin{aligned}
        \dist{\bprobseq}(\myvec{\hypf}_\lossf, \myvec{\hypf'}_\lossf) = \int_{(\Xspace \times \Yspace)^{\ntasks}} &\abs{\myvec{\hypf}_\lossf(x_1, y_1, \ldots, x_\ntasks, y_\ntasks) - \myvec{\hypf'}_\lossf(x_1, y_1, \ldots, x_\ntasks, y_\ntasks)} \\ 
        & d{P_1}(x_1, y_1) \ldots d{P_\ntasks}(x_\ntasks, y_\ntasks)
    \end{aligned}
\end{equation}
for $\myvec{\hypf}_\lossf, \myvec{\hypf'}_\lossf \in \hypspace_\lossf, \hypspace'_\lossf$ as a pseudo-metric in $\hypspacef^\ntasks$.
\end{definition}
%

Then, a \emph{distribution-driven} pseudo-metric is defined. 
\begin{definition}[\emph{distribution-driven} pseudo-metric]
    \label{def:dist_pseudometric}
    Given a distribution $P$ on $\Xspace \times \Yspace$. Consider the set of infimum expected risk for each $\hypspace$:
\begin{equation}
    \nonumber
    \hypspace^* \defeq \inf_{\hypf \in \hypspace} \risk_P(\hypf).
\end{equation}
The family of such sets is defined as 
$\bsetdist = \set{\hypspace^*, \hypspace \in \hypspacef}$.
The pseudo-metric in this space is given by $d_Q$:
\begin{equation}
    \nonumber
    \dist{\bdistf} = \int_\bprobspace \abs{\hypspace_1^* - \hypspace_2^*} d\bdistf
\end{equation}
\end{definition}
for $\hypspace_1^*, \hypspace_2^* \in \hypspacef^*$.
With these two pseudo-metrics, two capacities for families of hypothesis spaces are defined. For that the definition of $\epsilon$-cover is needed. 
\begin{definition}[$\epsilon$-cover]
    \label{def:epsilon_cover}
    Given a pseudo-metric $\dist{S}$ in a space $\mathcal{S}$, 
a set of $l$ elements $s_1, \ldots, s_l \in \mathcal{S}$ is an $\epsilon$-cover of $\mathcal{S}$ if 
$ \forall s \in S$ $\dist{S}(s, s_i) \leq \epsilon $
for some $i=1, \ldots, l$.  Let $\mathcal{N}(\epsilon, \mathcal{S}, \dist{S})$ denote the size of the smallest $\epsilon$-cover.
\end{definition}
%
Then, we can define the following capacities of a family space $\hypspacef$:
\begin{itemize}
    \item The \emph{sample-driven capacity} $\capacity{\epsilon}{\bsetsample} \defeq \sup_{\bprobseq} \mathcal{N}(\epsilon, \hypspacef^\ntasks, \dist{\bprobseq})$.
    \item The \emph{distribution-driven capacity} $\capacity{\epsilon}{\bsetdist} \defeq \sup_Q \mathcal{N}(\epsilon, \bsetdist, \dist{\bdistf})$.
\end{itemize}

% Uniform Convergence for bias learners (Comparison with Vapnik)
Using these capacities, the convergence (uniformly over all $\hypspace \in \hypspacef$) of bias learners can be proved~\cite[Theorem~2]{baxter2000model}. Moreover, the bias expected risk is bounded
\begin{equation}
    \nonumber
    \bemprisk(\hypspace) \leq \bexprisk(\hypspace) + \epsilon
\end{equation}
with probability $1 - \eta$, given sufficiently large $\ntasks$ and $\npertask$, 
\begin{equation}
    \nonumber
    \ntasks \geq \max \left( \frac{256}{\ntasks \epsilon^2} \log\frac{8\capacity{\frac{\epsilon}{32}}{\bsetdist}}{\eta} , \frac{64}{\epsilon^2}\right)  , \; \npertask \geq \max \left( \frac{256}{\ntasks \epsilon^2} \log\frac{8\capacity{\frac{\epsilon}{32}}{\bsetsample}}{\eta} , \frac{64}{\epsilon^2}\right) .
\end{equation}
It should be noted that the bound for $\npertask$ is inversely proportional to $\ntasks$, that is, the more tasks we have, the less samples we need for each task. 


% Multi-Task Learning (strictly speaking, with fixed tasks)
\subsubsection*{Multi-Task Learning}
The previous result is a result for pure Bias Learning, where we have an $(\bprobspace, \bdistf)$-environment of tasks. In Multi-Task Learning, we have a fixed number of tasks $\ntasks$ and a fixed sequence of distributions $\bprobseq = (P_1, \ldots, P_\ntasks)$, where $P_i$ is a distribution over $(\Xspace \times \Yspace)^\npertask$. The goal is not learning a hypothesis space $\hypspace$ but a sequence of hypothesis $\myvec{\hypf} = (\hypf_1, \ldots, \hypf_\ntasks), \hypf_1, \ldots, \hypf_\ntasks \in \hypspace $. Thus, the Multi-Task expected risk is
\begin{equation}\label{eq:mtlearn_exprisk}
    \risk_{\bprobseq}(\myvec{\hypf}) = \sum_{r=1}^\ntasks \risk_{P_r}(\hypf_r)  = \sum_{r=1}^\ntasks \int_{\Xspace \times Y} l(\hypf_r(x), y) d{P_r}(x, y),
\end{equation}
and the empirical risk is defined as
\begin{equation}\label{eq:mtlearn_emprisk}
    \bemprisk(\myvec{\hypf}) = \sum_{r=1}^\ntasks \hat{\risk}_{\sample_r}(\hypf_r) = \sum_{r=1}^\ntasks \sum_{i=1}^m l(\hypf_r(x_i^r), y_i^r).
\end{equation}
A similar result to that of Bias Learning is given for Multi-Task Learning~\cite[Theorem~4]{baxter2000model}:
\begin{equation}
    \nonumber
    \bemprisk(\myvec{\hypf}) \leq \risk_{\bprobseq}(\myvec{\hypf}) + \epsilon,
\end{equation}
with probability $1 - \eta$ given that the number of samples per task
\begin{equation}
    \nonumber
    \npertask \geq \max \left( \frac{64}{\ntasks \epsilon^2} \log\frac{4\capacity{\frac{\epsilon}{16}}{\bsetsample}}{\eta} , \frac{16}{\epsilon^2}\right).
\end{equation}
Observe that we do not need the \emph{distribution-driven} capacity in this case, just the \emph{sample-driven} capacity.
% Feature Learning
\subsubsection*{Feature Learning}
Feature Learning is a common way to encode bias. The most popular example are Neural Networks, where all the hidden layers can be seen as a Feature Learning engine that learns a mapping from the original space to a space with ''strong'' features.
In general, a set of ''strong'' feature maps is defined as $\mathcal{F} = \set{f, f: \Xspace \to \Fspace}$. Using these features, functions $g \in \mathcal{G}$ (which are tipically simple) are built:
$\Xspace \to_f \Fspace \to_g \Yspace$.
Thus, for each map $f$, the hypothesis space can be expressed as 
$\hypspace_f = \set{h = \mathcal{G} \circ f, g \in \mathcal{G}}$
, and the family of hypothesis spaces is 
$\hypspacef = \set{\hypspace_f, f \in \mathcal{F}}$.
Now, the Bias Learning problem is the problem of finding a good mapping $f$.
It is proved~\cite[Theorem~6]{baxter2000model} that in the Feature Learning case the capacities of $\hypspacef$ can be bounded by the capacities of $\mathcal{F}$ and $\mathcal{G}$ as
\begin{align*}
    \capacity{\epsilon}{\bsetsample} &\leq \capacity{\epsilon_1}{\mathcal{G}^\ntasks}^\ntasks \capf_{\mathcal{G}_\lossf}(\epsilon_2, \mathcal{F}) , \\
    \capacity{\epsilon}{\bsetdist} &\leq \capf_{\mathcal{G}_\lossf}(\epsilon, \mathcal{F})
\end{align*}
with $\epsilon = \epsilon_1 + \epsilon_2 $. Here, $\capf_{\mathcal{G}_\lossf}(\epsilon, \mathcal{F})$ is defined as 
$\capf_{\mathcal{G}_\lossf}(\epsilon, \mathcal{F}) \defeq \sup_P \mathcal{N}(\epsilon, \mathcal{F}, \dist{[P, \mathcal{G}_\lossf]})$, where
$$ \dist{[P, \mathcal{G}_\lossf]}(f, f') = \int_{\Xspace \times \Yspace} sup_{g \in \mathcal{G}} \abs{\loss{g \circ f(x)}{y} - \loss{g \circ f'(x)}{y}} dP(x, y)$$
is a pseudo-metric. Using these results alongside those presented for Bias Learning is useful to establish bounds for Feature Learning models like Neural Networks.

% Generalized VC-dim , Theorems 12, 13 and Corollary 13
\subsubsection*{Generalized VC-Dimension for Multi-Task Learning}
The concepts presented until now rely on the concepts of two capacities of a family of hypothesis spaces $\hypspacef$ to establish bounds in the difference $\bemprisk(\myvec{\hypf}) - \bexprisk(\myvec{\hypf})$, that is, the probability of deviations between the empirical and expected risks for a given hypothesis sequence. However, it would be more useful to find some result concerning the empirical error and the \emph{best expected error}.
To achieve this, a generalized VC-dimension is developed in~\cite{baxter2000model} for Multi-Task Learning with Boolean hypothesis.
%
\begin{definition}\label{def:gen_vcdim}
    Let $\hypspace$ be a space of boolean functions and $\hypspacef$ a boolean hypothesis space family. Denote the set of $\ntasks \times \npertask$ matrices in $\Xspace$ as $\Xspace^{\ntasks \times \npertask}$
For each $\mymat{x} \in \Xspace^{\ntasks \times \npertask}$ and each $\hypspace \in \hypspacef$ define the set of binary $\ntasks \times \npertask$ matrices
\begin{equation}
    \nonumber
    \hypspace_{\vert \mymat{x}} \defeq \set{\begin{pmatrix}
        \hypfun{x_1^1} & \ldots & \hypfun{x_\npertask^1} \\
        \vdots & \ddots & \vdots \\
        \hypfun{x_1^\ntasks} & \ldots & \hypfun{x_\npertask^\ntasks} \\
    \end{pmatrix}, \hypf \in \hypspace} ,
\end{equation}
and the corresponding family of such sets as
\begin{equation}
    \nonumber
    \hypspacef_{\vert \mymat{x}} = \bigcup_{\hypspace \in \hypspacef}  \hypspace_{\vert \mymat{x}}.
\end{equation}
For each $\ntasks, \npertask \geq 0$ define the number of binary matrices obtainable with $\hypspacef$ as
\begin{equation}
    \nonumber
    \Pi_\hypspacef(\ntasks, \npertask) \defeq \max_{\mymat{x} \in \Xspace^{\ntasks \times \npertask}} \cardinal{ \hypspacef_{\vert \mymat{x}} }.
\end{equation}
Note that $\Pi_\hypspacef(\ntasks, \npertask) \leq 2^{\ntasks \npertask}$ and if $\Pi_\hypspacef(\ntasks, \npertask) = 2^{\ntasks \npertask}$ we say that $\hypspacef$ shatters $\Xspace^{\ntasks \times \npertask}$.
For each $\ntasks > 0$ define
\begin{align}
    d_\hypspacef(\ntasks) &\defeq \max_{m: \Pi_\hypspacef(\ntasks, \npertask) = 2^{\ntasks \npertask}} m , \nonumber 
\end{align}
Here, $d_\hypspacef(\ntasks)$ is the generalized VC-dimension.
Also define
\begin{align}
    \overline{d}(\hypspacef) &\defeq \vcdim{\hypspacef^1} = \vcdim{\bigcup_{\hypspace \in \hypspacef} \hypspace}  ,  \nonumber \\
    \underline{d}(\hypspacef) &\defeq \max_{\hypspace \in \hypspacef} \vcdim{\hypspace}  . \nonumber
\end{align}
\begin{equation}
    \nonumber
    d_\hypspacef(\ntasks) \geq \max\left(\floor*{\frac{\overline{d}(\hypspacef)}{\ntasks}}, \underline{d}(\hypspacef) \right)
\end{equation}
where it can be observed that 
\begin{equation}
    \label{eq:genvc_inequalities}
    \overline{d}(\hypspacef) \geq  d_\hypspacef(\ntasks)\geq \underline{d}(\hypspacef).
\end{equation}

\end{definition}
\begin{proof}
    Let split the proof in three inequalities:
    \begin{itemize}
        \item  $ d_\hypspacef(\ntasks) \geq \underline{d}(\hypspacef)$  \\ 
        Let $\hypspace^\text{max}$ be such that $\vcdim{\hypspace^\text{max}} = \underline{d}(\hypspacef) = \max_{\hypspace \in \hypspacef} \vcdim{\hypspace}$. Then we can construct a ${\ntasks \times \underline{d}(\hypspacef)}$ matrix $\mymat{X}$ where each row can be shattered by $\hypspace^\text{max}$ and, thus,
        $ \Pi_{\left\lbrace \hypspace^\text{max} \right\rbrace}(\ntasks, \npertask) \geq \ntasks  \underline{d}(\hypspacef)$. %\comm{(es esto verdad?)}
        Also, for all $\ntasks, \npertask \geq 0$
        \begin{equation}
            \nonumber
            \Pi_\hypspacef(\ntasks, \npertask) = \max_{\mymat{x} \in \Xspace^{\ntasks \times \npertask}} \cardinal{ \hypspacef_{\vert \mymat{x}} } \geq \max_{\mymat{x} \in \Xspace^{\ntasks \times \npertask}} \cardinal{ \hypspace^\text{max}_{\vert \mymat{x}} } =  \Pi_{\left\lbrace \hypspace^\text{max} \right\rbrace}(\ntasks, \npertask) .
        \end{equation}
        Then, since $\set{m:  \Pi_{\left\lbrace \hypspace^\text{max} \right\rbrace}(\ntasks, \npertask) = 2^{\ntasks \npertask}} \subset \set{m: \Pi_\hypspacef(\ntasks, \npertask) = 2^{\ntasks \npertask}}$,
        \begin{equation}
            \nonumber
            d_\hypspacef(\ntasks) = \max_{m: \Pi_\hypspacef(\ntasks, \npertask) = 2^{\ntasks \npertask}} m \geq \max_{m:  \Pi_{\left\lbrace \hypspace^\text{max} \right\rbrace}(\ntasks, \npertask) = 2^{\ntasks \npertask}} m \geq \underline{d}(\hypspacef) .
        \end{equation}
        %\comm{Esta primera desigualdad creo que no está bien}
        \item $ d_\hypspacef(\ntasks) \geq \floor*{\frac{\overline{d}(\hypspacef)}{\ntasks}} $ \\
        Given a sequence $x_1, \ldots, x_{\overline{d}(\hypspacef)}$ shattered by $\hypspacef^1$ we can build a ${\ntasks \times \floor*{\frac{\overline{d}(\hypspacef)}{T}} }$ matrix $\mymat{x}$ that is shattered by $\hypspacef^1$ and therefore 
        $ d_\hypspacef(\ntasks) = \max_{m: \Pi_\hypspacef(\ntasks, \npertask) = 2^{\ntasks \npertask}} m \geq \floor*{\frac{\overline{d}(\hypspacef)}{T}}.$
        
        \item  $ d_\hypspacef(\ntasks) \leq \overline{d}(\hypspacef)$ \\
        We prove the stronger result $ d_\hypspacef(\ntasks) \leq \ceil*{\frac{\overline{d}(\hypspacef)}{\ntasks}} $. Suppose that $ d_\hypspacef(\ntasks) > \ceil*{\frac{\overline{d}(\hypspacef)}{\ntasks}} $, then there exists a $\ntasks \times d_\hypspacef(\ntasks)$ matrix that is shattered by $\hypspacef^1$, then, $\vcdim{\hypspacef^1} \geq \ntasks  d_\hypspacef(\ntasks) > \overline{d}(\hypspacef)$, which is a contradiction.

    \end{itemize}
\end{proof}


% Look at Ben-David version
Now we can present the relevant result expressed in~\cite[Corollary~13]{baxter2000model}.
\begin{theorem}\label{th:baxter_vcdim}
    Given a sequence $\bprobseq = (P_1, \ldots, P_\ntasks)$ on $(\Xspace \times \set{0, 1})^\ntasks$, and a sample $\bsample$ from this distribution. Consider also a sequence $\myvec{\hypf} = (\hypf_1, \ldots, \hypf_\ntasks)$ of boolean hypothesis $\hypf_i \in \hypspace$, then for every $\epsilon > 0$
% \begin{equation}
%     \nonumber
%     \bexprisk(\myvec{\hypf}) \leq \bemprisk(\myvec{\hypf}) + \epsilon,
% \end{equation}
\begin{equation}
    \nonumber
    \abs{\risk_{\bprobseq}(\myvec{\hypf}) - \bemprisk(\myvec{\hypf})} \leq \epsilon,
\end{equation}
with probability $1 - \eta$ given that the number of samples per task
\begin{equation}
    \label{eq:bound_npertask_genvcdim}
    \npertask \geq \frac{88}{\epsilon^2} \left[2 d_\hypspacef(\ntasks) \log \frac{22}{\epsilon} + \frac{1}{\ntasks}\log\frac{4}{\eta} \right] .
\end{equation}
\end{theorem}
Here, since $d_\hypspacef(\ntasks) \geq d_\hypspacef(\ntasks+1)$, it is easy to see that as the number of task $\ntasks$ increases, the number of examples needed per task can decrease. 
Moreover, as shown in~\cite[Theorem~14]{baxter2000model}, if this bound on $\npertask$ is not fulfilled, then we can always find a sequence of distributions $\bprobseq$ such that
\begin{equation}
    \nonumber
    \inf_{\hypf \in \hypspace} \bemprisk(\myvec{\hypf}) > \inf_{\hypf \in \hypspace} \risk_{\bprobseq}(\myvec{\hypf}) + \epsilon .
\end{equation}
With this results we can see that the condition~\eqref{eq:bound_npertask_genvcdim} has some important properties:
\begin{itemize}
    \item It is a computable bound, given that we know how to compute $d_\hypspacef(\ntasks)$.
    \item It provides a sufficient condition for the uniform convergence (in probability) of the empirical risk to the expected risk.
    \item It provides a necessary condition for the consistency of Multi-Task Learners, i.e. uniform convergence of the best empirical risk to the best expected risk.
\end{itemize}

% \subsubsection*{Conclusion}
% In~\cite{baxter2000model} several new concepts are developed. The $(\bprobspace, \bdistf)$-environment of tasks is useful to characterize the concept of related tasks. Moreover, using this definition, Baxter is able to give some important results of uniform convergence in the Bias Learning paradigm. From this general view, Multi-Task Learning
% is a particular case and the uniform convergence results are also valid. The Feature Learning approach, which can be seen as a more particular method of Multi-Task Learning has some interesting results splitting the analysis into the feature learning process and the construction of models over these features. Finally, the most important result is the definition of a generalized VC-dimension and the uniform convergence of Multi-Task Learning models using this concept. Although this is a result only valid for boolean hypothesis, it helps to shed some light on Multi-Task Learning and the reasons of its effectiveness.

\subsection{Learning with Related Tasks} % Ben-David
% Baxter gives the foundation for a theoretical work of a framework where the tasks share a common learning bias...
Using the work of~\cite{baxter2000model} as the foundation, several important notions and results are presented in~\cite{Ben-DavidB08} for boolean hypothesis functions defined over $\Xspace \times \set{0, 1}$.
% task relatedness
One of the main contributions of this work is a notion of task relatedness. In~\cite{baxter2000model} the tasks are related by sharing a common inductive bias that can be learned. In~\cite{Ben-DavidB08} a precise mathematical definition for task relatedness is given.
% task individual risks
The other important contribution is the focus on the individual risk of each task. In~\cite{baxter2000model} all the results are given for the Multi-Task empirical and expected risks, which are an average of the risks of each task. However, bounding this average does not bound the risk of each particular task. This is specially relevant if we are in a Transfer Learning scenario, where there is a target task that we want to solve and the remaining tasks can be seen as an aid to improve the performance in the target.

% F-related tasks
\subsubsection*{A Notion of Task Relatedness: $\frelset$-Related Tasks}
The main concept for the theory developed in~\cite{Ben-DavidB08} is a set of $\frelset$ of transformations $\frelf: \Xspace \to \Xspace$. Given a probability distribution $\distf$ over $\Xspace \times \set{0, 1}$, a set of tasks with distributions $P_1, \ldots, P_\ntasks$ are $\frelset$-related if, for each task there exists some $\frelf_i \in \frelset$ such that $P_i = \frelf_i(\distf)$.

\begin{definition}[$\frelset$-related task]\label{def:frel_tasks}
    Consider a measurable space $(\Xspace, \mathcal{A})$ and the corresponding measurable product space $(\Xspace \times \set{0, 1}, \mathcal{A} \times \powerset{\set{0, 1}})$. Consider $P$ a probability distribution over this product space and a function $\frelf: \Xspace \to \Xspace$, then we define the distribution $\frel{P}$ such that for any $S \in \mathcal{A}$,
    $$ \frel{P}(S) \defeq P(\set{(f(x), b), (x, b) \in S }).$$
    %
    Let $\frelset$ be a set of transformations $\frelf: \Xspace \to \Xspace$, and let $P_1, P_2$ be distributions over $(\Xspace \times \set{0, 1}, \mathcal{A} \times \powerset{\set{0, 1}})$, then the distributions $P_1, P_2$ are $\frelset$-related if $\frel{P_1}= P_2$ or $\frel{P_2} = P_1$ for some $\frelf \in \frelset$.
    % \begin{itemize}
    %     \item The distributions $P_1, P_2$ are $\frelset$-related if $\frel{P_1}= P_2$ or $\frel{P_2} = P_1$ for some $\frelf \in \frelset$.
    %     \item Two samples $\sample_1, \sample_2$ sampled from $P_1, P_2$ respectively are $\frelset$-related if $P_1, P_2$ are $\frelset$-related.
    % \end{itemize}
\end{definition}
This notion establishes a clear definition of related tasks but we are interested in how a learner can use this relatedness to improve the learning process.
For that, considering that $\frelset$ is a group under function composition, we regard at the action of the group $\frelset$ over the set of hypothesis $\hypspace$. This action defines the following equivalence relation in $\hypspace$:
$$ \hypf_1 \sim_\frelset \hypf_2 \iff \exists \frelf \in \frelset,  \hypf_1 \circ \frelf = \hypf_2 .$$
%
This equivalence relation defines equivalence classes $\equivclass{\hypf}$, that is let $h' \in \hypspace$ be an hypothesis, then $h' \in \equivclass{\hypf}$ iff $h' \sim_\frelset h$. 
We consider the quotient space 
$$\hypspace_\frelset \defeq \hypspace / \sim_\frelset = \set{\equivclass{\hypf}, \hypf \in \hypspace}.$$
It is important to observe that $\hypspace_\frelset = \hypspacef'$ is a hypothesis space family, since it is a set of equivalence classes $\equivclass{\hypf} = \hypspace'$, which are set of hypothesis.
%

\subsubsection*{The Multi-Task Empirical Risk Minimization}
This equivalence classes are useful to divide the learning process in two stages, this is called the \emph{Multi-Task ERM}. Consider the samples $\sample_1, \ldots, \sample_\ntasks$ from $T$ different tasks, then
\begin{enumerate}
    \item Select the best hypothesis class $\equivclass{\hypf^\frelset} \in \hypspace_\frelset$:
    \begin{equation}
        \nonumber
        \equivclass{\hypf^\frelset} \defeq \min_{\equivclass{\hypf} \in \hypspace_\frelset} \inf_{\hypf_1, \ldots, \hypf_\ntasks \in \equivclass{\hypf}} \frac{1}{\ntasks} \sum_{r=1}^\ntasks \hat{\risk}_{\sample_i}(\hypf_i) ,
    \end{equation}
    \item Select the best hypothesis $h^\diamond$ for the target task (without loss of generality, consider the first one):
    \begin{equation}
        \nonumber
        h^\diamond = \inf_{\hypf \in \equivclass{\hypf^\frelset}} \hat{\risk}_{\sample_1}(\hypf) .
    \end{equation}
\end{enumerate}

For example, consider the handwritten digits recognition problem, we might integrate $T$ different datasets designed in different conditions. Each dataset have been created using certain conditions of light and some specific scanner for getting the images. Even different pens or pencils might be influential in the stroke of the numbers. All these conditions are the $\frelset$ transformations, and each $\frelf \in \frelset$ generate a different bias for the dataset. However, there exists a probability for ''pure'' digits, e.g. the pixels of digit one have higher probability around a line in the middle of the picture than in the sides. This ``pure'' probability distribution $P_0$ and all the distributions $P_1, \ldots, P_T$ from which our datasets have been sampled might be $\frelset$-related among them and with $P_0$. If we first determine the $\frelset$-equivalent class of hypothesis $\equivclass{\hypf}$ suited for digit recognition in the first stage, then it will be easier to select $\hypf_1, \ldots, \hypf_\ntasks \in \equivclass{\hypf}$ for each dataset in the second one.

% Results for F-related tasks (Theorems 2, 3)
\subsubsection*{Bounds for $\frelset$-Related Tasks}
% Relation with Baxter work
The results of Theorem~\ref{th:baxter_vcdim} can be applied to the hypothesis quotient space of equivalent classes $\hypspace_\frelset$. However the following results is needed first.
%
Let $P_1, P_2$ be $\frelset$-related distributions, then this statement can be proved~\cite[Lemma~2]{Ben-DavidB08}:
\begin{equation}
    \label{eq:ben-david_lemma2}
    \inf_{\hypf \in \hypspace} \risk_{P_1}(\hypf) = \inf_{\hypf \in \hypspace} \risk_{P_2}(\hypf).
\end{equation}
This indicates that the the expected risk is invariant under transformations of $\frelset$.
Now, one of the main results~\cite[Theorem~2]{baxter2000model} can be given.
\begin{theorem}\label{th:ben-david_th2}
    Let $\frelset$ be a set of transformations $\frelf: \Xspace \to \Xspace$ that is a group under function composition. Let $\hypspace$ be a hypothesis space so that $\frelset$ acts as a group over $\hypspace$, and consider the quotient space $\hypspace_\frelset = \set{\equivclass{\hypf}, \hypf \in \hypspace}$.
    Consider $\bprobseq = (P_1, \ldots, P_\ntasks)$ a sequence of $\frelset$-related distributions over $\Xspace \times \set{0, 1}$, and $\myvec{z} = (\sample_1, \ldots, \sample_\ntasks)$ the corresponding sequence of samples  where $\sample_i$ is sampled using $P_i$, then for every $\equivclass{\hypf} \in \hypspace_\frelset$ and $\epsilon > 0$ 
    \begin{equation}
        \nonumber
        \abs{\inf_{\hypf_1, \ldots, \hypf_\ntasks \in \equivclass{\hypf}} \frac{1}{\ntasks} \sum_{r=1}^\ntasks \hat{\risk}_{\sample_r}(\hypf_r) - \inf_{\hypf' \in \equivclass{\hypf}}\risk_{P_1}(\hypf')}  \leq \epsilon
    \end{equation}
    with probability greater than $\eta$ if the number of samples from each distribution satisfies
    \begin{equation}
        \label{eq:ben-david_sample_inequality}
        \cardinal{\sample_i} \geq  \frac{88}{\epsilon^2} \left[2 d_{\hypspace_\frelset}(\ntasks) \log \frac{22}{\epsilon} + \frac{1}{\ntasks}\log\frac{4}{\eta} \right].
    \end{equation}
\end{theorem}
Note that, in contrast to Theorem~\ref{th:baxter_vcdim}, this result bounds the expected risk of a single task, not the average risk. This is the consequence of applying Theorem~\ref{th:baxter_vcdim} and substituting the average empirical error using the result from~\eqref{eq:ben-david_lemma2}.
Also observe that here the hypothesis space family used is the quotient space $\hypspace_\frelset$, and the VC-dimension of such family is used.
%
Using this result, a bound for learners using the Multi-Task ERM principle is given~\cite[Theorem~3]{Ben-DavidB08}
\begin{theorem}\label{th:ben-david_th3}
    Consider $\frelset$ and $\hypspace$ as in the previous theorem. Consider also the previous sequences of distributions $(P_1, \ldots, P_\ntasks)$ and corresponding samples $(\sample_1, \ldots, \sample_\ntasks)$. Consider $\underline{d}(\hypspace_\frelset) = \max_{\hypf \in \hypspace} \vcdim{\equivclass{\hypf}}$.
    Let $h^\diamond$ be the hypothesis selected using the Multi-Task ERM principle, then for every $\epsilon_1, \epsilon_2 > 0$
    \begin{equation}
        \nonumber
        {\hat{\risk}_{\sample_1}(\hypf^\diamond) - \inf_{\hypf' \in \hypspace}\risk_{P_1}(\hypf')}  \leq 2(\epsilon_1 + \epsilon_2)
    \end{equation}
    with probability greater than $\eta$ if
    \begin{equation}
        \label{eq:ben-david_th3_ineq1}
        \cardinal{\sample_1} \geq  \frac{64}{\epsilon^2} \left[2 \underline{d}(\hypspace_\frelset) \log \frac{12}{\epsilon} + \frac{1}{\ntasks}\log\frac{8}{\eta} \right], \; 
    \end{equation}
    and for $i \neq 1$
    \begin{equation}
        \label{eq:ben-david_th3_ineq2}
        \cardinal{\sample_i} \geq  \frac{88}{\epsilon^2} \left[2 d_{\hypspace_\frelset}(\ntasks) \log \frac{22}{\epsilon} + \frac{1}{\ntasks}\log\frac{8}{\eta} \right] .
    \end{equation}
\end{theorem}
The idea of the proof of this theorem helps to understand how using different tasks can help to improve the performance in the target task. 
Consider $\hypf^* = \inf_{\hypf \in \hypspace} \risk_{P_1}(\hypf)$ the best hypothesis for the $P_1$ distribution.
According to Theorem~\ref{th:ben-david_th2}, for $\equivclass{\hypf^*}$ we have that
\begin{equation}
    \nonumber
    {\inf_{\hypf_1, \ldots, \hypf_\ntasks \in \equivclass{\hypf^*}} \frac{1}{\ntasks} \sum_{r=1}^\ntasks \hat{\risk}_{\sample_r}(\hypf_r) \leq \inf_{\hypf' \in \equivclass{\hypf^*}}\risk_{P_1}(\hypf')}  + \epsilon_1.
\end{equation}
%
Also, in the first stage of Multi-Task ERM principle, we select the hypothesis class $\equivclass{\hypf^\frelset}$ that minimizes $\inf_{\myvec{\hypf} \in \equivclass{\hypf}} \risk_{\bprobseq}(\myvec{\hypf})$ where $\myvec{\hypf}$ is a sequence of hypothesis of $\hypspace_\frelset$.
According to Theorem~\ref{th:ben-david_th2}, for $\equivclass{\hypf^\frelset}$ we have that
\begin{equation}
    \nonumber
    {\inf_{\hypf' \in \equivclass{\hypf^\frelset}}\risk_{P_1}(\hypf')} \leq \inf_{\hypf_1, \ldots, \hypf_\ntasks \in \equivclass{\hypf^\frelset}} \frac{1}{\ntasks} \sum_{r=1}^\ntasks \hat{\risk}_{\sample_r}(\hypf_r)  + \epsilon_1.
\end{equation}
Using these two inequalities we get
\begin{equation}
    \nonumber
    \inf_{\hypf' \in \equivclass{\hypf^\frelset}}\risk_{P_1}(\hypf') \leq \inf_{\hypf' \in \equivclass{\hypf^*}}\risk_{P_1}(\hypf') + 2\epsilon_1 
\end{equation}
under the condition~\eqref{eq:ben-david_sample_inequality}. This bounds the risk of the hypothesis space given by the equivalence class of $\hypf^\frelset$ and establishes the inequality~\eqref{eq:ben-david_th3_ineq2}.
%

Once we select $\equivclass{\hypf^\frelset}$, the second stage is just ERM using this hypothesis space.
%
According to the~\cite{vapnik1982estimation},
\begin{equation}\nonumber
    \inf_{\hypf \in \hypspace} \risk_{z_1}(\hypf) - \inf_{\hypf \in \hypspace} \risk_{P_1}(\hypf) \leq \epsilon_2
\end{equation}
if
\begin{equation}
    \nonumber
    \cardinal{\sample_1} \geq  \frac{64}{\epsilon^2} \left[2 \vcdim{\hypspace} \log \frac{12}{\epsilon} + \frac{1}{\ntasks}\log\frac{8}{\eta} \right] .
\end{equation}
Since the ERM will not use the whole space $\hypspace$ but the subset $\equivclass{\hypf^\frelset} \subset \hypspace$, and
$$\vcdim{\equivclass{\hypf^\frelset}} \leq \max_{\equivclass{\hypf} \in \hypspace_\frelset} \vcdim{\equivclass{\hypf}} = \underline{d}(\hypspace_\frelset).$$
 then we can write the inequality~\eqref{eq:ben-david_th3_ineq1} of the theorem.
%
The advantage of using multiple tasks is then illustrated in this bound and it will be defined by the gap between $\vcdim{\hypspace}$ and $\underline{d}(\hypspace_\frelset)$. If $\underline{d}(\hypspace_\frelset)$ is smaller than $\vcdim{\hypspace}$, the number of samples needed to solve the target task will also be smaller.
Also, the sample complexity of the rest of tasks is given by $d_{\hypspace_\frelset}(\ntasks)$.

That is, Multi-Task Learning allows to select a subset of hypothesis from which a learner can use the ERM principle. In this stage, the sample complexity is controlled by the generalized VC-dimension of the set of equivalent classes of hypothesis. Once the best equivalent class has been selected, 
the VC-dimension of this subset, compared to the VC-dimension of the whole set of hypothesis, is what marks the difference between Single Task and Multi-Task Learning.

% Analysis of generalized VCdim
\subsubsection*{Analysis of generalized VC-dimension with $\frelset$-related tasks}
As we have seen in Theorem~\ref{th:ben-david_th3}, the VC-dimensions $\vcdim{\hypspace}, \underline{d}(\hypspace_\frelset)$ and $d_{\hypspace_\frelset}(\ntasks)$ are crucial for stating the advantage of Multi-Task over Single Task Learning. 
To understand better how these concepts interact, Ben-David et al. give some theoretical results.
Recall that, given a hypothesis space $\hypspace$, $\hypspace_\frelset$ is a family of hypothesis spaces composed by the hypothesis spaces $\equivclass{\hypf}, \hypf \in \hypspace$, then
\begin{align*}
    \nonumber
    d_{\hypspace_\frelset}(\ntasks) &= \max_{\set{\npertask ,\Pi_{\hypspace_\frelset} = 2^{\ntasks \npertask}} } \npertask, \\
    \underline{d}(\hypspace_\frelset) &= max_{\hypf \in \hypspace} \vcdim{\equivclass{\hypf}}, \\    
    \overline{d}(\hypspace_\frelset) &= \vcdim{\bigcup_{\equivclass{\hypf} \in \hypspace_\frelset} \equivclass{\hypf}}= \vcdim{\hypspace}.
\end{align*}
Using the result from~\eqref{eq:genvc_inequalities} we observe that
\begin{equation}
    \nonumber
    \underline{d}(\hypspace_\frelset) \leq d_{\hypspace_\frelset}(\ntasks) \leq \vcdim{\hypspace} .
\end{equation}
That is, the best we can hope when bounding the sample complexity in Theorem~\ref{th:ben-david_th3} is $\underline{d}(\hypspace_\frelset) = d_{\hypspace_\frelset}(\ntasks)$. 
Ben-David et al. give evidence that, with some restrictions on $\hypspace$, this lower bound can be achieved~\cite[Theorem~4]{Ben-DavidB08}.
\begin{theorem}
    If the support of $\hypf$ is bounded, i.e. $ \cardinal{\set{x \in \Xspace,  \hypf(x) = 1}} < M$, for all $\hypf \in \hypspace$, then there exists $\ntasks_0$ such that for all $\ntasks>\ntasks_0$
    \begin{equation}
        \nonumber
        d_{\hypspace_\frelset}(\ntasks) = \underline{d}(\hypspace_\frelset).
    \end{equation} 
\end{theorem}
Thus, a sufficient condition on the hypothesis space $\hypspace$ to achieve the lowest $d_{\hypspace_\frelset}(\ntasks)$ is a bounded support of any hypothesis. Although this condition may be too restricting, it can also be proved that the upper limit of $d_{\hypspace_\frelset}(\ntasks)$, that is, $\vcdim{\hypspace}$, under some conditions on $\frelset$ is not achieved. 

The following result~\cite[Theorem~6]{Ben-DavidB08} shows this.
\begin{theorem}\label{th:ben-david_th6}
    If $\frelset$ is finite and $\frac{\ntasks}{\log (\ntasks)} \geq \vcdim{\hypspace}$, then
    \begin{equation}
        \nonumber
        d_{\hypspace_\frelset}(\ntasks) \leq 2 \log(\cardinal{\frelset})
    \end{equation}
\end{theorem}
This inequality indicates that, given a finite set of transformation $\frelset$, there are scenarios when $\vcdim{\hypspace}$ is arbitrarily large but $d_{\hypspace_\frelset}(\ntasks)$ is bounded, and therefore, the right-hand side of inequality~\eqref{eq:ben-david_th3_ineq2} is also bounded. That is, the Multi-Task bound, which substitutes $\vcdim{\hypspace}$ by $d_{\hypspace_\frelset}(\ntasks)$ is a better one in this cases. 

% \subsubsection*{Conclusion}
% \comm{TODO?}


\subsection{Other bounds for Multi-Task Learning}
\comm{TODO: Completar después de escribir capítulo 2. Usar discusión de cotas en Maurer et al. 2016 para estructurar la subsección.}
The work of Baxter~\cite{baxter2000model} set the foundations for the theoretical analysis of MTL. 
In this work, an MTL extension to the VC-dimension is given, and it is used to develop some results bounding the difference between the Multi-Task empirical and expected risks 
\begin{equation}
    \nonumber
    \abs{\risk_{\bprobseq}(\myvec{\hypf}) - \bemprisk(\myvec{\hypf})}
\end{equation}
for any sequence of hypothesis $\myvec{\hypf} \in \hypspace^\ntasks$, see Theorem~\ref{th:baxter_vcdim}. This is a necessary condition for the consistency of Multi-Task Learners, but not a sufficient one.
Then, Ben-David et al.~\cite{Ben-DavidS03,Ben-DavidB08} defines a notion of task relatedness, see Definition~\ref{def:frel_tasks}. Using this notion and building an appropiate hypothesis space $\hypspace = \equivclass{h}$, they bound the \emph{excess risk} of an Empirical Multi-Task Learner, that is, the difference between the best empirical risk, achieved by such Learner, and the best possible expected risk (Theorem~\ref{th:ben-david_th3})
\begin{equation}
    \nonumber
    \abs{\inf_{\myvec{\hypf} \in \hypspace^\ntasks} \risk_{\bprobseq}(\myvec{\hypf}) - \inf_{\myvec{\hypf} \in \hypspace^\ntasks} \bemprisk(\myvec{\hypf})} .
\end{equation}
Moreover, using this task relatedness definition not only the Multi-Task average excess risk is bounded, but the individual excess risk of each task (Theorem~\ref{th:ben-david_th6}).

%
The works discussed until this point on the VC-dimension, and the corresponding extensions to the MTL framework expressed in Definition~\ref{def:gen_vcdim}, to bound the differences between empirical and expected risks.
%
However in~\cite{AndoZ05}, Ando and et al. rely on another notion  of complexity, the Rademacher Complexity~\cite{BartlettM02}, which measures how well a family of hypothesis can approximate random noise. The Rademacher complexity, unlike the VC-dimension, is distribution-dependent. That is the VC-dimension only uses properties of the hypothesis space $\hypspace$, while the Rademacher complexity also depends on $\distf$. In first place, they use two pseudo-metrics different to those of Definitions~\ref{def:sample_pseudometric} or~\ref{def:dist_pseudometric} and they define the corresponding covering numbers. Then they bound the covering numbers of the set of hypothesis used in~\eqref{eq:mtl_struct_learn} using the Rademacher complexity of those sets of functions. 
However, this bounds are also dependent on the dimensions $k$ and $\dimx$, which make them unfeasible for kernel extensions.





To control the complexity or VC-dimension of the hypothesis space regularization is used, so bounds for regularized methods are also of interest.

In~\cite{Maurer06}, Maurer improves the bounds from~\cite{baxter2000model} and~\cite{Ben-DavidS03,Ben-DavidB08} in the particular case of using a feature extractor $g$, i.e. $\hypf_r = f_r \circ g$.
Then, in~\cite{ArgyriouMP09} an extension of the spectral theorem to vector-valued function is shown. 
%This is useful to compute the\comm{TODO}
\comm{TODO: meter a maurer2009}

In~\cite{PontilM13} bounds for trace-norm regularized methods are given.

Then, in~\cite{MaurerPR16} bounds for the Multi-Task Representation Learning (MTRL)~\cite{ArgyriouEP06} are given. That is, methods where the group sparse regularization $\norm{\mymat{W}^\intercal}_{2, 1}$ is used. Moreover, this bounds are not limited to MTL but are also valid for the Learning to Learn paradigm.


\subsection{Learning Under Privileged Information}\label{subsec:ch3_lupi}
Another important motivation for Multi-Task Learning, specially for Joint Learning, can be found in the Learning Under Privileged Information paradigm.
% The classical learning paradigm tries to minimize the expected risk by minimizing the empirical risk...
The standard machine learning paradigm tries to find the hypothesis $\hypf$ from a set of hypothesis $\hypspace$ that minimizes the expected risk $\emprisk$ given a set of training samples.
% Vapnik gives develops the theory of Statisical Learning Theory and it seems complete
Vapnik is one of the main contributors to the theory of statistical learning~\cite{Vapnik00}. In this theory several important results are provided: necessary and sufficient conditions for the consistency of learning processes and bounds for the rate of convergence, which uses the notion of VC-dimension. A new inductive principle, Structural Risk Minimization (SRM), and an algorithm, Support Vector Machine (SVM), that makes use of this notion to improve the learning process.

% However, machines need many examples to learn. What is lacking?
% An Intelligent Teacher is important in human learning, providing additional information: examples, metaphors...
Nowadays learning approaches based on Deep Neural Networks, which are not focused on controlling the capacity of the set of hypothesis, outperform the SVM approaches in many problems. However, these popular Deep Learning approaches require large amounts of data to learn good hypothesis.
It is commonly believed that machines need much more samples to learn than humans do. Vapnik~\cite{VapnikV09, VapnikI15a} reflects on this belief and states that humans typically learn under the supervision of an Intelligent Teacher.
This Teacher shares important knowledge by providing metaphors, examples or clarifications that are helpful for the students.



\subsubsection*{LUPI Paradigm}
% How can that additional information be incorporated in Machine Learning?
The additional knowledge provided by the Teacher is the Privileged Information that is available only during the training stage.
To incorporate the concept of Intelligent Teacher in the Machine Learning framework, Vapnik introduces the paradigm of Learning Under Privileged Information (LUPI).
% LUPI
% Definition and notation
In the LUPI paradigm describes the following model. Given a set of i.i.d. triplets
$$ z = \set{(x_1, x^*_1, y_1), \ldots, (x_\nsamples,x^*_\nsamples, y_\nsamples)}, \; x \in \Xspace, x^* \in \Xspace^*, y \in \Yspace $$
generated according to an unknown distribution $P(x, x^*, y)$, the goal is to find the hypothesis $\hyp{x}{\param^*}$ from a set of hypothesis $\hypspace = \set{\hyp{x}{\param}, \param \in \paramspace}$ that minimizes some expected risk 
$$ \exprisk = \int \loss{\hyp{x}{\param}}{y} dF(x, y). $$

Note that the goal is the same that in the standard paradigm, however with the LUPI approach we are provided additional information, which is available only during the training stage. This additional information is encoded in the elements $x^*$ of a space $\Xspace^*$, which is different from $\Xspace$. The goal of the Teacher is, given a pair $(x_i, y_i)$, to provide a useful information $x^* \in \Xspace^*$ given some probability $\cond{x^*}{x}$. That is, the ''intelligence'' of the Teacher is defined by the choice of the space $\Xspace^*$ and the conditional probability $\cond{x^*}{x}$. 
% Examples
To understand better this paradigm consider the following example.
\\
% Example of doctor: images and commentaries
\textbf{Example.} Consider that the goal is to find a decision rule that classifies biopsy images into cancer or non-cancer. Here, $\Xspace$ is the space of images, i.e. the matrix of pixels, for example $[0, 1]^{64 \times 64}$. The label space is $\Yspace = \set{0, 1}$. An Intelligent Teacher might provide a student of medicine with commentaries about the images, for example: ''There is an area of unusual concentration of cells of Type A.'' or ''There is an aggresive proliferation of cells of Type B''. These commentaries are the elements $x^*$ of certain space $X^*$ and the Teacher also chooses the probability $\cond{x^*}{x}$.
% \\
% % Example of high quality and low quality images
% \textbf{Example 2.} Consider the \comm{TODO} 

\subsubsection*{Analysis of convergence rates}
To get a better insight of how the Privileged Information can help in the Learning process, Vapnik provides a theoretical analysis of its influence on the learning rates.
In the standard learning paradigm, how well the expected risk $\exprisk$ can be bounded is controlled by two factors: the empirical risk $\emprisk$ and the VC-dimension of the set of hypothesis $\mathcal{\hypspace}$.
In the case of classification, where $\Yspace = \set{-1, 1}$ and the loss $\lossf(\hyp{x}{\param}, y) = \ind_{\hyp{x}{\param} y \leq 0}$, the risks can be expressed as
\begin{align*}
    \nonumber
    \exprisk (\param) &= \int \ind_{y \hyp{x}{\param} \leq 0} d\distf(x, y) =  P(\hyp{x}{\param} y \leq 0) ,\\
    \emprisk(\param) &= \sum_{i=1}^\nsamples \ind_{y_i \hyp{x_i}{\param} \leq 0} = \nu(\param) .
\end{align*}
In~\cite[Theorem~6.8]{vapnik1982estimation} the following bound for the rate of convergence is given with probability $1 - \eta$:
\begin{equation}
    \nonumber
    P(\hyp{x}{\param_\nsamples} y \leq 0) \leq \nu(\param_\nsamples) + \bigO{\frac{\vc \log\left(\frac{2\nsamples}{\vc} \right) - \log\eta}{\nsamples} \sqrt{\nu(\param_\nsamples) \frac{\nsamples}{\vc \log\left(\frac{2\nsamples}{\vc} \right) - \log\eta}} } .
\end{equation}
That is, the bound is controlled by the ratio $\vc/\nsamples$, where $\vc$ is the $\vcdim{\hypspace}$. If this $VC$-dimension is finite, the bound goes to zero as $\nsamples$ grows.
However, two different cases can be considered.
\\
% Convergence of separable hypothesis

\textbf{Separable case:} the training data can be classified in two groups without errors. That is, there exists $\param_\nsamples \in \eta$ such that $y_i \hyp{x_i}{\param_\nsamples} > 0$ for $i = 1, \ldots, \nsamples$, and thus $\nu(\param_\nsamples) = 0$.
In this case, the following bound for the rate of converge holds
\begin{equation}
    \nonumber
    P(\hyp{x}{\param_\nsamples} y \leq 0) \leq \bigO{\frac{\vc \log\left(\frac{2\nsamples}{\vc} \right) - \log\eta}{\nsamples} } .
\end{equation}
\\
% Convergence of non-separable
\textbf{Non-Separable case:} the training data cannot be classified in two groups without errors. That is, for all $\param_\nsamples \in \paramspace$, there exists $i = 1, \ldots, \nsamples$, such that $y_i \hyp{x_i}{\param_\nsamples} \leq 0$, and thus $\nu(\param_\nsamples) > 0$.
In this case, the following bound for the rate of converge holds
\begin{equation}
    \nonumber
    P(\hyp{x}{\param_\nsamples} y \leq 0) \leq \nu(\param_\nsamples) + \bigO{\sqrt{\frac{\vc \log\left(\frac{2\nsamples}{\vc} \right) - \log\eta}{\nsamples}} } .
\end{equation}
Note that there is an important difference here in the rate of convergence. The separable case has a convergence rate of $\vc/\nsamples$, while the non-separable case has a rate of $\sqrt{\vc/\nsamples}$. Vapnik tries to address the question of why there exists such difference.

% Oracle SVM: separable
\subsubsection*{Oracle SVM}
Vapnik tries to answer these question by looking at Support Vector Machines. In the separable case, one has to minimize the functional 
$$J(\hplane) = \norm{\hplane}^2$$
subject to the constraints
$$y_i \left( \hplane x_i + \bias \right) \geq 1.$$
However, in the non-separable case the functional to minimize is 
$$J(\hplane, \xi_1, \ldots, \xi_\nsamples) = \norm{\hplane}^2 + C \sum_{i=1}^\nsamples \xi_i$$
subject to the constraints
$$y_i \left( \hplane x_i + b \right) \geq 1 - \xi_i.$$
That is, in the separable case $\dimx$ parameters (of $\hplane$) have to be estimated using $\nsamples$ examples, while in the non-separable case $\dimx + \nsamples$ parameters (considering $\hplane$ and the slack variables $\xi_1, \ldots, \xi_\nsamples$) have to be estimated with $\nsamples$ examples. 

What would happen if the parameters $\xi_1, \ldots, \xi_\nsamples$ were known?
In~\cite{VapnikI15a} an \emph{Oracle} SVM is considered. Here, the learner (Student) is supplied with a set of triplets
\begin{equation}
    \nonumber
    (x_1, \xi_1^0, y_1), \ldots, (x_\nsamples, \xi_\nsamples^0, y_\nsamples)
\end{equation}
where $\xi^0_1, \ldots, \xi^0_\nsamples$ are the slack variables for the best decision rule $\hyp{x}{\param_0} = \inf_{\param \in \paramspace} \exprisk(\param) $:
\begin{equation}
    \nonumber
    \xi_i^0 = \max \left(0, 1 - \hyp{x}{\param_0}  \right), \; \forall i = 1, \ldots, \nsamples .
\end{equation}
An \emph{Oracle} SVM has to minimize the functional
$$J(\hplane) = \norm{\hplane}^2$$
subject to the constraints
$$y_i \left( \hplane x_i + \bias \right) \geq 1 - \xi_i^0 .$$
Since the slack variables $\xi_i^0$ are known in advance, it can be shown~\cite{VapnikV09} that for the \emph{Oracle} SVM the following bound holds
\begin{equation}
    \nonumber
    P(\hyp{x}{\param_\nsamples} y \leq 0) \leq P(1 - \xi^0 \leq 0) + \bigO{\frac{\vc \log\left(\frac{2\nsamples}{\vc} \right) - \log\eta}{\nsamples} } ,
\end{equation}
where $P(1 - \xi^0 \leq 0)$ is the probability error of the hypothesis $\hyp{x}{\param_0}$.
That is, we recover the rate $\vc/\nsamples$.

% Intelligent Teacher: separable but we need to control two models
\subsubsection*{From Oracle to Intelligent Teacher}
The \emph{Oracle} SVM is a theoretical construct, but we can approximate it by modelling the slack variables with the information provided by the Teacher in the LUPI paradigm.
That is, the Teacher defines a space $\Xspace^*$ and a set of functions $\set{f^*(x, \param^*) , \param^* \in \paramspace^*}$. Then model the slack variables as
\begin{equation}
    \nonumber
    \xi^* = f^*(x^*, \param^*) .
\end{equation}
From the pairs generated by some random generator in nature, the Teacher also defines the probability $\cond{x^*}{x}$ to provide the triplets
\begin{equation}
    \nonumber
    (x_1, x^*_1, y_1), \ldots, (x_\nsamples, x^*_\nsamples, y_\nsamples) .
\end{equation}
Then, we can consider the problem where the goal is to minimize
$$ J(\param, \param^*) = \sum_{i=1}^\nsamples \max(0, f^*(x_i^*, \param^*)) $$
subject to the constraints
$$ \hyp{x_i}{\param} \geq 1 - f^*(x_i^*, \param^*).$$
Let $f(x, \param_\nsamples), h(x. \param_\nsamples)$ that minimize this problem. Then, in~\cite[Proposition~2]{VapnikV09} the following results for the bound of convergence is given
\begin{equation}
    \nonumber
    P(\hyp{x}{\param_\nsamples} y \leq 0) \leq P(1 - f^*(x^*, \param^*_\nsamples) \leq 0) + \bigO{\frac{(\vc + \vc^*) \log\left(\frac{2\nsamples}{(\vc + \vc^*)} \right) - \log\eta}{\nsamples} } ,
\end{equation}
where $d^*$ is the VC-dimension of the space of hypothesis $\set{f(x, \param^*) \in \paramspace^*}$. This result shows that, to maintain the best convergence rate $\vc/\nsamples$, we need to estimate the $P(1 - f^*(x^*, \param^*_\nsamples) \leq 0)$. Although this probability is unknown,  we can control it. Considering
\begin{equation}
    \nonumber
    \param^*_0 = \inf_{\param^* \in \paramspace^*} \int_{\Xspace^*} \max(0, f^*(x^*, \param^*)- 1) dP(x^*)
\end{equation}
and
\begin{equation}
    \nonumber
    \param^*_\nsamples = \inf_{\param^* \in \paramspace^*} \sum_{i=1}^\nsamples \max(0, f^*(x^*_i, \param^*)- 1) .
\end{equation}
Consider $\set{f^*(x^*, \param^*), \param^* \in \paramspace^*}$ such that $f^*(x^*, \alpha^*) < B, \param^* \in \paramspace^*$, then 
$$\set{\max(0, f^*(x^*, \param^*)- 1), \param^* \in \paramspace^*}$$
 is a set of totally bounded non-negative functions, then we have the standard bound~\cite{Vapnik00}
\begin{equation}
    \nonumber
    P(1 - f^*(x^*, \param^*_0) \leq 0) \leq P(1 - f^*(x^*, \param^*_\nsamples) \leq 0) + \bigO{\sqrt{\frac{\vc^* \log\left(\frac{2\nsamples}{\vc^*} \right) - \log\eta}{\nsamples}} } .
\end{equation}
with probability $1 - 2\eta$.
That is, to have a rate of $\vc/\nsamples$ for $\param_\nsamples$, we need to estimate $\param^*_\nsamples$, which has a rate of $\sqrt{\vc^*/\nsamples}$. However, observe that $\Xspace^*$ is the space suggested by the Teacher, which hopefully has a much lower capacity, and thus, the convergence will be faster in this space.

\subsubsection*{SVM+ }
Vapnik describes an extension of the SVM that embodies the LUPI paradigm~\cite{VapnikV09,VapnikI15a}. Given a set of triplets
\begin{equation}
    \nonumber
    (x_1, x^*_1, y_1), \ldots, (x_\nsamples, x^*_\nsamples, y_\nsamples) ,
\end{equation}
the idea is to model the slack variables of the standard SVM using the elements $x^* \in \Xspace^*$ as
$$ \xi(x^*, y) = \left[y (w^* \phi^*(x^*) + b^*) \right]_+  = \max\left( y (w^* \phi^*(x^*) + b^*), 0  \right).$$
The minimization problem is the following:
\begin{equation}
    \label{eq:svmplus_original}
    \begin{aligned}
        & \argmin_{w, w^*, b, b^*}
        & &  C \sum_{i=1}^\nsamples \left[y_i (\dotp{w^*}{\phi^*(x_i^*)} + b^*) \right]_+ + \frac{1}{2} \dotp{w}{w} + \frac{\mu}{2} \dotp{w^*}{w^*} \\
        & \text{s.t.}
        & & y_{i} ( \dotp{w}{\phi(x_{i})} + b) \geq 1 - \left[y_i (\dotp{w^*}{\phi^*(x^*_i)} + b^*) \right]_+ .
    \end{aligned}
\end{equation}
Here $\phi$ and $\phi^*$ are two transformations that can be different.
However, note that problem~\eqref{eq:svmplus_original} is not convex due to the positive part term in the objective function. Vapnik et al. propose a relaxation of this problem to obtain a convex one. The idea is to model the slack variables $\xi$ as
$$ \xi(x^*, y) = \left[y (w^* \phi^*(x^*) + b^*) \right] + \zeta(x^*, y) ,$$
where $\zeta(x^*, y) \geq 0$.
The minimization problem is then
\begin{equation}
    \label{eq:svmplus_delta_primal}
    \begin{aligned}
        & \argmin_{w, w^*, b, b^*, \zeta_i}
        & &  C \sum_{i=1}^\nsamples \left( \left[y_i (\dotp{w^*}{\phi^*(x_i^*)} + b^*) \right] + \zeta_i \right) + C \Delta \sum_{i=1}^\nsamples \zeta_i \\
        & & &\qquad + \frac{1}{2} \dotp{w}{w} + \frac{\mu}{2} \dotp{w^*}{w^*} \\
        & \text{s.t.}
        & & y_{i} ( \dotp{w}{\phi(x_{i})} + b) \geq 1 - \left[y_i (\dotp{w^*}{\phi^*(x^*_i)} + b^*) + \zeta_i \right] ,\\
        & & &y_i (\dotp{w^*}{\phi^*(x^*_i)} + b^*) + \zeta_i \geq 0 ,\\
        & & &\zeta_i \geq 0, \\
        & \text{for } & & i=1, \ldots, \nsamples.
    \end{aligned}
\end{equation}
Problem~\eqref{eq:svmplus_delta_primal} is convex and the corresponding dual problem is
\begin{equation}\label{eq:svmplus_delta_dual}
    \begin{aligned}
        & \argmin_{\alpha_i, \delta_i} 
        & & \frac{1}{2} \sum_{i, j=1}^\nsamples y_i y_j \alpha_i \alpha_j k(x_i, x_j) +\frac{1}{2 \mu} \sum_{i, j=1}^\nsamples y_i y_j (\alpha_i - \delta_i) (\alpha_j - \delta_i) k^*(x^*_i, x^*_j)  - \sum_{i=1}^\nsamples \alpha_i \\
        & \text{s.t.}
        & & 0 \leq \delta_i \leq C \\
        & & & 0 \leq \alpha_i \leq C + \delta_i, \\
        & & & \sum_{i=1}^{\nsamples}{\delta_i y_i} = 0, \; 
        \sum_{i=1}^{\nsamples}{\alpha_i y_i} = 0, \\
        & \text{for } & & i=1, \ldots, \nsamples.
        \end{aligned}
\end{equation}
where we use the kernel functions
$$k(x_i, x_j) = \dotp{\phi(x_i)}{\phi(x_j)}, \; k^*(x^*_i, x^*_j) = \dotp{\phi^*(x^*_i)}{\phi^*(x^*_j)}.$$ 
We can observe in Problem~\eqref{eq:svmplus_delta_dual} that the LUPI paradigm exerts a similarity control, correcting the similarity in space $\Xspace$ with the similarity in the privileged space $\Xspace^*$. For that reason, $\Xspace$ and $\Xspace^*$ are named Decision Space and Correction Space, respectively.


\subsubsection*{Connection between SVM+ and MTLSVM}
% Liang and Cherkassky
In~\cite{LiangC08} the connection between SVM+ and Multi-Task Learning SVM (MTLSVM) is discussed. 
% Definition
The MTLSVM proposed in~\cite{LiangC08} is a Multi-Task Learning model based on the SVM. It solves the primal problem
\begin{equation}
    \label{eq:mtlsvm_primal}
    \begin{aligned}
        & \argmin_{w, b, v_r, b_r, \xi_i^r}
        & & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \xi_i^r + \frac{1}{2} \dotp{w}{w} + \sum_{r=1}^\ntasks \frac{\mu}{2} \dotp{v_r}{v_r} \\
        & \text{s.t.}
        & & y_{i}^r ( \dotp{w}{\phi(x_{i}^r)} + b + \dotp{v_r}{\phi_r(x_{i}^r)} + b_r) \geq 1 - \xi_i^r ,\\
        & & &\xi_i^r \geq 0, \\
        & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
    \end{aligned}
\end{equation}
Here, a combination of a common model for all tasks
$$ \dotp{w}{\phi(x_{i})} + b $$
and a task-specific model
$$ \dotp{v_r}{\phi_r(x_{i}^r)} + b_r $$
is used. Here, the common transformation $\phi$ and the task-independent ones $\phi_r$ can be different 
The dual problem corresponding to~\eqref{eq:mtlsvm_primal} is
\begin{equation}\label{eq:mtlsvm_dual}
    \begin{aligned}
        & \argmin_{\alpha_i} 
        & & \frac{1}{2} \sum_{r, s=1}^\ntasks \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s k(x_i^r, x_j^s) + \frac{1}{2 \mu} \sum_{r, s=1}^\ntasks  \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s \delta_{rs} k_r(x^r_i, x^s_j) \\
        & & & \qquad - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r \\
        & \text{s.t.}
        & & 0 \leq \alpha_i^r \leq C \\
        & & & \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r} = 0, \\
        & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
        \end{aligned}
\end{equation}
% Similarities and differences
In~\cite{LiangC08} some similarities between MTLSVM and SVM+ are pointed out. Problem~\eqref{eq:mtlsvm_primal} can be regarded as an adaptation of~\eqref{eq:svmplus_delta_primal} to solve MTL problems, where different tasks are incorporated and multiple correcting spaces are defined using the transformations $\phi_r$.
If we consider the problem~\eqref{eq:mtlsvm_primal} with a single task, it is a modification of the SVM+ problem~\eqref{eq:svmplus_delta_primal} where the slack variables are modeled as
$$ \xi(x, y) = y (w^* \phi^*(x) + b^*)  .$$
That is, it is a relaxation of the original problem~\eqref{eq:svmplus_delta_primal} where the second constraint to model the positive part of the slack variables disappears.
This relaxation gives place to some important differences between both models. Since the auxiliary primal variables $\zeta_i$ are no longer required, this is reflected in a simpler dual form~\eqref{eq:mtlsvm_dual}, where only $\nsamples$ dual variables have to be estimated, instead of the $2\nsamples$ dual variables of~\eqref{eq:svmplus_delta_dual}.
The Multi-Task element is reflected on~\eqref{eq:mtlsvm_dual} through the $\delta_{rs}$ function, which makes the correction of similarity only possible between elements of the same task.
%

A major remark can be make about the differences between MTLSVM and SVM+. The results for the improved rate of convergence with an Intelligent Teacher may not be valid with MTLSVM, since we are not modelling the slack variables $\xi$ adequately. 
It is still a work in progress to study the rate of convergence of MTLSVM and to establish more clear links with SVM+.
% Relation with the simplified approach?




    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%%%%%%%%%             SECTION         %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Multi-Task Learning with Kernel Methods}\label{sec:ch3_mtl_kernelmethods}

% Most multi-task methods are linear models, which may not be flexible enough to capture certain dependencies.
% Deep Learning is a very popular and cost-effective way of overcoming this problem. The final linear models are substituted by the neural network output and the parameters are learned together using back propagation.
% However, one of the main problems of deep learning is the lack of theoretical results and the non-convexity of the problems.
% Other alternative to extend the MTL models non-linearly is by using the kernels.

% Most multi-task methods reviewed in Section~\ref{sec:ch3_overview} are based on linear models. This produces the models that are more interpretable and makes it easier to enforce some kind of coupling between tasks so there exists some kind of transfer learning. However, linear models are not powerful enough for most real-world problems, which might have non-linear properties.
% %

% Deep Learning is a very popular and cost-effective way of overcoming this problem. Neural Networks with multiple layers, where each layer may apply a non-linear transformation, are very powerful models that can estimate function using a hierarchical strategy where each new layer builds new features based on the output of the previous layer. 
% This idea is very useful in MTL, where some features are jointly learned for all the tasks and, in the last layer, final task-specific linear models are build using them.
% %
% Despite its great success, Deep Learning has some inconveniences. One is that to estimate the extensive number of parameters involved in a deep architecture, large quantities of data are needed. This large datasets are not usually of public domain.
% Other problem found in Deep Learning is the lack of mathematical guarantees, at least when compared with other methods of Machine Learning.

% %
% The other alternative to extend MTL models non-linearly is by using Kernel Methods. Kernel Methods, although they are more computationally challenging, offer some interesting characteristics. In the first place most problems using kernels are formulated as convex problems, which have a single optimum solution. Furthermore, they also offer some well-studied mathematical properties such as consistency of the methods, i.e. the algorithms will approximate this solution as the number of data grows, or rates of convergence, i.e. how fast this solution is approximated.
% Also, Kernel Methods, thanks to this mathematical properties, usually require less data than Deep Learning models to achieve competitive results. 

% % Evgeniou
% \subsection{Regularized Multi-Task Learning}
% The work of~\cite{EvgeniouP04} presents a SVM-based MTL problem formulation. 
% The goal is to find a decision function for each task, each being defined by a vector
% $$w_r = w + v_r,$$
% where $w$ is common to all tasks and $v_r$ is task-specific.
% The primal problem of \emph{regularized MTL} SVM, using the unified formulation, is 
% \begin{equation}
%     \label{eq:regmtlsvm_primal}
%     \begin{aligned}
%         & \argmin_{w, v_r, \xi_i^r}
%         & & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \xi_i^r + \frac{1}{2} \dotp{w}{w} + \sum_{r=1}^\ntasks \frac{\mu}{2} \dotp{v_r}{v_r} \\
%         & \text{s.t.}
%         & & y_{i}^r ( \dotp{w}{x_{i}^r} + \dotp{v_r}{x_{i}^r}) \geq p_i^r - \xi_i^r ,\\
%         & & &\xi_i^r \geq 0, \\
%         & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
%     \end{aligned}
% \end{equation}
% % Leveraging common and specific information
% Note that $\mu$ is a parameter that controls the tradeoff between the relevance of common and specific models. That is, when $\mu$ tends to infinite, the resulting model approaches a common-task standard SVM; when $\mu$ tends to zero, a independent task approach is taken, with one standard SVM problem for each task.
% This is also reflected in the corresponding dual problem
% \begin{equation}\label{eq:regmtlsvm_dual}
%     \begin{aligned}
%         & \argmin_{\alpha_i} 
%         & & \frac{1}{2} \sum_{r, s=1}^\ntasks \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s \dotp{x_i^r}{x_j^s} + \frac{1}{2 \mu} \sum_{r, s=1}^\ntasks  \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s \delta_{rs} \dotp{x_i^r}{x_j^s} \\
%         & & & \qquad - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} p_i^r \alpha_i^r \\
%         & \text{s.t.}
%         & & 0 \leq \alpha_i^r \leq C \\
%         & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
%         \end{aligned}
% \end{equation}
% In this dual form, as $\mu$ grows, the task-specific part goes to zero, and the most important term is the first one, corresponding to the common part. The opposite effect is obtained when $\mu$ shrinks.
% % Common + specific model which is equivalent to penalizing individual norm and variance
% Moreover, in~\cite{EvgeniouP04} it is shown that solving~\eqref{eq:regmtlsvm_primal} is equivalent to solving the problem
% \begin{equation}
%     \nonumber
%     \begin{aligned}
%         & \argmin_{ww_r, \xi_i^r}
%         & & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \xi_i^r +  \frac{1}{2} \sum_{r=1}^\ntasks \norm{w_r}^2 + \frac{\mu}{2} \sum_{r=1}^\ntasks  \norm{w_r - \sum_{s=1}^\ntasks w_s}^2 \\
%         & \text{s.t.}
%         & & y_{i}^r ( \dotp{w_r}{x_{i}^r}) \geq p_i^r - \xi_i^r ,\\
%         & & &\xi_i^r \geq 0, \\
%         & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
%     \end{aligned}
% \end{equation}
% Now, only the $w_r$ variables are included, and it is clearer that $\mu$ penalizes the variance of the $w_r$ vectors, so all models $w_r$ will tend to a common model as $\mu$ grows.



% Learning Multiple Tasks with Kernel Methods
The Multi-Task Learning paradigm can be seen as learning a vector-valued function
$f: \Xspace \to \reals^\ntasks$
, where each element of the vector corresponds to a different task. 


\subsection{Vector-Valued Reproducing Kernel Hilbert Spaces}%Single-Task Learning with MTL Kernels}
Consider $\Yspace$ a Hilbert space with inner product $\ydotp{.}{.}$, then we can study the Hilbert spaces of functions with values in $\Yspace$. The kernels in such spaces are operator-valued functions $K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)$.
We look at these spaces from three different and equivalent perspectives: continuous evaluation functionals, positive-definite kernels and feature maps.

\paragraph*{Continuous evaluation functionals.}
The first approach considered is that of continuous evaluation functionals. That is, given a Hilbert space with continuous evaluation functionals, we can find the reproducing kernel operator.
Consider the vector-valued Hilbert space $\mathcal{H}$ of functions defined in $\Xspace$ and values in $\Yspace$
\begin{equation*}
    \begin{aligned}
        &\hilbertspace &\to &\Yspace &\to &\reals \\
        &f &\to &f(x) &\to &\ydotp{y}{f(x)}
    \end{aligned}
\end{equation*}
Consider the functionals $L_{x, y} f = \ydotp{y}{f(x)}$, if this functionals are continuous, we can apply Riesz Representation theorem. That is, for every $x \in \Xspace, y \in \Yspace$ we can find an unique $g_{x, y} \in \hilbertspace$ such that for all $f \in \hilbertspace$,
\begin{equation}\label{eq:vector_riesz}
    L_{x, y} f = \ydotp{y}{f(x)} = \dotp{g_{x,y}}{f}_{\hilbertspace} .
\end{equation}
We can now give the definition of vector-valued Hilbert space from the point of view of continuous functionals~\cite[Definition 2.1]{MicchelliP05}
\begin{definition}[vector-valued RKHS]
    We say that $\hilbertspace$ is a vector-valued RKHS when for any $x \in \Xspace$ and $y \in \Yspace$, the functional $L_{x, y} f = \ydotp{y}{f(x)}$ is continuous.
\end{definition}
Note that this is a definition similar to the scalar case but we use the inner product of $\Yspace$ to construct the scalar-valued functionals $L_{x, y}$. The price we pay is that it is necessary to express the Riesz representation as dependent of the elements $y \in \Yspace$. To get rid of this dependence for every $x \in \Xspace$ we can define the linear operator
\begin{equation*}
    \begin{aligned}
        g_x: &\Yspace &\to &\hilbertspace \\
             & y      &\to &g_x y = g_{x, y}.
    \end{aligned}
\end{equation*}
This operator is well defined because $g_{x, y}$ is unique for every $x \in \Xspace, y \in \Yspace$ and its linearity is easy to check from the linearity of the inner product $\ydotp{.}{.}$.\\

Using this results can now define the operator
\begin{equation}\label{eq:vector_kernel}
    \begin{aligned}
        K(x, \hat{x}) : &\Yspace &\to &\Yspace \\
                        & y      &\to &K(x, \hat{x}) y = (g_{\hat{x}} y) (x)
    \end{aligned}    
\end{equation}
for every $x, \hat{x} \in \Xspace$. Observe that $K(x, \hat{x})$ is linear since $g_x$ is linear.
It is possible then to prove that $K(x, \hat{x})$ is a reproducing kernel in $\mathcal{H}$ as seen~\cite[Propositon 2.1]{MicchelliP05}.
To do that, first we have to define a vector-valued kernel and the corresponding reproducing property.
\begin{definition}[operator-valued Kernel]
    If $\Yspace$ is a finite-dimensional Hilbert space, an operator-valued kernel is a function
    \begin{equation}
        \nonumber
        K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)
    \end{equation}
    which is symmetric and positive definite.
\end{definition}
For the clarity of the text an operator-valued $K$ will be referred just as kernel unless an explicit distinction is needed.
\begin{definition}[Reproducing Property of operator-valued operators]
    If $\Yspace$ is a Hilbert space, a function
    \begin{equation}
        \nonumber
        K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)
    \end{equation}
    has the reproducing property if $\forall x \in \Xspace, y \in \Yspace, \forall f \in \hilbertspace,$ $ \ydotp{y}{f(x)} = \dotp{K(\cdot, x) y}{f}.$ 
\end{definition}
A kernel with the reproducing property is also called a reproducing kernel. The next proposition shows how we can build a reproducing kernel for a space $\hilbertspace$ in which the evaluation functionals are continuous.
\begin{proposition}
    If for every $x, \hat{x} \in \Xspace$, the function $K(x, \hat{x})$ is defined as in Equation~\eqref{eq:vector_kernel}, then the function
    \begin{equation}
        \nonumber
        K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)
    \end{equation}
    is a reproducing kernel.
\end{proposition}
\begin{proof}
    To prove that $K$ is a reproducing kernel we need to see
    \begin{enumerate}
        \item\label{item:vvkernel_bounded} $K(x, \hat{x})$ is bounded for every $x, \hat{x} \in \Xspace$ so $K$ is well defined.
        \item\label{item:vvkernel_symmetric} $K$ is symmetric: $K(x, \hat{x})^* = K(\hat{x}, x)$.
        \item\label{item:vvkernel_positive}  $K$ is positive definite: given $n \in \naturals$, for any  $x_1, \ldots, x_n \in \Xspace,  y_1, \ldots, y_n \in \Yspace$,
        \begin{equation}
            \nonumber
            \sum_{i, j =1}^n \ydotp{y_i}{K(x_i, x_j) y_j} \geq 0.
        \end{equation}
        \item\label{item:vvkernel_repr} It has the reproducing property: $\forall x \in \Xspace, y \in \Yspace, \forall f \in \hilbertspace,$ $ \ydotp{y}{f(x)} = \dotp{K(\cdot, x) y}{f}.$
    \end{enumerate}
    To prove~\ref{item:vvkernel_bounded} and~\ref{item:vvkernel_symmetric},observe that by
    applying~\eqref{eq:vector_riesz} to $f = g_{\hat{x}}\hat{y}$, for every $x \in \Xspace, y \in \Yspace$ there exists an unique $g_{x, y} = g_{x}y$ such that
    \begin{equation}
        \ydotp{y}{(g_{\hat{x}} \hat{y}) (x)} = \dotp{g_{x}y}{g_{\hat{x}}\hat{y}} .
    \end{equation}
    and combining this result with~\eqref{eq:vector_kernel} we get
    \begin{equation}
        \nonumber
        \ydotp{y}{K(x, \hat{x}) \hat{y}} = \ydotp{y}{(g_{\hat{x}} \hat{y})(x) }= \dotp{g_{x}y}{g_{\hat{x}}\hat{y}}.
    \end{equation}
    Also, using the definitions,
    \begin{equation}
        \nonumber
        \ydotp{K(\hat{x}, x) y}{ \hat{y}} = \ydotp{(g_{x} y)(\hat{x})}{\hat{y}} = \ydotp{\hat{y}}{(g_{x} y)(\hat{x})} = \dotp{g_{\hat{x}} \hat{y}}{g_x y}.
    \end{equation}
    Since both operators $K(x, \hat{x}), K(\hat{x}, x)$ are linear, by the Uniform Boundness Principle~\cite{Akhiezer1961TheoryOL}, $K(x, \hat{x}), K(\hat{x}, x)$ are bounded (hence continuous) and $K(x, \hat{x})^* = K(\hat{x}, x)$.
    \\
    To prove~\ref{item:vvkernel_positive} we write
    \begin{equation}
        \nonumber
        \sum_{i, j =1}^n \ydotp{y_i}{K(x_i, x_j) y_j} = \sum_{i, j =1}^n \dotp{g_{x_i} y_i}{g_{x_j} y_j}  = \norm{\sum_{i=1}^n g_{x_i} y_i}^2 \geq 0
    \end{equation}
    Finally, to prove~\ref{item:vvkernel_repr} we use that 
    $\forall x \in \Xspace, y \in \Yspace, \forall f \in \hilbertspace, \exists g_{x, y} \in \hilbertspace$ such that 
    $$ \ydotp{y}{f(x)} = \dotp{g_{x, y}}{f} = \dotp{g_{x} y}{f} =\dotp{K(\cdot, x) y}{f}.$$
\end{proof}
\paragraph*{Semi-positive definite kernels.}
The second approach changes the point of view. Given a kernel $K$, the Hilbert space from which $K$ is the reproducing kernel is built.
To do this, we use~\cite[Theorem 2.1]{MicchelliP05} which extends the Moore-Aronszanj's Theorem:
\begin{theorem}\label{th:moore-arons_vector}
    If $K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)$ is a kernel, then there exists a unique (up to an isometry) RKHS which admits $K$ as its reproducing kernel. 
\end{theorem}
The proof is similar to that of the Moore-Aronszanj's Theorem, considering the space of the completion of the span of $\left\lbrace K_x = K(\cdot, x) , x \in \Xspace \right\rbrace$.

\paragraph*{Feature map.}
The last approach is based on feature maps, which provide a very simple way of generating kernels.
\begin{lemma}
    Any feature map $\Phi: \Xspace \to \mathcal{L}(\mathcal{W}, \mathcal{Y})$ defines a kernel as
\begin{equation}
    \label{eq:kernel_featmap}
    K(x, \hat{x}) = \Phi(x) \Phi(\hat{x})^*: \Yspace \to \Yspace .
\end{equation}
\end{lemma}
\begin{proof} We need to prove that it is bounded, symmetric and positive definite:
    \begin{enumerate}
        \item Since $\Phi(x)$ is continuous, the adjoint is $\Phi(x)^*$ is continuous and the composition $\Phi(x) \circ \Phi(\hat{x})^*$ is also continuous.
        \item It is symmetric since  $(\Phi(x) \circ \Phi(\hat{x})^*)^* = ((\Phi(x)^*)^* \circ \Phi(\hat{x})^*) = (\Phi(x) \circ \Phi(\hat{x})^*) $.
        \item Is positive definite since, given any $x_1, \ldots, x_n \in \Xspace,  y_1, \ldots, y_n \in \Yspace$
        \begin{equation}
            \nonumber
            \begin{aligned}
                \sum_{i, j =1}^n \ydotp{y_i}{K(x_i, x_j) y_j} &= \sum_{i, j =1}^n \ydotp{y_i}{ \Phi(x_i) \Phi(x_j)^* y_j} \\
                & = \sum_{i, j =1}^n \ydotp{\Phi(x_i)^* y_i}{ \Phi(x_j)^* y_j} = \norm{\sum_{i=1}^n \Phi(x_i)^* y_i}^2 \geq 0 .
            \end{aligned}
        \end{equation}
    \end{enumerate}
\end{proof}
Since $K$ as defined in~\eqref{eq:kernel_featmap} is a kernel, according to Theorem~\ref{th:moore-arons_vector} we can find its corresponding vector-valued hilbert space $\hilbertspace$.

\subsubsection*{Representer Theorem for Operator-Valued Kernels}
The Representer Theorem is a crucial result in Optimization and Machine Learning. Given a regularized empirical risk, under some assumptions, the theorem gives a precise description of the minimizer $f^*$ as a finite linear combination of functions $K(\cdot, x_i)$ where $x_i$ are part of the empirical sample.
This result is extended in~\cite[Theorem 4.2]{MicchelliP05} for operator-valued kernels.
\begin{theorem}
    Let $\mathcal{Y}$ be a Hilbert space and let $\mathcal{H}$ be the Hilbert space of $\mathcal{Y}$-valued functions with an operator-valued reproducing kernel $K$. Let $V: \Yspace^n \times \reals_+ \to \reals$ be a function strictly increasing in its second variable and consider the problem of minimizing the functional    
    \begin{equation}\label{eq:representer_functional}
        \begin{aligned}
            E(f) \defeq V((f(x_1), \ldots, f(x_n)), \norm{f}^2)
        \end{aligned}
    \end{equation}
    in $\hilbertspace$.
    If
    % for every $(f_1, \ldots, f_n) \in \Yspace^n$ the function $h: \reals_+ \to \reals_+$ defined as $h(t) \defeq V((f_1, \ldots, f_n)), t)$ is strictly increasing and
    $f_0$ minimizes the $E$, then $f_0 = \sum_{j=1}^n K(\cdot, x_j) c_j$ where $c_j \in \mathcal{Y}$. In addition, if $V$ is strictly convex, the minimizer is unique. 
\end{theorem}

\subsubsection*{A useful bijection between kernels}
The scalar-valued kernels are well known and studied but this is not the case for operator-valued kernels. However, as shown in~\cite{Hein04kernels,BaldassarreRBV12} we can find a bijection between operator-valued kernels and scalar-valued ones.
\begin{lemma}\label{lemma:kernel_bijection}
    Let $\mathcal{Y}$ be a finite-dimensional Hilbert space and
    $K: \Xspace \times \Xspace \to \mathcal{L}(\mathcal{Y})$
    be an operator-valued kernel. Consider also the scalar-valued kernel
    $L: (\Xspace, \mathcal{Y}) \times (\Xspace, \mathcal{Y}) \to \reals$
    such that $L((x, z), (\hat{x}, \hat{z})) = \ydotp{z}{K(x, \hat{x}) \hat{z}}$, then the map $K \to L$ is a bijection.
\end{lemma}
Moreover, if we focus on finite dimensional Hilbert spaces, that is, isomorphic to $\reals^d$ given an operator-valued kernel $K$ the corresponding scalar-valued kernel $L$ is defined using the normal basis as
$$ L((x, e_r), (\hat{x}, e_s)) = \ydotp{e_r}{K(x, \hat{x}) e_s} = K(x, \hat{x})_{rs} .$$
That is, each pair $(x, \hat{x})$ defines a matrix which contains the information of how the different outputs, or tasks, are related.
 
% \begin{figure}[t]
%     \centering
% \begin{tikzpicture}[node distance=1cm, auto,]
%     %nodes
%     \node[punkt] (market) {Market (b)};
%     % We make a dummy figure to make everything look nice.
%     \node[above=of market] (dummy) {};
%     \node[right=of dummy] (t) {Ultimate borrower}
%       edge[pil,bend left=45] (market.east) % edges are used to connect two nodes
%     \node[left=of dummy] (g) {Ultimate lender}
%       edge[pil, bend right=45] (market.west)
%       %edge[pil,<->, bend left=45] node[auto] {Direct (a)} (t);
% \end{tikzpicture}
% \caption{Bijections between kernel functions.}
% \end{figure}

% \begin{figure}[t]
%     \centering
% \begin{tikzpicture}[node distance=1cm, auto,]
%     %nodes
%     \node[] (w) {};
% \end{tikzpicture}
% \caption{Bijections between kernel functions.}
% \end{figure}


\subsection{Tensor Product of Reproducing Kernel Hilbert Spaces}
Consider the space of the tensor product of two scalar-valued RKHS' $\hilbertspace_1 \otimes \hilbertspace_2$ with reproducing kernels $K_1, K_2$, where the functions $f \in \hilbertspace_i$ are defined as $f: \Xspace_i \to \Yspace_i$ for $i=1, 2$. This tensor space is also a Hilbert space endowed with the inner product:
\begin{equation}
    \label{eq:innerprod_tensor}
    \begin{aligned}
        \dotp{}{}: &(\Xspace_1 \otimes \Xspace_2) \times &(\Xspace_1 \otimes \Xspace_2) &\to & \reals \otimes \reals \\
    &(f_1 \otimes f_2) &(\hat{f}_1 \otimes \hat{f}_2) &\to &\dotp{f_1}{\hat{f}_1} \dotp{f_2}{\hat{f}_2}.
    \end{aligned}
\end{equation}
It is easy to check that this inner product is symmetric because the inner products of both $\hilbertspace_1$ and $\hilbertspace_2$ are symmetric. It is linear because the inner products of both $\hilbertspace_1$ and $\hilbertspace_2$ are linear and the tensor product is linear. Finally, it is positive definite since 
$  \dotp{f_1}{f_1}  \dotp{f_2}{f_2} \geq 0$ and
$\dotp{f_1}{f_1}  \dotp{f_2}{f_2} = 0 \implies \dotp{f_i}{f_i} = 0$ for $i=1$ or $i=2$; taking $i=1$ without loss of generality, then $f_1 = 0$, so $f_1 \otimes f_2 = 0$. To apply the Riesz Theorem in this Hilbert space it is necessary to check wether the evaluation functionals are continuous or, equivalently, bounded.
%
\begin{proposition}[RKHS as tensor product of RKHS']
    The space $\hilbertspace = \hilbertspace_1 \otimes \hilbertspace_2$ with the inner product defined in~\eqref{eq:innerprod_tensor} have bounded evaluation functionals $L_{x_1 \otimes x_2} $ defined as $L_{x_1 \otimes x_2} (f_1 \otimes f_2) =L_{x_1}(f_1) \otimes L_{x_2}(f_2) = f_1(x_1) f_2(x_2)$ for $x_1 \otimes x_2 \in \Xspace_1 \otimes \Xspace_2$.
\end{proposition}
\begin{proof}
    It is necessary to ensure that the operators $L_{x_1 \otimes x_2}$ are bounded. Given any $f_1 \otimes f_2 \in \hilbertspace_1 \otimes \hilbertspace_2$, 
    $$ \norm{L_{x_1 \otimes x_2} (f_1 \otimes f_2)} \defeq \norm{f_1(x_1) f_2(x_2)} \leq \norm{f_1(x_1)}  \norm{f_2(x_2)} \leq M_{x_1} \norm{f_1}_{\hilbertspace_1} M_{x_2} \norm{f_2}_{\hilbertspace_2} .$$
\end{proof}
%
We also need to define the kernel function for this tensor product space.
\begin{proposition}
    The kernel function
    \begin{equation}
        \nonumber
        \begin{aligned}
            K_1 \otimes K_2: &(\hilbertspace_1 \otimes \hilbertspace_2) \times &(\hilbertspace_1 \otimes \hilbertspace_2) &\to & \reals \\
        &(x_1 \otimes x_2) &(\hat{x}_1 \otimes \hat{x}_2) &\to & K_1(x_1, x_2)  K_2(x_1, x_2) 
        \end{aligned}
    \end{equation}
    is a reproducing kernel for the Hilbert space $\hilbertspace_1 \otimes \hilbertspace_2$.
\end{proposition} 
\begin{proof}
    First, $K_1 \otimes K_2$ is a kernel, that is, it is symmetric and positive-definite. The proof is very similar to the symmetry and positive definitiness proof of the inner product~\eqref{eq:innerprod_tensor}.
    To observe that $K_1 \otimes K_2$ has the reproducing property, we write
    $$ \dotp{K_1 \otimes K_2 (\cdot, x_1 \otimes x_2)}{f_1 \otimes f_2} \defeq \dotp{K_1(\cdot, x_1)}{f_1} \dotp{K_2(\cdot, f_2)}{x_2} = f_1(x_1) f_2(x_2) .$$
\end{proof}
\subsubsection*{Another useful bijection between kernels}
To understand the connection between tensor product RKHS' and vector-valued RKHS' we study a special case of operator-valued kernels.
A standard assumption is that the relation among the different outputs is independent of the pair $(x, \hat{x})$:
$$ K(x, \hat{x})  = k(x, \hat{x}) M$$
where $k$ is a scalar-valued kernel and $M$ is some fixed operator $M \in \mathcal{L}(\mathcal{Y})$. That is, the operator $K(x, \hat{x})$ decouples in two parts: the similarity between $x$ and $\hat{x}$ measured by $k(\cdot,\cdot)$ and the interaction between the different outputs expressed by $M$. In those cases it is easier to express the operator-valued kernel as the tensor product of two spaces. The following lemma shows how we can express such tensor product kernel.
%
% Consider the tensor product space $\Xspace \otimes \Tspace$, where $\Tspace$ is the space of tasks. We assume that $\Tspace$ has finite dimension, therefore it is isomorphic to $\reals^k$. We consider $\hilbertspace$ as the Hilbert space of functions $f: \Xspace \to \reals$ with reproducing kernel $K_\Xspace$, and $\Tspace = (\reals^k)^*$ as the space of linear functions $g: \reals^k \to \reals$, which is isomorphic to $\reals^k$. In $\reals^k$ we consider a general inner product induced by the positive definite matrix $A$.
% Then $(\reals^k)^*$, by the Riesz Representation Theorem, $ \forall x \in \reals^k, \exists u_x \in (\reals^k)^*$ such that $\forall w \in (\reals^k)^*$ 
% $$ \dotp{w}{u_x}_A = w x, $$
% that is, $u_x = A^{-1}x$.
% Then, the kernel is defined as
% $K_{\Tspace}(x, y) = \dotp{u_x}{u_y}_A = x^\intercal A^{-1} y$.
% Now we can define the following tensor product of two scalar-valued kernels:   
% \begin{equation}
%     \nonumber
%     \begin{aligned}
%         K_\Xspace \otimes K_\Tspace: &(\Xspace \otimes \reals^k) \times &(\Xspace \otimes \reals^k) &\to & \mathcal{L}(\reals) \otimes \mathcal{L}(\reals) \\
%     &(x \otimes z) &(\hat{x} \otimes \hat{z}) &\to & K_\Xspace(x, \hat{x}) \otimes K_\Tspace(z, \hat{z}).
%     \end{aligned}
% \end{equation}
% The function $K_\Xspace \otimes K_\Tspace$ is the reproducing kernel of $\hilbertspace \otimes (\reals^k)^*$.
% %

\begin{lemma}
    Let $\mathcal{Y}$ be a finite-dimensional Hilbert space and
    $K: \Xspace \times \Xspace \to \mathcal{L}(\mathcal{Y})$
    be a separable operator-valued kernel, that is
    $K(x, \hat{x}) = k(x, \hat{x})M$
    with $M \in \mathcal{L}^+(\mathcal{Y})$.   
    Consider the kernel
    $K_\Xspace \otimes K_\Yspace: (\Xspace \otimes \mathcal{Y}) \times (\Xspace \otimes \mathcal{Y}) \to \reals$
    such that $K_\Xspace \otimes K_\Yspace((x \otimes z), (\hat{x} \otimes \hat{z})) = K_\Xspace(x, \hat{x}) K_\Yspace(z, \hat{z})$, with $K_\Yspace(z, \hat{z}) = \ydotp{z}{M \hat{z}}$ then the map $K \to K_\Xspace \otimes K_\Yspace$ is a bijection.
\end{lemma}
\begin{proof}
    First, observe that $K_\Yspace$ is the reproducing kernel of $\Yspace$ with the inner product induced by the operator $M^{-1} \in \mathcal{L}^+(\mathcal{Y})$.
    By the Lemma~\ref{lemma:kernel_bijection} there exists a bijection between $K$ and $L$ where $L((x, z), (\hat{x}, \hat{z})) = \ydotp{z}{K(x, \hat{x}) \hat{z}}$. When $K(x, \hat{x}) = k(x, \hat{x})M$, we define 
    $$ K_\Xspace \otimes K_\Yspace(x \otimes z, \hat{x} \otimes \hat{z}) = K_\Xspace(x, \hat{x}) K_\Yspace(z, \hat{z}) = K_\Xspace(x, \hat{x}) \ydotp{z}{M \hat{z}} = L((x, z), (\hat{x}, \hat{z})) ,$$
    and the bijection is trivial by definition.
    %

    Moreover, observe that using this kernel with a basis of $\Yspace$,  $z=e_r, \hat{z}=e_s$, 
    $$ K_\Xspace \otimes K_\Yspace((x \otimes e_r) , (\hat{x} \otimes e_s)) = K_\Xspace(x, \hat{x}) (M)_{rs}.$$
\end{proof}
This kind of kernels are called separable kernels~\cite{AlvarezRL12, KadriDPCRA16}, however their connection with operator-valued kernels is not very clear. This subsection tries to shed some light over the construction of separable kernels and how they relate to operator-valued kernels.
 

\subsection{Kernel Methods for Multi-Task Learning}

There exists a plethora of work about Single-Task Learning within regularization theory, some general formulation is
\begin{equation}
    \label{eq:stl_general_formulation}
    \begin{aligned}
        & \sum_{i=1}^{\nsamples} \lossf(y_i, \dotp{w}{\phi(x_i)}) + \lambda \dotp{w}{w} .\\
    \end{aligned}
\end{equation}
Here, $\lossf$ is the loss function and $\phi$ is a transformation to include non-linearity. Popular models such as Ridge Regression or SVMs are particular cases of this formulation for different choices of $\lossf$.
One crucial result for problems using this formulation is the \emph{Representer Theorem}, which states that the any minimizer of problem~\eqref{eq:stl_general_formulation} has the form
\begin{equation}
    \label{eq:representerth_sol}
    w = \sum_{i=1}^\nsamples c_j \phi(x_j) .
\end{equation}
Given $w$ represented as in~\eqref{eq:representerth_sol}, we write
\begin{equation}
    \nonumber
    \dotp{w}{\phi(\hat{x})} = \sum_{i=1}^\nsamples c_j \dotp{\phi(x_j)}{\phi(\hat{x})}.
\end{equation}
This is very useful because we can apply the kernel trick and use the transformations $\phi$ only implicitly.
In this subsection it is shown how a broad class of Multi-Task problems can be expressed as regularized Single-Task problems.

\subsubsection*{Linear MTL Models}
Building upon the ideas discussed in~\cite{EvgeniouP04}, two useful results are presented in~\cite{EvgeniouMP05}, which show how we can apply Single-Task Learning methods to Multi-Task Learning problems.
The first result~\cite[Proposition 1]{EvgeniouMP05} is defined for linear models and illustrates under which conditions we can adapt these results for MTL.
Consider the linear MTL problem where we want to estimate the task parameters $u_r: r = 1, \ldots, T$, so we define $\myvec{u}^\intercal = (u_1^\intercal, \ldots, u_T^\intercal) \in \reals^{\ntasks \dimx}$. Then we want to minimize
\begin{equation}
    \label{eq:mtl_general_formulation}
    \begin{aligned}
        &R(\myvec{u}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{u_r}{x_i^r}) + \mu J(\myvec{u}) ,\\
    \end{aligned}
\end{equation}
where $J(\myvec{u})= \myvec{u}^\intercal \mymat{E} \myvec{u}$ is the regularizer where different choices of $J(\myvec{u})$, i.e. choices of the matrix $E$, we can encode different beliefs about the task structure. For example, if $J(\myvec{u}) = \sum_{r=1}^T \norm{u_r}^2$ the problem decouples and we get independent task learning, so there is no relation among tasks; if $J(\myvec{u}) = \sum_{r, s=1}^T \norm{u_r - u_s}^2$ we are enforcing the parameters from different tasks to be close, so we expect all tasks to be similar.
%

Then, Evgeniou et al. propose to consider a vector $\myvec{w} \in \reals^p$ with $p \geq \ntasks \dimx$ such that we can express $\dotp{u_r}{x}$ as $\dotp{\mymat{B}_r^\intercal \myvec{w}}{x}$,  where $\mymat{B}_r$ is a $p \times \dimx$ matrix yet to be specified. One condition for $\mymat{B}_r$ is to be full rank so we can find such $\myvec{w}$.
Note that we can also interpret $\mymat{B}_r$ as a feature map $f: \reals^\dimx \to \reals^p$ such that $\dotp{u_r}{x} = \dotp{\myvec{w}}{\mymat{B}_r x}$. 
Observe that using the matrices $\mymat{B}_r$ we have the following MTL kernel
\begin{equation}
    \nonumber
    \hat{k}(x^r, y^s) = \hat{k}((x, r), (y, s)) = x^\intercal B_r^\intercal B_s y .
\end{equation}
Using these feature maps we would like to write the MTL problem as a Single-Task problem 
\begin{equation}
    \label{eq:mtl_as_stl}
    \begin{aligned}
        &S(\myvec{w}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{w}}{\mymat{B}_r x_i^r}) + \mu \dotp{\myvec{w}}{\myvec{w}} ,\\
    \end{aligned}
\end{equation}
We also define the feature matrix $\mymat{B}$ as the concatenation $\mymat{B} = (B_r: r=1, \ldots, T) \in \reals^{p \times \ntasks \dimx}$, then we present the first result of~\cite{EvgeniouMP05}.
\begin{proposition}\label{prop:evgeniou1}
    If the feature matrix $\mymat{B}$ is full rank and we define the matrix $\mymat{E}$ in equation~\eqref{eq:mtl_general_formulation} as to be $\mymat{E} = (\mymat{B}^\intercal \mymat{B})^{-1}$ then we have that
    \begin{equation}
        \nonumber
        S(\myvec{w}) = R(B^\intercal \myvec{w}).
    \end{equation}
    and therefore $\myvec{u}^* = B^\intercal \myvec{w}^*$.
\end{proposition}
One important consequence of this result is that since we can solve the MTL problem~\eqref{eq:mtl_general_formulation} as the STL problem~\eqref{eq:stl_general_formulation}, then we can apply the \emph{Representer Theorem}. That is, the solution $\myvec{w}^*$ of problem~\eqref{eq:stl_general_formulation} has the form
\begin{equation}
    \nonumber
    \myvec{w} = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask c_i B_r x_i^r ,
\end{equation}
and the prediction can be expressed as
\begin{equation}
    \nonumber
    \dotp{\myvec{w}}{\hat{x}^s} = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r  (x_i^r)^\intercal B_r^\intercal B_s \hat{x}^s = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask c_i  \hat{k}(x_i^r, \hat{x}^s).
\end{equation}

\subsubsection*{Kernel Extension of MTL Models}
Evgeniou et al. also extend this results to kernelized models. In the following lemma~\cite[Lemma 2]{EvgeniouMP05} the give the conditions under which the extension is possible.

\begin{lemma}\label{lemma:evgeniou_2}
    If $G$ is a kernel on $\mathcal{T} \times \mathcal{T}$ and, for every $r=1, \ldots, T$ there are prescribed mappings
    $z_r: \Xspace \to \mathcal{T}$
    such that
    \begin{equation}
        \label{eq:evgeniou_lemma2}
        K((x, r), (y, s)) = G(z_r(x), z_s(t)),\; x, t \in \Xspace,\; r,s \in [T]
    \end{equation}
    then $K$ is a multi-task kernel.
\end{lemma}
That defines $K$ as a semi-positive functional over the product space $\Xspace \times \mathcal{T}$, which is named multi-task kernel.
The mappings described in~\cite{EvgeniouMP05} are
$$ z_r(x) = B_r x$$
where $B_r$ are the $p \times d$ matrices previously defined.
Then, two examples of multi-task kernels using this lemma are given. The polynomial kernel is defined as
$$ K((x, r), (y, s)) = \left( x^\intercal B_r^\intercal B_s y\right) $$
and the multi-task Gaussian kernel is defined as
$$ K((x, r), (y, s)) = \exp\left(-\gamma \norm{B_r x - B_s y}^2 \right) .$$
That is, using the result of Proposition~\ref{prop:evgeniou1}, when the matrix $E$ has a block structure, we can incorporate the task-regularizer information into the Gaussian kernel using that
\begin{align*}
    \norm{B_r x - B_s y}^2 
    &= x^\intercal B_r^\intercal B_r x + y^\intercal B_s^\intercal B_s y - 2 x_r^\intercal B_r^\intercal B_s y_s \\
    &= x^\intercal E_{rr}^{-1} x + y^\intercal E_{ss}^{-1} y - 2 x_r^\intercal E_{rs}^{-1} y_s .
\end{align*}
That is, we use the task information in the original space and then apply the non-linear transformation.
%

\subsubsection*{Alternative Kernel Extension of MTL Models}
The kernel extension proposed in~\cite{EvgeniouMP05} proposes to use a mapping in the original finite space to incorporate the task information and then applying the kernel trick over this new mapped features. However, these results do not permit to perform the, possibly infinite dimensional, mapping corresponding to a kernel and then incorporate the task information in the new space.
Here we show another approach which makes it is possible to replicate the results for the infinite-dimensional case by using tensor products. Consider a general Hilbert space $\hilbertspace$ and the functional
\begin{equation}
    \label{eq:mtl_kernel_altext_original}
    \begin{aligned}
        &R({u_1, \ldots, u_T}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{u_r}{\phi(x_i^r)}) + \mu \sum_r \sum_s E_{rs} \dotp{u_r}{u_s} ,\\
    \end{aligned}
\end{equation}
where $u_1, \ldots, u_T \in \hilbertspace$ and $E$ is a $\ntasks \times \ntasks$ matrix. The following lemma illustrates how to solve this problem as a single task problem.
\begin{lemma}
    The predictions $\dotp{u_r^*}{\phi(x)}$ of the solutions $u_1^*, \ldots, u_\ntasks^*$ from the Multi-Task optimization problem~\eqref{eq:mtl_kernel_altext_original} can be obtained solving the problem
    \begin{equation}
        \label{eq:mtl_kernel_tensor}
        \begin{aligned}
            &S(\myvec{w}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{w}}{(B_r \otimes \phi(x_i^r))}) + \mu  \myvec{w}^\intercal \myvec{w} ,\\
        \end{aligned}
    \end{equation}
    where $\bm{w} \in \reals^p \otimes \hilbertspace$ with $p \geq \ntasks$ and $B_r$ are the columns of a full rank matrix $B \in \reals^{p \times \ntasks}$ such that $\mymat{E}^{-1} = \mymat{B}^\intercal \mymat{B}$.
\end{lemma}

\begin{proof}
    Replicating the idea of~\cite{EvgeniouMP05}, we can write
$$ \myvec{u} = \sum_{t=1}^T e_r \otimes u_r ,$$
such that $\myvec{u} \in \reals^T \otimes \hilbertspace$. Then, we can reformulate~\eqref{eq:mtl_kernel_altext_original} as
\begin{equation}
    \label{eq:mtl_kernel_altext_tensor}
    \begin{aligned}
        &R(\myvec{u}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{u}}{e_r \otimes \phi(x_i^r)}) + \mu \left(  \myvec{u}^\intercal (E \otimes I) \myvec{u} \right),\\
    \end{aligned}
\end{equation}
Since ${E} \in \reals^{\ntasks \times \ntasks}$ is positive definite, we can find $B \in \reals^{p \times \ntasks}, p \geq \ntasks$ and $\rank{B} = \ntasks$ such that $E^{-1} = 
{B^\intercal} {B}$, using for example the SVD. 
% In the case that ${E} \in \reals^{\ntasks \times \ntasks}$ is a semipositive definite matrix with rank $r$ we can find $B \in \reals^{p \times r}, p \geq \ntasks$ and $\rank{B} = r$ such that $E^{+} = 
% {B^\intercal} {B}$.
% For the rest of the analysis we will consider a positive definite matrix $\mymat{E}$ but the results are also valid for positive semidefinite matrices.
% 
Using the properties of the tensor product of linear maps,  $$ E^{-1} \otimes I = (B^\intercal B) \otimes I = (B^\intercal \otimes I) (B \otimes I),$$
%
Consider the change of variable $\myvec{u} = (B^\intercal \otimes I)\myvec{w}$, where $\myvec{w} \in \reals^p \otimes \hilbertspace$. Observe that this can always be done because 
%the columns of $B$ generates $\reals^T$, so we can always find $\hat{w}$ such that $e_r = B w$   .The condition of full rank for $B$ is necessary in this step.
$B$ is full rank. Rewriting \eqref{eq:mtl_kernel_altext_tensor} using $\myvec{w}$,
\begin{equation}
    \nonumber
    \begin{aligned}
        R((B^\intercal \otimes I) \myvec{w}) &= \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{(B^\intercal \otimes I) \myvec{w}}{ (e_r \otimes \phi(x_i^r))}) + \mu  \myvec{w}^\intercal (B^\intercal \otimes I)^\intercal (E \otimes I) (B^\intercal \otimes I)\myvec{w} \\
        &= \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{w}}{(B \otimes I) (e_r \otimes \phi(x_i^r))}) + \mu  \myvec{w}^\intercal \myvec{w} ,\\
    \end{aligned}
\end{equation}
which is equivalent to
\begin{equation}
    \nonumber
    \begin{aligned}
        &S(\myvec{w}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{w}}{(B_r \otimes \phi(x_i^r))}) + \mu  \myvec{w}^\intercal \myvec{w} .\\
    \end{aligned}
\end{equation}

We are thus considering a regularized functional $S(\myvec{w})$ where we seek the minimum over functions $w$ in the Hilbert space $\reals^p \otimes \hilbertspace$. Note that in this space the inner product is:
\begin{equation}
    \nonumber
    \begin{aligned}
        \dotp{}{}: &(\reals^p \otimes \hilbertspace) \times &(\reals^p \otimes \hilbertspace) &\to &\reals \\
    &(z_1, \phi(x_1)), &(z_2, \phi(x_2)) &\to &\dotp{z_1}{z_2} k(x_1, x_2)
    \end{aligned}
\end{equation}
Where $k(\cdot, \cdot)$ is the reproducing kernel of the space of functions $\phi(\cdot)$.
However we are only interested in those cases where $z = B e_r = B_r$ for some $r=1, \ldots, T$, then $\dotp{B_r}{B_s} = E^{-1}_{rs}.$
Since the regularizer is clearly increasing in $\norm{w}^2$, we can apply the Representer theorem, which states that the minimizer of $S(\myvec{w})$ has the form
\begin{equation}
    \nonumber
    %\label{eq:representer_tensor}
    \myvec{w}^* = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r (B_r \otimes \phi(x_i^r)) ,
\end{equation}
Using the correspondence between $\myvec{u}^*$ and $\myvec{w}^*$, 
\begin{equation}
    \nonumber
    \myvec{u}^* = B^\intercal \myvec{w}^* =  \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r (\vect{(\dotp{B_1}{B_r}, \ldots, \dotp{B_\ntasks}{B_r})} \otimes \phi(x_i^r)).
\end{equation}
Then, we can recover the predictions corresponding to the solutions $u_r^*$ as
\begin{equation}
    \nonumber
    \begin{aligned}
        \dotp{u_r}{\phi(\hat{x}^s)} &= \dotp{\myvec{u}}{e_s \otimes \phi(\hat{x}^s)} \\
        &= \dotp{\sum_{s=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r (\vect{(\dotp{B_1}{B_r}, \ldots, \dotp{B_\ntasks}{B_r})} \otimes \phi(x_i^r))}{e_s \otimes \phi(\hat{x}^s)} \\
        &= \sum_{s=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r  \dotp{B_s}{B_r} \dotp{\phi(x_i^r)}{\phi(x_s)} \\
        &= \sum_{s=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r  (E^{-1})_{rs} k(x_i^r, \hat{x}^s).
    \end{aligned}
\end{equation}
\end{proof}
Observe that, applying the corresponding feature map $\mymat{B} \otimes I$, the predictions can be obtained equivalently using the common $\myvec{w}$ as
\begin{equation}
    \nonumber
    \begin{aligned}
        \dotp{\myvec{w}}{(B \otimes I) (e_s \otimes \phi(\hat{x}^s))} 
        &= \dotp{\myvec{w}}{ (B_s \otimes \phi(\hat{x}^s))} \\ 
        &= \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r \dotp{B_r \otimes \phi(x_i^r)}{B_s \otimes \phi(\hat{x}^s)} \\
        &= \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r \dotp{B_r}{B_s} \dotp{\phi(x_i^r)}{\phi(\hat{x}^s)} \\ 
        &= \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r (E^{-1})_{rs} k(x_i^r, \hat{x}^s).
    \end{aligned}
\end{equation}
That is, we have expressed the Multi-Task problem as a Single-Task problem with the Multi-Task kernel is
\begin{equation}
    \nonumber
    \hat{k}(x_i^r, x_j^s) = (E^{-1})_{rs} k(x_i^r, x_j^s).
\end{equation}
%
Note that the kernels obtained in this way, unlike those obtained using Lemma~\ref{eq:evgeniou_lemma2}, split the inter-task relations and the similarity between data points. That is, we can implicitly send our data into another,  a possibly infinite-dimensional, space and apply the task information after this transformation.
This kind of kernels are usually called separable kernels~\cite{AlvarezRL12} but, to the best of knowledge, this is the first time the they are constructed using tensor products.

% However, there is another alternative for the kernel extension: first apply the non-linear transformation and then use the task information in the augmented space.
% To do that we need to extend the notions of matrices to operators in (potentially infinite-dimensional) Hilbert spaces. Extending the general MTL formulation of~\eqref{eq:mtl_general_formulation}
% \begin{equation}
%     \label{eq:mtl_general_formulation_nonlinear}
%     \begin{aligned}
%         &R(\myvec{u}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{u_r}{\phi(x_i^r)}) + \mu J(\myvec{u}) ,\\
%     \end{aligned}
% \end{equation}
% where $\phi$ is a non-linear transformation
% $\phi: \Xspace \to \mathcal{Y}$ where $\mathcal{Y}$
%  is a Hilbert space and 
% $ J(\myvec{u}) = \dotp{u}{\mymat{E} u} = \dotp{u \mymat{E}}{u}$
% where $E$ is semi-positive definite linear operator in $\mathcal{Y}$.
% Consider also the non-linear extension of~\eqref{eq:mtl_as_stl}:
% \begin{equation}
%     \label{eq:mtl_as_stl_nonlinear}
%     \begin{aligned}
%         &S(\myvec{w}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{w}}{\mymat{B}_r \phi(x_i^r)}) + \mu \dotp{\myvec{w}}{\myvec{w}} ,\\
%     \end{aligned}
% \end{equation}
% where $\phi$ is the same non-linear transformation and $B_r$ is a linear operator
% $$B_r: \mathcal{Y} \to \mathcal{Y} .$$
% \comm{Puede ser otro conjunto de llegada?, $w \in \mathcal{Z}$}
% The goal now is to define under which conditions we can state an analogous result to that of Proposition~\ref{prop:evgeniou1}. To do that, we replicate the proof of~\cite{EvgeniouMP05} replacing the matrix arguments for operator ones.
% \begin{proposition}\label{prop:evgeniou1}
%     If the linear operator $\mymat{B}$ is injective and we define the operator $\mymat{E}$ in equation~\eqref{eq:mtl_general_formulation_nonlinear} as to be $\mymat{E} = (\mymat{B}^* \mymat{B})^{-1}$ then we have that
%     \begin{equation}
%         \nonumber
%         S(\myvec{w}) = R(B^* \myvec{w}).
%     \end{equation}
%     and therefore $\myvec{u}^* = B^* \myvec{w}^*$.
% \end{proposition}

% If B is injective then B* is injective
% If B, B* are injective, B*B is injective
% We have two options:
%   1. We prove the same for surjective
%   2. We restrict the domain of B*B
% This allows us to define E properly

% For the other direction we have a problem because we need to decompose the operator E = T T* which is not trivial

\subsubsection*{Examples of Multi-Task Kernels}
Using the framework for Multi-Task learning with Kernel methods we can choose different regularizations, induced by the matrix $\mymat{E}$, which lead to different Multi-Task approaches. 

\paragraph*{Independent Tasks} The trivial case when $E = I_{\ntasks}$ and therefore $B =  I_{\ntasks}$ , that is 
$$B_r^\intercal =  (\overbrace{0}^1, \ldots, \overbrace{1}^{r}, \overbrace{0}^T), $$
and the kernel is
\begin{equation}
    \nonumber
    \hat{k}(x_i^r, x_j^s) = \dotp{B_r}{B_s} k(x_i^r, x_j^s) = (\delta_{rs}) k(x_i^r, x_j^s).
\end{equation}
This approach is not a proper MTL method because each task is learned separately, and no coupling is being enforced among tasks.

\paragraph*{Independent Parts with Shared Common Model}
When the matrix $B$ is selected such that its columns are
$$B_r^\intercal =  (\overbrace{0}^1, \ldots, \overbrace{1}^{r}, \overbrace{0}^T, \overbrace{\frac{1}{\mu}}^{T+1}), $$
the corresponding multi-task kernel is:
\begin{equation}
    \nonumber
    \hat{k}(x_i^r, x_j^s) = \dotp{B_r}{B_s} k(x_i^r, x_j^s) = ( \frac{1}{\mu} + \delta_{rs}) k(x_i^r, x_j^s)
\end{equation}
% Evgeniou
This is equivalent to the approach presented in the work of~\cite{EvgeniouP04} where it is named \emph{regularized MTL}.
The goal is to find a decision function for each task, each being defined by a vector
$$w_r = w + v_r,$$
where $w$ is common to all tasks and $v_r$ is task-specific.
The primal problem of \emph{regularized MTL} SVM, using the unified formulation, is 
\begin{equation}
    \label{eq:regmtlsvm_primal}
    \begin{aligned}
        & \argmin_{w, v_r, \xi_i^r}
        & & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \xi_i^r + \frac{1}{2} \dotp{w}{w} + \sum_{r=1}^\ntasks \frac{\mu}{2} \dotp{v_r}{v_r} \\
        & \text{s.t.}
        & & y_{i}^r ( \dotp{w}{x_{i}^r} + \dotp{v_r}{x_{i}^r}) \geq p_i^r - \xi_i^r ,\\
        & & &\xi_i^r \geq 0, \\
        & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
    \end{aligned}
\end{equation}
% Leveraging common and specific information
Note that $\mu$ is a parameter that controls the tradeoff between the relevance of common and specific models. That is, when $\mu$ tends to infinite, the resulting model approaches a common-task standard SVM; when $\mu$ tends to zero, a independent task approach is taken, with one standard SVM problem for each task.
This is also reflected in the corresponding dual problem
\begin{equation}\label{eq:regmtlsvm_dual}
    \begin{aligned}
        & \argmin_{\alpha_i} 
        & & \frac{1}{2} \sum_{r, s=1}^\ntasks \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s \dotp{x_i^r}{x_j^s} + \frac{1}{2 \mu} \sum_{r, s=1}^\ntasks  \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s \delta_{rs} \dotp{x_i^r}{x_j^s} \\
        & & & \qquad - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} p_i^r \alpha_i^r \\
        & \text{s.t.}
        & & 0 \leq \alpha_i^r \leq C \\
        & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
        \end{aligned}
\end{equation}
In this dual form, as $\mu$ grows, the task-specific part goes to zero, and the most important term is the first one, corresponding to the common part. The opposite effect is obtained when $\mu$ shrinks.
% Common + specific model which is equivalent to penalizing individual norm and variance
Moreover, in~\cite{EvgeniouP04} it is shown that solving~\eqref{eq:regmtlsvm_primal} is equivalent to solving the problem
\begin{equation}
    \nonumber
    \begin{aligned}
        & \argmin_{ww_r, \xi_i^r}
        & & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \xi_i^r +  \frac{1}{2} \sum_{r=1}^\ntasks \norm{w_r}^2 + \frac{\mu}{2} \sum_{r=1}^\ntasks  \norm{w_r - \sum_{s=1}^\ntasks w_s}^2 \\
        & \text{s.t.}
        & & y_{i}^r ( \dotp{w_r}{x_{i}^r}) \geq p_i^r - \xi_i^r ,\\
        & & &\xi_i^r \geq 0, \\
        & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
    \end{aligned}
\end{equation}
Now, only the $w_r$ variables are included, and it is clearer that $\mu$ penalizes the variance of the $w_r$ vectors, so all models $w_r$ will tend to a common model as $\mu$ grows.

This is a very interesting approach because, as it was pointed out in~\cite{LiangC08}, this approach has connections with the SVM+ approach from~\cite{VapnikI15a}.


\paragraph*{Graph Laplacian}
When the tasks are considered as nodes in a graph, and the weights of the edges of this graph portrait the relation between each pair of tasks,
the matrix $\mymat{E}$ can be seen as a Laplacian matrix $\mymat{L} = \mymat{D} - \mymat{A}$. Here $\mymat{A}$ is the adjacency matrix indicating the weights of the edges between each pair of tasks, and $\mymat{D}$ is the degree matrix, a diagonal matrix where each diagonal term is the sum of the corresponding row of $\mymat{A}$. Observe that using this matrix, the regularization term is 
\begin{align*}
    \myvec{u}^\intercal (L \otimes I) \myvec{u}  
    &=\sum_{r=1}^T \sum_{s=1}^T u_r^\intercal L_{rs} u_s \\
    &= \sum_{r=1}^T \sum_{s=1}^T u_r^\intercal (D - A)_{rs} u_s \\
    &= \sum_{r=1}^T  \sum_{s=1}^T u_r^\intercal \left(\delta_{rs} \sum_{q} A_{rq} \right) u_s - \sum_{r=1}^T  \sum_{s=1}^T u_r^\intercal A_{rs} u_s \\
    &=  \sum_{r=1}^T u_r^\intercal \sum_{q} A_{rq} u_r + \sum_{r=1}^T u_s^\intercal \sum_{q} A_{sq} u_s  - \sum_{r=1}^T  \sum_{s=1}^T u_r^\intercal A_{rs} u_s \\
    &=  \sum_{r=1}^T  \sum_{s=1}^T u_r^\intercal A_{rs} u_s + u_s^\intercal A_{rs} u_s - u_r^\intercal A_{rs} u_s \\
    &= \sum_{r=1}^T \sum_{s=1}^T A_{rs} \norm{u_r - u_s}^2 \; .
\end{align*}
That is, the distance between task models is penalized, weighted by the degree of similatiy between the tasks as indicated by the graph.
Here, the multi-task kernel is defined as 
\begin{equation}
    \nonumber
    \hat{k}(x_i^r, x_j^s) = \dotp{B_r}{B_s} k(x_i^r, x_j^s) = \left(\mymat{L}^{+}\right)_{rs} k(x_i^r, x_j^s) .
\end{equation}
Observe that the pseudoinverse is used because the Laplacian matrices are semipositive definite. 

%\subsection{Task-Specific Kernels for Kernel Methods}

% On learning vector-valued functions 2004

% Kernels for multi-task learning 2004

% Learning multiple tasks with kernel methods? 2005

% Multi-output learning via spectral filtering 2012

% Kernels for vector-valued functions: A review 2012

% Bounds for vector-valued function estimation 2016

% Operator-valued Kernels for Learning from Functional Response Data 2016



% \subsection{Connection with SVM+}
% % Cai and Cherkassky
% Another extension of the \emph{regularized MTL} SVM model can be found in 
% the multi-task problem described in~\cite{LiangC08} for classification, also adapted for regression problems in~\cite{CaiC09}. Using the unified formulation
% \begin{equation}\label{eq:mtlsvm_primal_unif}
%     \nonumber
%     \begin{aligned}
%         & \argmin_{w, b, v_r, b_r, \xi_i^r}
%         & & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \xi_i^r + \frac{1}{2} \dotp{w}{w} + \frac{\mu}{2} \sum_{r=1}^\ntasks \dotp{v_r}{v_r} \\
%         & \text{s.t.}
%         & & y_{i} ( \dotp{w}{\phi(x_{i}^r)} + b + \dotp{v_r}{\phi_r(x_{i}^r)} + b_r) \geq p_i^r - \xi_i^r ,\\
%         & & &\xi_i^r \geq 0, \\
%         & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
%     \end{aligned}
% \end{equation}
% Comparing~\eqref{eq:regmtlsvm_primal} and~\eqref{eq:mtlsvm_primal_unif} we observe that the subyacent idea is the same, but there exists some differences. In first place, note that~\eqref{eq:regmtlsvm_primal} is described as a linear model, while in~\eqref{eq:mtlsvm_primal_unif} not only non-linear transformations of the data are used, but different transformations can be selected $\phi, \phi_r$ for the common part and for each task-specific term, respectively. Moreover, it is relevant to note the incorporation of the bias terms in~\eqref{eq:mtlsvm_primal_unif}.
% The dual form of~\eqref{eq:mtlsvm_primal_unif} is
% \begin{equation}\label{eq:mtlsvm_dual_unif}
%     \begin{aligned}
%         & \argmin_{\alpha_i} 
%         & & \frac{1}{2} \sum_{r, s=1}^\ntasks \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s \left[k(x_i^r, x_j^s) + \delta_{rs} k_r(x^r_i, x^s_j) \right] - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} p_i^r \alpha_i^r \\
%         & \text{s.t.}
%         & & 0 \leq \alpha_i^r \leq C \\
%         & & & \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r} = 0, \\
%         & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
%         \end{aligned}
% \end{equation}
% In~\eqref{eq:mtlsvm_dual_unif}, $\ntasks$ equality constraints that are not present in~\eqref{eq:regmtlsvm_dual} have been added. This a direct consequence of the incorporation of bias terms in the primal formulation. Since the original SMO algorithm does not account for multiple equality constraints, in~\cite{CaiC12} a generalized SMO algorithm is developed.
% Also, it is important to observe the use of different kernel spaces through the functions $k$ and $k_r$.
% % Connection with LUPI!
% This has connections with the LUPI paradigm~\cite{VapnikI15a} and the proposed SVM+ that embodies this paradigm, as described in Subsection~\ref{subsec:ch3_lupi}. The kernel space for the common part is named the decision space, and the spaces corresponding to the kernel functions $k_r$ are the correcting spaces. That is, each task can independently correct the similarity defined by the common decision space.

% When is there a representer theorem? Vector versus matrix regularizers.

% Multi-task least-squares support vector machines. Shuo

% Multi-task Gaussian process prediction. Bonilla

% Sparse coding for multitask and transfer learning





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%%%%%%%%%             SECTION         %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Conclusions}\label{sec-conclusions-2}

In this chapter, we covered\dots
