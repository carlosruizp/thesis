% Chapter 4

\chapter{Adaptive Graph Laplacian Multi-Task Support Vector Machine} % Write in your own chapter title
\label{Chapter5}
\lhead{Chapter \ref{Chapter5}. 
\emph{Adaptive Graph Laplacian Multi-Task Support Vector Machine}} % Write in your own chapter title to set the page header

{\bf \small{

}}

\section{Introduction}

\section{Graph Laplacian Multi-Task Support Vector Machine}
In~\cite{RuizAD20} we proposed a convex formulation of the Graph Laplacian MTL SVM which includes a common regularization term and whose primal problem is
%
\begin{equation}\label{eq:primal_cvx-graphLap}
  \begin{aligned}
  & \argmin_{\myvec{w}, \myvec{v}_1, \ldots, \myvec{v}_T, b , \myvec{\xi}}
  & & { \sum_{r=1}^T C_r \sum_{i=1}^{n_r} {\xi_i^r}  + \frac{\nu}{2} \sum_{r=1}^T \sum_{s=1}^T A_{rs} {\| \myvec{v}_r - \myvec{v}_s \|}^2 + \frac{1}{2} \sum_r \norm{\myvec{v}_r}^2 + \frac{1}{2} \norm{\myvec{w}}^2} \\
  & \text{s.t.}
  & & y_i^r (\lambda (\myvec{w} \cdot \myvec{x}_i^r) + (1 - \lambda) (\myvec{v}_r \cdot \myvec{x}_i^r) + b_r) \geq p_i^r - \xi_i^r  ,\\
  & & & \xi_i^r \geq 0,  \;  i = 1, \dotsc, n_r, \; r=1, \dotsc, T .
  \end{aligned}
\end{equation}
%
% Note that with $\lambda=0$ this problem reduces to the one shown in~\cite{evgeniou2005learning}, since the vector $w$ would not have any influence in the errors $\xi_i^r$ and then $w = 0$ would be optimal.
% This formulation is also interesting because it combines in a convex manner a common and task-specific models which can be built using two different kernels, one for $w$ and another for the $v_r$ vectors;
% in other words,  
The corresponding Lagrangian is
\begin{equation}\label{eq:lagr_cvx-graphLap}
\begin{aligned}
        \mathcal{L}&(\myvec{w}, \myvec{v}_r, b_r, \xi_i^r, \myvec{\alpha}, \myvec{\beta}) \\
        &= \sum_{r=1}^T C_r \sum_{i=1}^{n_r}{\xi_{i}^r} + \frac{\nu}{2} \sum_{r=1}^T \sum_{s=1}^T A_{rs} \norm{\myvec{v}_r - \myvec{v}_s}^2 + \frac{1}{2} \sum_r \norm{\myvec{v}_r}^2  + \frac{1}{2} \norm{\myvec{w}}^2
        \\ &\quad  - \sum_{r=1}^T \sum_{i=1}^{n_r}{ \alpha_i^r [y_{i}^r (\lambda (\myvec{w} \cdot \myvec{x}_i^r) + (1 - \lambda) (\myvec{v}_r \cdot \myvec{x}_i^r) + b_r) - p_{i}^r + \xi_{i}^r]   } - \sum_{r=1}^T \sum_{i=1}^{n_r}{ \beta_i^r \xi_i^r }.
\end{aligned}
\end{equation}
Taking derivatives
\begin{align*}
    & \frac{\partial \mathcal{L}}{\partial \myvec{w}} = 0 \implies \myvec{w} = \lambda \sum_{r=1}^\ntasks \sum_{i=1}^{m_r}{\alpha_i^r y_i^r x_i^r}  \; , \\
    & \frac{\partial \mathcal{L}}{\partial \myvec{v}_r} = 0 \implies \myvec{v}_r + \sum_{s=1}^T (L_{rs} + L_{sr}) (\myvec{v}_r^* - \myvec{v}_s^*)= \sum_{i=1}^{m_r}{\alpha_i^r y_i^r x_i^r}  \; , \\
    & \frac{\partial \mathcal{L}}{\partial b_r} = 0 \implies  \sum_{i=1}^{m_r}{\alpha_i^r y_i^r } = 0  \; ,\\
    & \frac{\partial \mathcal{L}}{\partial \xi_i^r} = 0 \implies C_r - \alpha_i^r - \beta_i^r = 0 \; \; .
\end{align*}
Observe that
\begin{align*}
    \myvec{v}^\intercal (I_T \otimes I_d) \myvec{v} &= \sum_{r=1}^T \norm{v_r}^2 , \\
    \myvec{v}^\intercal (L \otimes I_d) \myvec{v} &= \frac{1}{2} \sum_{r=1}^T \sum_{s=1}^T A_{rs} \norm{v_r - v_s}^2 , 
\end{align*}
Proof:
\begin{align*}
    \myvec{v} (\mymat{L} \otimes \mymat{I}_d) \myvec{v} &= \myvec{v} (\mymat{D} \otimes \mymat{I}_d) \myvec{v} - \myvec{v} (\mymat{A} \otimes \mymat{I}_d) \myvec{v} \\
    &= \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks D_{rs} v_r^\intercal v_s - \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks A_{rs} v_r^\intercal v_s \\
    &= \sum_{r=1}^\ntasks D_{rr} v_r^\intercal v_r - \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks A_{rs} v_r^\intercal v_s \\
    &= \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks A_{rs} v_r^\intercal v_r - \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks A_{rs} v_r^\intercal v_s \\
    &= \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks A_{rs} (v_r^\intercal v_r - v_r^\intercal v_s)  \\
\end{align*}
If $A$ is symmetric, that is $A_{rs} = A_{sr}$, then
\begin{align*}
    \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks A_{rs} (v_r^\intercal v_r - v_r^\intercal v_s) &=
    \sum_{r=1}^\ntasks \sum_{s=r}^\ntasks \lbrace A_{rs}  (v_r^\intercal v_r - v_r^\intercal v_s) + A_{sr} (v_s^\intercal v_s - v_s^\intercal v_r) \rbrace \\
    &=
    \sum_{r=1}^\ntasks \sum_{s=r}^\ntasks \lbrace (A_{rs} + A_{sr})  (v_r^\intercal v_r + v_s^\intercal v_s - 2v_r^\intercal v_s) \rbrace \\
    &=
    \sum_{r=1}^\ntasks \sum_{s=r}^\ntasks \lbrace (A_{rs} + A_{sr})  \norm{w_r - w_s}^2 \rbrace \\
    &=
    \frac{1}{2} \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks \lbrace (A_{rs} + A_{sr})  \norm{w_r - w_s}^2 \rbrace \\
    &=
     \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks \lbrace A_{rs}  \norm{w_r - w_s}^2 \rbrace .\\
\end{align*}
\section{Adaptive Graph Laplacian Algorithm}

If we want to learn $A$ from data we would like the matrix to meet some requirements:
\begin{itemize}
    \item $A$ has to be symmetric, so we can express the regularizer using the Laplacian in the dual form.
    \item The rows of $A$ add up to 1. 
\end{itemize} 

\section{Experiments}

\section{Conclusions}\label{sec-conclusions-4}

In this chapter, we have\dots
