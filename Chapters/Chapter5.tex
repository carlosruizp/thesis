% Chapter 4

\chapter{Adaptive Graph Laplacian for Multi-Task Learning} % Write in your own chapter title
\label{Chapter5}
\lhead{Chapter \ref{Chapter5}.
    \emph{Adaptive Graph Laplacian Multi-Task Support Vector Machine}} % Write in your own chapter title to set the page header

{\bf \small{

    }}

%\section{Introduction}
In Chapter~\ref{Chapter3} we have divided the \acrfull{mtl} strategies into feature-based, parameter-based and combination-based ones.
The feature-based approaches, which try to find a representation shared by all tasks to obtain leverage in the learning process, rely on the assumption that all tasks can indeed share a common latent representation.
In the case of combination-based strategies, which combine a common part with task-specific parts in the models, a similar belief is held and, although this approach has many good properties, it relies on the assumption that all tasks can share the same common information.
However, this might not be the case in some \acrshort{mtl} scenarios, where there can exist groups of tasks that share some information, but are unrelated to the rest.

%
%Previous work
%
Some parameter-based approaches rely on enforcing low-rank matrices~\citep{AndoZ05,ChenTLY09,PongTJY10}, assuming, thus, that all tasks parameters belong to the same subspace.
Others try to find the underlying task structure, either by task-relation learning or by clustering the tasks. In the task-relation learning we find strategies using Gaussian Processes and a Bayesian approach such as in~\citet{BonillaCW07,ZhangY10}. We also have the \acrfull{gl} approaches, such as the works of~\citet{EvgeniouMP05} or~\citet{argyriou2013learning}, where the tasks are assumed to be nodes of a graph, and the goal is to learn the weights on the edges, which determine the degree of relationship between tasks.
In these works, iterated algorithms are used to learn both the model parameters and the graph of task relations.
The clustering approaches are similar: they also use specific regularizers and alternating algorithms to find the clusters.
However, these works using regularization as the mean to enforce the coupling between tasks are limited to linear models.

In general, in the \acrshort{gl} strategies, the idea is to penalize the distance between the parameters of different tasks. It is more natural for linear or kernel approaches, where the models for each task $r=1, \ldots, \ntasks$ are defined as
\begin{equation}
    \nonumber
    f_r(x) = w_r \cdot \phi({x}) + b_r
\end{equation}
and $\phi(x)$ is a transformation, which can be the identity $\phi(x)=x$ in linear models, or an implicit transformation to an \acrshort{rkhs} in kernel models.
Here, the models are determined by the parameters $w_r$, so pushing together these parameters enforces the models to be similar.
The idea is to assume that the relationship between tasks can be modelled using a graph; then, the adjacency matrix $\fm{A}$ of such graph is used to define the regularization
\begin{equation}
    \label{eq:gl_regularization}
    \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} \norm{w_r - w_s}^2 ,
\end{equation}
where $(A)_{rs}$ are positive scalars that weight the pairwise distances.
Although~\eqref{eq:gl_regularization} is easy to compute for the linear case, it is not that direct in the case of kernel models where, as shown by the Representer Theorem, the optimal parameters $w_r^*$ are elements of an \acrshort{rkhs}.

Besides, the choice of the weights $(A)_{rs}$ is not trivial, and it is crucial for the good performance of this strategy.
First, the values $(A)_{rs}$ have to be bounded, otherwise its interpretability is lost and, moreover, one term can dominate the sum, so only the models of two tasks would be enforced to be similar.
%
Even with bounded weights, in absence of expert knowledge to select them, it is necessary to find a procedure that finds a set of weights $(A)_{rs}$ that reflects the real relations between tasks.


In this chapter, a framework based on tensor products of \acrshort{rkhss} is presented in Section~\ref{sec:mtl_kernelmethods}, and it is applied for the \acrshort{gl} regularization with kernel methods in Section~\ref{sec:mtl_kernelmethods}. Moreover, in Section the \acrshort{gl} strategy is combined with the convex \acrshort{mtl} one presented in Chapter~\ref{Chapter4}; and we implement this convex \acrshort{gl} approach for the L1, L2 and LS-\acrshort{svm}.
%
Finally, a data-driven procedure to automatically select the weights of the adjacency matrix is given in Section~\ref{sec:adapconvexgl}.
























%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%             SECTION         %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Kernels for Multi-Task Learning}\label{sec:mtl_kernelmethods}

% Most multi-task methods are linear models, which may not be flexible enough to capture certain dependencies.
% Deep Learning is a very popular and cost-effective way of overcoming this problem. The final linear models are substituted by the neural network output and the parameters are learned together using back propagation.
% However, one of the main problems of deep learning is the lack of theoretical results and the non-convexity of the problems.
% Other alternative to extend the \acrshort{mtl} models non-linearly is by using the kernels.

% Most multi-task methods reviewed in Section~\ref{sec:ch3_overview} are based on linear models. This produces the models that are more interpretable and makes it easier to enforce some kind of coupling between tasks so there exists some kind of transfer learning. However, linear models are not powerful enough for most real-world problems, which might have non-linear properties.
% %

% Deep Learning is a very popular and cost-effective way of overcoming this problem. Neural Networks with multiple layers, where each layer may apply a non-linear transformation, are very powerful models that can estimate function using a hierarchical strategy where each new layer builds new features based on the output of the previous layer. 
% This idea is very useful in \acrshort{mtl}, where some features are jointly learned for all the tasks and, in the last layer, final task-specific linear models are build using them.
% %
% Despite its great success, Deep Learning has some inconveniences. One is that to estimate the extensive number of parameters involved in a deep architecture, large quantities of data are needed. This large datasets are not usually of public domain.
% Other problem found in Deep Learning is the lack of mathematical guarantees, at least when compared with other methods of Machine Learning.

% %
% The other alternative to extend \acrshort{mtl} models non-linearly is by using Kernel Methods. Kernel Methods, although they are more computationally challenging, offer some interesting characteristics. In the first place most problems using kernels are formulated as convex problems, which have a single optimum solution. Furthermore, they also offer some well-studied mathematical properties such as consistency of the methods, i.e. the algorithms will approximate this solution as the number of data grows, or rates of convergence, i.e. how fast this solution is approximated.
% Also, Kernel Methods, thanks to this mathematical properties, usually require less data than Deep Learning models to achieve competitive results. 

% % Evgeniou
% \subsection{Regularized \acrshort{mtl}}
% The work of~\cite{EvgeniouP04} presents a \acrshort{svm}-based \acrshort{mtl} problem formulation. 
% The goal is to find a decision function for each task, each being defined by a vector
% $$w_r = w + v_r,$$
% where $w$ is common to all tasks and $v_r$ is task-specific.
% The primal problem of \emph{regularized \acrshort{mtl}} \acrshort{svm}, using the unified formulation, is 
% \begin{equation}
%     \label{eq:regmtlsvm_primal}
%     \begin{aligned}
%         & \argmin_{w, v_r, \xi_i^r}
%         & & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \xi_i^r + \frac{1}{2} \dotp{w}{w} + \sum_{r=1}^\ntasks \frac{\mu}{2} \dotp{v_r}{v_r} \\
%         & \text{s.t.}
%         & & y_{i}^r ( \dotp{w}{x_{i}^r} + \dotp{v_r}{x_{i}^r}) \geq p_i^r - \xi_i^r ,\\
%         & & &\xi_i^r \geq 0, \\
%         & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
%     \end{aligned}
% \end{equation}
% % Leveraging common and specific information
% Note that $\mu$ is a parameter that controls the tradeoff between the relevance of common and specific models. That is, when $\mu$ tends to infinite, the resulting model approaches a common-task standard \acrshort{svm}; when $\mu$ tends to zero, a independent task approach is taken, with one standard \acrshort{svm} problem for each task.
% This is also reflected in the corresponding dual problem
% \begin{equation}\label{eq:regmtlsvm_dual}
%     \begin{aligned}
%         & \argmin_{\alpha_i} 
%         & & \frac{1}{2} \sum_{r, s=1}^\ntasks \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s \dotp{x_i^r}{x_j^s} + \frac{1}{2 \mu} \sum_{r, s=1}^\ntasks  \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s \delta_{rs} \dotp{x_i^r}{x_j^s} \\
%         & & & \qquad - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} p_i^r \alpha_i^r \\
%         & \text{s.t.}
%         & & 0 \leq \alpha_i^r \leq C \\
%         & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
%         \end{aligned}
% \end{equation}
% In this dual form, as $\mu$ grows, the task-specific part goes to zero, and the most important term is the first one, corresponding to the common part. The opposite effect is obtained when $\mu$ shrinks.
% % Common + specific model which is equivalent to penalizing individual norm and variance
% Moreover, in~\cite{EvgeniouP04} it is shown that solving~\eqref{eq:regmtlsvm_primal} is equivalent to solving the problem
% \begin{equation}
%     \nonumber
%     \begin{aligned}
%         & \argmin_{ww_r, \xi_i^r}
%         & & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \xi_i^r +  \frac{1}{2} \sum_{r=1}^\ntasks \norm{w_r}^2 + \frac{\mu}{2} \sum_{r=1}^\ntasks  \norm{w_r - \sum_{s=1}^\ntasks w_s}^2 \\
%         & \text{s.t.}
%         & & y_{i}^r ( \dotp{w_r}{x_{i}^r}) \geq p_i^r - \xi_i^r ,\\
%         & & &\xi_i^r \geq 0, \\
%         & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
%     \end{aligned}
% \end{equation}
% Now, only the $w_r$ variables are included, and it is clearer that $\mu$ penalizes the variance of the $w_r$ vectors, so all models $w_r$ will tend to a common model as $\mu$ grows.


% Kernels are functions that are used as a measure of similarity between data points, and have a notable relevance in \acrshort{ml} due to the success of kernel methods, such as the \acrshort{gp} or \acrshort{svm}. 
% %
% % Moreover, when using kernel methods, the parametric point of view for estimating a function of the primal formulation is changed for a non-parametric one in the dual problem, where the complexity of the model increase with the number of examples.
% However, as we have observed in the previous section, designing \acrshort{mtl} is not a trivial task, and the proposals are fewer than those of linear models or \acrshort{nns}. 
% Although the combination-based MTLSVM has a strong motivation in the \acrshort{lupi} paradigm, it assumes a common model for all tasks, which may not be always useful. Other approaches consider the pairwise task relations, so a more fine-grained inter-task coupling can be imposed.
% In this section, we give some definitions and propose a formulation to extend some useful results, heading in this direction, for linear models, namely those of~\cite{EvgeniouMP05}, to kernelized ones.

In \acrshort{mtl} different functions have to be estimated, and we would like to capture the degree of relation between the tasks as well. Using kernels for these goals imposes some new challenges, that can be tackled from different perspectives.
% % Learning Multiple Tasks with Kernel Methods
% One of them is interpreting the \acrshort{mtl} paradigm so that it can be seen as learning a vector-valued function, thus using a vector-valued \acrfull{rkhs}.
%
Here we propose a reformulation where a tensor product of scalar \acrshort{rkhss} is used instead of the vector-valued ones, which leads to some more general results, including another extension of the Representer Theorem.
%
Finally, we take these definitions to show how to use them to solve \acrshort{gl} \acrshort{mtl} problems and also give some examples of commonly used \acrshort{gl} \acrshort{mt} kernels.




\subsection{Tensor Product of Reproducing Kernel Hilbert Spaces}
Given two finite-dimensional vector spaces $V$ and $W$ over a field $F$, with dimensions $n$ and $m$, the tensor product of these spaces, $V \otimes W$, is associated with the bilinear map
\begin{equation}
    \nonumber
    \begin{aligned}
         & V \times W & \to V & \otimes W & \\
         & (v, w)     & \to v & \otimes w &
    \end{aligned},
\end{equation}
such that
\begin{equation}
    \label{eq:kron_product}
    \begin{bmatrix}
        v_1    \\
        \vdots \\
        v_n
    \end{bmatrix}
    \otimes
    \begin{bmatrix}
        w_1    \\
        \vdots \\
        w_n
    \end{bmatrix}
    =
    \begin{bmatrix}
        v_1 w_1 \\
        \vdots  \\
        v_1 w_n \\
        \vdots  \\
        v_m w_1 \\
        \vdots  \\
        v_m w_n \\
    \end{bmatrix} ;
\end{equation}
this product is also called Kronecker product.
The tensor product can be constructed from the basis of the vector spaces. Consider $B_V$ and $B_W$ the basis of $V$ and $W$, respectively; then the tensor product $V \otimes W$ is a vector space which has as basis the set $\set{v \otimes w, \; v \in B_V, \; w \in B_W}$.
%
Since this definition seems recursive, we can also consider the interpretation of the tensor product as a quotient space.
%Let $L$ be the vector space defined by the Cartesian product $V \times W$.
Let $R$ be the linear subspace of the Cartesian product $V \times W$ that is the span of the elements of one of the following forms:
\begin{align*}
    (v_1 + v_2, w) - (v_1, w) - (v_2, w), \\
    (v, w_1 + w_2) - (v, w_1) - (v, w_2), \\
    (sv, w) - s(v, w),                    \\
    (v, sw) - s(v, w),
\end{align*}
where $v, v_1, v_2 \in V$, $w, w_1, w_2 \in W$ and $s \in F$. Then, we can define the tensor product space as the quotient space $(V \times W)/R$. Observe that these are just the relations to ensure that the tensor product satisfies 
\begin{align*}
    (v_1 \otimes w) + (v_2 \otimes w) = ((v_1 + v_2) \otimes w),\\
    (v \otimes w_1) + (v \otimes w_2) = (v \otimes (w_1 + w_2)), \\
    s(v \otimes w) = (sv \otimes w),                    \\
    s(v \otimes w) = (v \otimes sw).
\end{align*}
More concretely, we can call this definition as the algebraic tensor product of vector spaces and denote it as $V \otimes_{\text{alg}} W$, which is equivalent to $V \otimes W$ in the finite dimensional case.
If $\hilbertspace_1$ and $\hilbertspace_2$ are finite-dimensional Hilbert spaces, then the tensor product space $\hilbertspace_1 \otimes \hilbertspace_2$ is a Hilbert space with inner product
\begin{equation}
    \label{eq:innerprod_tensor}
    \begin{aligned}
        \dotp{}{}:\; & (\rkhs_1 \otimes \rkhs_2) & \times &  & (\rkhs_1 \otimes \rkhs_2)     & \to & \reals                &                       &   \\
                     & (f_1 \otimes f_2)         &        &  & (\hat{f}_1 \otimes \hat{f}_2) & \to & \dotp{f_1}{\hat{f}_1} & \dotp{f_2}{\hat{f}_2} & .
    \end{aligned}
\end{equation}
%
Extending this definition to the infinite-dimensional case is not trivial, and we follow roughly the work of~\citet{Kadison1983}.
%Let $X_1, X_2$ be finite-dimensional vector spaces.
Consider the \acrshort{rkhss} $\hilbertspace_1$ of functions  $f: \Xspace_1 \to \reals$ and $\hilbertspace_2$ of functions $g: \Xspace_2 \to \reals$; with the quotient space definition we can define the tensor product space $H_1 \otimes_{\text{alg}} H_2$ of functions
\begin{equation}\nonumber
    \begin{aligned}
        f \otimes_{\text{alg}} g:\; & \Xspace_1 & \times &  & \Xspace_2 & \to &        & \reals   \\
                                    & x_1      &      &  & x_2      & \to & f(x_1) & g(x_2) ,
    \end{aligned}
\end{equation}
which is a pre-Hilbert space with the inner product defined in~\eqref{eq:innerprod_tensor}.
We can complete this space, i.e. include the limits of the Cauchy series, to get the corresponding Hilbert space~\citep{Kadison1983}, that we will denote as $\rkhs_1 \otimes \rkhs_2$.

% Also, consider $\hilbertspace_1 \otimes \hilbertspace_2$ as the space of the tensor product of two scalar-valued RKHS' $\hilbertspace_1$ and $\hilbertspace_2$ with reproducing kernels $K_1, K_2$, where the functions $f \in \hilbertspace_i$ are defined as $f: \Xspace_i \to \Yspace_i$ for $i=1, 2$. This tensor space is also a Hilbert space endowed with the inner product:
% \begin{equation}
%     \label{eq:innerprod_tensor}
%     \begin{aligned}
%         \dotp{}{}: &(\rkhs_1 \otimes \rkhs_2) &\times& &(\rkhs_1 \otimes \rkhs_2) &\to &\reals &&  \\
%     &(f_1 \otimes f_2) && &(\hat{f}_1 \otimes \hat{f}_2) &\to &\dotp{f_1}{\hat{f}_1} & \dotp{f_2}{\hat{f}_2} &.
%     \end{aligned}
% \end{equation}
% It is easy to check that this inner product is symmetric because the inner products of both $\hilbertspace_1$ and $\hilbertspace_2$ are symmetric. It is linear because the inner products of both $\hilbertspace_1$ and $\hilbertspace_2$ are linear and the tensor product is linear. Finally, it is positive definite since 
% $  \dotp{f_1}{f_1}  \dotp{f_2}{f_2} \geq 0$ and
% $\dotp{f_1}{f_1}  \dotp{f_2}{f_2} = 0 \implies \dotp{f_i}{f_i} = 0$ for $i=1$ or $i=2$; taking $i=1$ without loss of generality, then $f_1 = 0$, so $f_1 \otimes f_2 = 0 \in \rkhs_1 \otimes \rkhs_2$. 
To apply the Riesz Theorem in this Hilbert space it is necessary to check whether the evaluation functionals are continuous or, equivalently, bounded.
Recall that given an \acrshort{rkhs} $\rkhs$ of functions $f: \Xspace \to \reals$, the evaluation functionals $E_{x}$ are defined as
\begin{equation}
    \nonumber
    \begin{aligned}
        E_x :\; & \rkhs &\to & \; \reals \\
          & f     &\to & \; f(x) .
    \end{aligned}
\end{equation} 
%
\begin{proposition}[\acrshort{rkhs} as tensor product of \acrshort{rkhss}]\label{prop:bounded_tensorfun}
    Let $\rkhs_1$ and $\rkhs_2$ be \acrshort{rkhss} ; then
    the space $\hilbertspace = \hilbertspace_1 \otimes \hilbertspace_2$ , with evaluation functionals $E_{(x_1, x_2)} $ defined as
    $$E_{(x_1, x_2)} (f_1 \otimes f_2) =E_{x_1}(f_1) \otimes E_{x_2}(f_2) = f_1(x_1) \otimes f_2(x_2)$$
    for $x_1 \otimes x_2 \in \Xspace_1 \otimes \Xspace_2$,
    is an \acrshort{rkhs} with the inner product defined in~\eqref{eq:innerprod_tensor}.
\end{proposition}
\begin{proof}
    It is necessary to ensure that the operators $E_{(x_1, x_2)}$ are bounded. Given any $f_1 \otimes f_2 \in \hilbertspace_1 \otimes \hilbertspace_2$,
    $$ \norm{E_{(x_1, x_2)} (f_1 \otimes f_2)} = \dotp{f_1(x_1)}{f_1(x_1)} \dotp{f_2(x_2)}{f_2(x_2)} \leq \norm{f_1(x_1)}^2  \norm{f_2(x_2)}^2 .$$
    Since $\hilbertspace_1$ and $\hilbertspace_2$ are \acrshort{rkhss}, they have bounded evaluation functionals; then, the product
    $ \norm{f_1(x_1)}  \norm{f_2(x_2)} $
    is also bounded.
\end{proof}
%
Recall also that, according to the Riesz theorem~\cite{Whittaker1991ACI}, if the evaluation functionals $E_x$ of a Hilbert space $\rkhs$ are bounded, for every $x \in \Xspace$ there exists a single $k^x \in \rkhs$, such that 
$$ E_x(f) = \dotp{f}{k^x}, \forall f \in \hypspace .$$
Therefore, if we define a function $k(x, \hat{x}) = \dotp{k^x}{k^{\hat{x}}}$, it is a kernel because it is positive definite, and with $k(x, \cdot) = k^x$ we can also check that it is a reproducing kernel.
%
Now we define the kernel function for the tensor product of \acrshort{rkhss}.
\begin{proposition}
    Let $\rkhs_1$ and $\rkhs_2$ be \acrshort{rkhss} whose reproducing kernels are $K_1$ and $K_2$, respectively;
    then the function
    \begin{equation}
        \nonumber
        \begin{aligned}
            K_1 \otimes K_2: & (\Xspace_1 \times \Xspace_2) & \times &  & (\Xspace_1 \times \Xspace_2) & \to &                     & \reals              &   \\
                             & (x_1 , x_2)                  &        &  & (\hat{x}_1 , \hat{x}_2)      & \to & K_1(x_1, \hat{x}_1) & K_2(x_2, \hat{x}_2) & ,
        \end{aligned}
    \end{equation}
    is a reproducing kernel for the Hilbert space $\hilbertspace_1 \otimes \hilbertspace_2$.
\end{proposition}
\begin{proof}
    First, we define $(K_1 \otimes K_2)^{(x_1 , x_2)} =  K_1^{x_1} \otimes K_2^{x_2}  \in \rkhs_1 \otimes \rkhs_2$,
    where for $x_1 \in \Xspace_1$  $K_1^{x_1} \in \rkhs_1$ is the element that satisfies
    \begin{equation}\nonumber
        \dotp{K_1^{x_1}}{f} = f(x_1) , \; \forall f \in \rkhs_1 ,
    \end{equation}
    and for $x_2 \in \Xspace_2$  $K_2^{x_2} \in \rkhs_2$ is the element that satisfies
    \begin{equation}\nonumber
        \dotp{K_2^{x_2}}{g} = g(x_2) , \; \forall g \in \rkhs_2  ;
    \end{equation}
    these elements $K_1^{x_1}$ and $K_2^{x_2}$ exist because $\rkhs_1$ and $\rkhs_2$ are \acrshort{rkhss}.
    Moreover, the reproducing kernels of such spaces are defined as
    $$ K_1(x_1, \hat{x_1}) = \dotp{K_1^{x_1}}{K_1^{\hat{x_1}}} $$
    for $\rkhs_1$, and as 
    $$ K_2(x_2, \hat{x_2}) = \dotp{K_2^{x_2}}{K_2^{\hat{x_2}}} $$
    for $\rkhs_2$.
    %
    Then, $(K_1 \otimes K_2)^{(x_1 , x_2)}$ satisfies that for every $f \otimes g \in \rkhs_1 \otimes \rkhs_2$,
    \begin{equation}\nonumber
        \dotp{(K_1 \otimes K_2)^{(x_1 , x_2)}}{f \otimes g} = f(x_1) g(x_2) .
    \end{equation}
    The evaluation functionals are bounded as shown in the proof of Proposition~\ref{prop:bounded_tensorfun}, then we can apply the Riesz theorem: for every $(x_1 , x_2) \in \Xspace_1 \otimes \Xspace_2$ there exists a single element $K^{(x_1 , x_2)} \in \rkhs_1 \otimes \rkhs_2$ such that
    \begin{equation}
        \nonumber
        \dotp{K^{(x_1 , x_2)}}{f \otimes g} = f(x_1) \otimes g(x_2) = f(x_1) g(x_2)\;  \forall f \otimes g \in \rkhs_1 \otimes \rkhs_2 ;
    \end{equation}
    thus, this element must be $K^{(x_1 , x_2)} = (K_1 \otimes K_2)^{(x_1 , x_2)}$.
    %
    Then, we can define
    \begin{align*}
        (K_1 \otimes K_2)((x_1 , x_2), (\hat{x}_1 , \hat{x}_2)) & = \dotp{(K_1 \otimes K_2)^{(x_1 , x_2)}}{(K_1 \otimes K_2)^{(\hat{x}_1 , \hat{x}_2)}} \\
                                                                & = \dotp{K_1^{x_1} \otimes K_2^{x_2}}{K_1^{\hat{x}_1} \otimes K_2^{\hat{x}_2}}         \\
                                                                & = \dotp{K_1^{x_1}}{K_1^{\hat{x_1}}} \dotp{K_2^{x_2}}{K_2^{\hat{x_2}}}                 \\
                                                                & = K_1(x_1, \hat{x}_1) K_2(x_2, \hat{x}_2) .
    \end{align*}


    We observe that $K_1 \otimes K_2$ is symmetric and positive-definite because both $K_1$ and $K_2$ are symmetric and positive definite, %The proof is very similar to the symmetry and positive definiteness proof of the inner product~\eqref{eq:innerprod_tensor}.
    and, by construction, it has the reproducing property.
\end{proof}
%
Although these definitions are quite general and abstract, in our case of interest, \acrshort{mtl}, our spaces are $\Xspace_1 = \reals^\dimx$ and $\Xspace_2 = \reals^\ntasks$, that is, the feature space and the space of tasks.
%
Consider the \acrshort{rkhs} $\hypspace_\Xspace$ corresponding to the feature space $\Xspace = \reals^\dimx$, with functions $f: \reals^\dimx \to \reals$ and reproducing kernel $K_\Xspace$.
Also, consider the space of tasks is $\Tspace = \reals^\ntasks$, and let $M \in \reals^{\ntasks \times \ntasks}$ be a symmetric positive-definite matrix; thus, we can express it as $M = B^\intercal B$.
Using the feature map $\phi(u) = \fm{B} u$ for $u \in \reals^\ntasks$, we define in this space the kernel function as
$$ K_\Tspace(u, \hat{u}) = \dotp{\phi(u)}{\phi(\hat{u})} = \dotp{B u}{B \hat{u}} = \dotp{u}{M \hat{u}} = u^\intercal M \hat{u},$$
for every $u, \hat{u} \in \reals^\ntasks$. Here, $\rkhs_\ntasks$ is an \acrshort{rkhs}, isomorphic to $\reals^\ntasks$, with the reproducing kernel $K_\Tspace$.
%
Now, we can consider the tensor product space $\hypspace_\Xspace \otimes \reals^\ntasks$. Given an instance $x$ from task $t$, we can encode with the vector
\begin{equation}
    \label{eq:task_enc}
    e_t = (0, \ldots, \overbrace{1}^{t}, \ldots, 0);
\end{equation}
so $(\phi(x), e_t)$ is an element of $\hypspace_\Xspace \otimes \reals^\ntasks$ and $\phi$ satisfies that $K_\Xspace(x, \hat{x}) = \dotp{\phi(x)}{\phi(\hat{x})}$.
Therefore, the kernel of the space $\hypspace_\Xspace \otimes \reals^\ntasks$ is the following one:
\begin{equation}
    \nonumber
    \begin{aligned}
        (K_\Xspace \otimes K_\Tspace)((x_i , e_r), (x_j , e_s)) &= K_\Xspace(x_i, x_j) K_\Tspace(e_r, e_s) \\
        &= K_\Xspace(x_i, x_j) e_r^\intercal M e_s \\
        &= K_\Xspace(x_i, x_j) M_{rs} .
    \end{aligned}
\end{equation}
That is, although the theoretical framework of the tensor product of \acrshort{rkhss} is complex, the resulting kernel $(K_\Xspace \otimes K_\Tspace)$ is a quite natural result: the kernel between instances of different tasks is a product of the kernel between features and the kernel between tasks.
%
It is also important to observe that the matrix $\fm{M}$ defines the similarity among tasks, since the encoding defined as in~\eqref{eq:task_enc} does not provide information about such tasks. Unlike in the feature space, where the features trivially define the similarity between instances, in the space of tasks it is necessary to define an adequate matrix $M$.
We will call this kind of kernels \acrshort{mtl} kernels or \acrshort{mt} kernels.
\begin{definition}[\acrshort{mtl} kernel]\label{def:mtl_kernel}
    Given a kernel function $K_\Xspace$ defined on $\reals^\dimx \times \reals^\dimx$ and a positive definite matrix $M \in \reals^{(\ntasks \times \ntasks)}$, 
    a kernel function $K$ defined as
    \begin{equation}
        \nonumber
        \begin{aligned}
            K:\; & (\reals^\dimx \times \reals^\ntasks) & \times &  & (\reals^\dimx \times \reals^\ntasks) & \to &                     & \reals              &   \\
                             & (x_i, e_r)                  &        &  & (x_j , e_s)      & \to & K_\Xspace(x_i, x_j) & (e_r^\intercal M e_s) & 
        \end{aligned}
    \end{equation}
    is an \acrshort{mtl} kernel.
\end{definition}
%
This kind of kernels will be useful to express \acrshort{mtl} problems with a \acrshort{ctl} formulation.
%
They also have a close connection with operator-valued kernels, concretely with the particular case of separable kernels, concepts that we define in the Appendix~\ref{AppendixB}.

% Also, let $\Yspace$ be a finite-dimensional Hilbert space, so it is isomorphic to $\reals^\dimx$, and consider the kernel $K_\Yspace(y, \hat{y}) = \ydotp{y}{M \hat{y}}$. Since $M$ is symmetric and positive definite, we can write $M = B^\intercal B$. Then, considering the map $\Phi(y) = B y$, we can express the kernel as
% $$ K_\Yspace(y, \hat{y}) = \ydotp{y}{M \hat{y}} = \ydotp{\phi(y)}{\phi(\hat{y})},$$
% so $K_\Yspace(y, \hat{y})$ is a reproducing kernel for $\Yspace$.
% %\subsubsection*{Separable Kernels}
% To understand the connection between tensor product RKHS's and vector-valued RKHS's we study a special case of operator-valued kernels.
% A standard assumption is that the relation among the different outputs is independent of the pair $(x, \hat{x})$, that is, the kernel is separable~\citep{AlvarezRL12, KadriDPCRA16}.
% \begin{definition}[Separable Kernel]
%     An operator-valued kernel $K(x, \hat{x}): \Yspace \to \Yspace$ is called separable if
%     $$ K(x, \hat{x})  = k(x, \hat{x}) M$$
%     here $k(\cdot, \cdot)$ is a scalar-valued kernel and $M$ is some fixed operator $M \in \mathcal{L}(\mathcal{Y})$.
% \end{definition}
% That is, the operator $K(x, \hat{x})$ decouples in two parts: the similarity between $x$ and $\hat{x}$ measured by $k(\cdot,\cdot)$ and the interaction between the different outputs expressed by $M$.
%In those cases it is easier to express the operator-valued kernel as the tensor product of two spaces. 
%The following lemma shows how we can express such tensor product kernel.
%
% Consider the tensor product space $\Xspace \otimes \Tspace$, where $\Tspace$ is the space of tasks. We assume that $\Tspace$ has finite dimension, therefore it is isomorphic to $\reals^k$. We consider $\hilbertspace$ as the Hilbert space of functions $f: \Xspace \to \reals$ with reproducing kernel $K_\Xspace$, and $\Tspace = (\reals^k)^*$ as the space of linear functions $g: \reals^k \to \reals$, which is isomorphic to $\reals^k$. In $\reals^k$ we consider a general inner product induced by the positive definite matrix $A$.
% Then $(\reals^k)^*$, by the Riesz Representation Theorem, $ \forall x \in \reals^k, \exists u_x \in (\reals^k)^*$ such that $\forall w \in (\reals^k)^*$ 
% $$ \dotp{w}{u_x}_A = w x, $$
% that is, $u_x = A^{-1}x$.
% Then, the kernel is defined as
% $K_{\Tspace}(x, y) = \dotp{u_x}{u_y}_A = x^\intercal A^{-1} y$.
% Now we can define the following tensor product of two scalar-valued kernels:   
% \begin{equation}
%     \nonumber
%     \begin{aligned}
%         K_\Xspace \otimes K_\Tspace: &(\Xspace \otimes \reals^k) \times &(\Xspace \otimes \reals^k) &\to & \mathcal{L}(\reals) \otimes \mathcal{L}(\reals) \\
%     &(x \otimes z) &(\hat{x} \otimes \hat{z}) &\to & K_\Xspace(x, \hat{x}) \otimes K_\Tspace(z, \hat{z}).
%     \end{aligned}
% \end{equation}
% The function $K_\Xspace \otimes K_\Tspace$ is the reproducing kernel of $\hilbertspace \otimes (\reals^k)^*$.
% %

% Given a symmetric, positive definite operator $M$, we can interpret the separable kernels as the tensor product of kernels.
% Let $\Xspace$ a non-empty set and consider the \acrshort{rkhs} $\rkhs$ of functions $f: \Xspace \to \reals$, with reproducing kernel $k_\rkhs(\cdot, \cdot)$.
% Also, let $\Yspace$ be a finite-dimensional Hilbert space, so it is isomorphic to $\reals^\dimx$, and consider the kernel $K_\Yspace(y, \hat{y}) = \ydotp{y}{M \hat{y}}$. Since $M$ is symmetric and positive definite, we can write $M = B^\intercal B$. Then, considering the map $\Phi(y) = B y$, we can express the kernel as
% $$ K_\Yspace(y, \hat{y}) = \ydotp{y}{M \hat{y}} = \ydotp{\phi(y)}{\phi(\hat{y})},$$
% so $K_\Yspace(y, \hat{y})$ is a reproducing kernel for $\Yspace$.
% %\comm{\\TODO: Explicar en Chapter 2 cómo construir kernels escalares\\}
% Now, the separable operator-valued kernel
% $K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)$
% such that
% $K(x, \hat{x}) = k(x, \hat{x})M$
% with $M \in \mathcal{L}(\Yspace)$,
% can be expressed as
% $$ k_\rkhs \otimes K_\Yspace((x , y), (\hat{x} , \hat{y})) = k_\rkhs(x, \hat{x}) K_\Yspace(y, \hat{y}) = k_\rkhs(x, \hat{x}) \ydotp{y}{M \hat{y}} .$$
% Moreover, observe that using this kernel with the standard basis of $\Yspace$,  $\set{e_1, \ldots, e_\dimx}$,
% $$ k_\rkhs \otimes K_\Yspace((x , e_r) , (\hat{x} , e_s)) = k_\rkhs(x, \hat{x}) (M)_{rs}.$$
% This last result will be useful for expressing \acrshort{mtl} problems as standard or \acrshort{ctl} ones, as we will see next.

% \begin{lemma}
%     Let $\Yspace$ be a finite-dimensional Hilbert space and
%     $K: \Xspace \times \Xspace \to \mathcal{L}(\Yspace)$
%     be a separable operator-valued kernel, that is
%     $K(x, \hat{x}) = k(x, \hat{x})M$
%     with $M \in \mathcal{L}^+(\Yspace)$.   
%     Consider the kernel
%     $K_\Xspace \otimes K_\Yspace: (\Xspace \otimes \Yspace) \times (\Xspace \otimes \Yspace) \to \reals$
%     such that $K_\Xspace \otimes K_\Yspace((x \otimes z), (\hat{x} \otimes \hat{z})) = K_\Xspace(x, \hat{x}) K_\Yspace(z, \hat{z})$, with $K_\Yspace(z, \hat{z}) = \ydotp{z}{M \hat{z}}$ then the map $K \to K_\Xspace \otimes K_\Yspace$ is a bijection.
% \end{lemma}
% \begin{proof}
%     First, observe that $K_\Yspace$ is the reproducing kernel of $\Yspace$ with the inner product induced by the operator $M^{-1} \in \mathcal{L}^+(\mathcal{Y})$.
%     By Lemma~\ref{lemma:kernel_bijection} there exists a bijection between $K$ and $L$ where $L((x, z), (\hat{x}, \hat{z})) = \ydotp{z}{K(x, \hat{x}) \hat{z}}$. When $K(x, \hat{x}) = k(x, \hat{x})M$, we define 
%     $$ K_\Xspace \otimes K_\Yspace(x \otimes z, \hat{x} \otimes \hat{z}) = K_\Xspace(x, \hat{x}) K_\Yspace(z, \hat{z}) = K_\Xspace(x, \hat{x}) \ydotp{z}{M \hat{z}} = L((x, z), (\hat{x}, \hat{z})) ,$$
%     and the bijection is trivial by definition.
%     %

%     Moreover, observe that using this kernel with a basis of $\Yspace$,  $z=e_r, \hat{z}=e_s$, 
%     $$ K_\Xspace \otimes K_\Yspace((x \otimes e_r) , (\hat{x} \otimes e_s)) = K_\Xspace(x, \hat{x}) (M)_{rs}.$$
% \end{proof}
% This kind of kernels are called separable kernels~\citep{AlvarezRL12, KadriDPCRA16}, however their connection with operator-valued kernels is not very clear. This subsection tries to explain the construction of separable kernels and how they relate to operator-valued kernels.



\subsection{Kernel extensions for Multi-Task Learning} \label{subsec:kernels_mtl}
There exists a plethora of work about \acrshort{stl} Learning within regularization theory, where the problem to solve is
\begin{equation}
    \label{eq:stl_general_formulation}
    \begin{aligned}
        \min_{w} & \sum_{i=1}^{\nsamples} \lossf(y_i, \dotp{w}{\phi(x_i)}) + \lambda \dotp{w}{w} . \\
    \end{aligned}
\end{equation}
Here, $\lossf$ is the loss function and $\phi$ is a transformation to include non-linearity. Popular models such as Ridge Regression or SVMs are particular cases of this formulation for different choices of $\lossf$ and $\phi$.
One crucial result for this kind of problems is the {Representer Theorem}, which states that any minimizer of problem~\eqref{eq:stl_general_formulation} has the form
\begin{equation}
    \label{eq:representerth_sol}
    w = \sum_{i=1}^\nsamples c_j \phi(x_j) .
\end{equation}
%\comm{\\TODO: Explicar el Representer Theorem y Kernel Trick en Chapter 2 \\}
Given $w$ represented as in~\eqref{eq:representerth_sol}, we write
\begin{equation}
    \nonumber
    \dotp{w}{\phi(\hat{x})} = \sum_{i=1}^\nsamples c_j \dotp{\phi(x_j)}{\phi(\hat{x})}.
\end{equation}
This is very useful because we can apply the kernel trick and use the transformations $\phi$ only implicitly.
In this subsection it is shown how a broad class of \acrshort{mtl} problems can be expressed as regularized \acrfull{stl} problems.

%\subsubsection*{Linear \acrshort{mtl} Models}
Building upon the ideas discussed in~\cite{EvgeniouP04}, two useful results are presented in~\cite{EvgeniouMP05}, which show how we can apply \acrshort{stl} Learning methods to \acrshort{mtl} problems.
The first result~\cite[Proposition 1]{EvgeniouMP05} is given for linear models and illustrates under which conditions we can adapt these results in an \acrshort{mtl} context.
Consider the linear \acrshort{mtl} problem where we want to estimate the task parameters $u_r \in \reals^\dimx: r = 1, \ldots, T$, so we define $\myvec{u}^\intercal = (u_1^\intercal, \ldots, u_T^\intercal) \in \reals^{\ntasks \dimx}$. Then we want to minimize
\begin{equation}
    \nonumber
    \begin{aligned}
         & R(\myvec{u}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{u_r}{x_i^r}) + \mu \left(\myvec{u}^\intercal \mymat{E} \myvec{u}\right) , \\
    \end{aligned}
\end{equation}
where
\begin{equation}
    \nonumber
    %\label{eq:regularizer_matrixE}
    J(\myvec{u})= \myvec{u}^\intercal \mymat{E} \myvec{u}
\end{equation}
is the regularizer, and different choices of $J(\myvec{u})$, i.e. choices of the matrix $E$, can encode different beliefs about the task structure. For example, if $J(\myvec{u}) = \sum_{r=1}^T \norm{u_r}^2$ the problem decouples and we get independent task learning, so there is no relation among tasks; if $J(\myvec{u}) = \sum_{r, s=1}^T \norm{u_r - u_s}^2$ we are enforcing the parameters from different tasks to be close, so we expect all tasks to be related.
%

Then, Evgeniou \emph{et al.} propose to consider a vector $\myvec{w} \in \reals^p$ with $p \geq \ntasks \dimx$ such that we can express $\dotp{u_r}{x}$ as $\dotp{\mymat{B}_r^\intercal \myvec{w}}{x}$,  where $\mymat{B}_r$ is a $p \times \dimx$ matrix yet to be specified. One condition for $\mymat{B}_r$ is to be full rank $d$, so we can find such a $\myvec{w}$.
Note that we can also interpret $\mymat{B}_r$ as a feature map $f: \reals^\dimx \to \reals^p$ such that $\dotp{u_r}{x} = \dotp{\myvec{w}}{\mymat{B}_r x}$.
Observe that using the matrices $\mymat{B}_r$ we have the following kernel:
\begin{equation}
    \nonumber
    \hat{k}(x^r, y^s) = \hat{k}((x, r), (y, s)) = x^\intercal B_r^\intercal B_s y .
\end{equation}
Using these feature maps we would like to write the \acrshort{mtl} problem as a \acrshort{stl} problem
\begin{equation}
    \nonumber%\label{eq:mtl_as_stl}
    \begin{aligned}
         & S(\myvec{w}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{w}}{\mymat{B}_r x_i^r}) + \mu \dotp{\myvec{w}}{\myvec{w}} . \\
    \end{aligned}
\end{equation}
We also define the feature matrix $\mymat{B}$ as the concatenation $\mymat{B} = (B_r: r=1, \ldots, T) \in \reals^{p \times \ntasks \dimx}$, then we present the first result of~\cite{EvgeniouMP05}.
\begin{proposition}\label{prop:evgeniou1}
    Consider the \acrshort{mtl} problem 
    \begin{equation}
        \label{eq:mtl_general_formulation}
        \begin{aligned}
             & R(\myvec{u}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{u_r}{x_i^r}) + \mu \left(\myvec{u}^\intercal \mymat{E} \myvec{u}\right) , \\
        \end{aligned}
    \end{equation}
    and the \acrshort{ctl} problem
    \begin{equation}
        \label{eq:mtl_as_stl}
        \begin{aligned}
             & S(\myvec{w}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{w}}{\mymat{B}_r x_i^r}) + \mu \dotp{\myvec{w}}{\myvec{w}} ; \\
        \end{aligned}
    \end{equation}
    if the feature matrix $\mymat{B}$ has full rank and we define the matrix $\mymat{E}$ in equation~\eqref{eq:mtl_general_formulation} as $\mymat{E} = (\mymat{B}^\intercal \mymat{B})^{-1}$, then
    \begin{equation}
        \nonumber
        S(\myvec{w}) = R(B^\intercal \myvec{w}),
    \end{equation}
    and therefore $\optim{\myvec{u}} = B^\intercal \optim{\myvec{w}}$.
\end{proposition}
One important consequence of this result is that since we can solve the \acrshort{mtl} problem~\eqref{eq:mtl_general_formulation} as the STL problem~\eqref{eq:stl_general_formulation} with $\phi$ being the identity function, then we can apply the {Representer Theorem}. That is, the solution $\optim{\myvec{w}}$ of problem~\eqref{eq:stl_general_formulation} has the form
\begin{equation}
    \nonumber
    \myvec{w} = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r B_r x_i^r .
\end{equation}
Here, we are using the transformations $\phi(x_i^r) = B_r x_i^r$, and the prediction of a new instance $\hat{x}^s$ can be expressed as
\begin{equation}
    \nonumber
    \dotp{\myvec{w}}{\hat{x}^s} = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r  (x_i^r)^\intercal B_r^\intercal B_s \hat{x}^s = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r  \hat{k}(x_i^r, \hat{x}^s).
\end{equation}

%\subsubsection*{A Posteriori Kernel Extension of \acrshort{mtl} Models}
Evgeniou \emph{et al.} also extend these results to kernelized models. In the following lemma~\cite[Lemma 2]{EvgeniouMP05} they give the conditions under which the extension is possible.

\begin{lemma}\label{lemma:evgeniou_2}
    Given a space $\mathcal{T}$ such that for every $r=1, \ldots, T$  there are prescribed mappings
    $\psi_r: \Xspace \to \mathcal{T}$, if $G$ is a kernel on $\mathcal{T} \times \mathcal{T}$, then
    \begin{equation}
        \label{eq:evgeniou_lemma2}
        K((x, r), (\hat{x}, s)) = G(\psi_r(x), \psi_s(t)),\; x, \hat{x} \in \Xspace,\; r,s = 1, \ldots, \ntasks ,
    \end{equation}
    is a kernel that incorporates the multi-task information.
\end{lemma}
%This defines $K$ as a semi-positive functional over the product space $\Xspace \times \mathcal{T}$, which is named a multi-task kernel.
The mappings described in~\cite{EvgeniouMP05} are
$$ \psi_r(x) = B_r x ,$$
where $B_r$ are the $p \times d$ matrices previously defined.
Then, two examples of multi-task kernels using this lemma are given. The linear kernel is defined as
$$ K((x, r), (y, s)) = x^\intercal B_r^\intercal B_s y $$
and the multi-task Gaussian kernel is defined as
$$ K((x, r), (y, s)) = \exp\left(-\gamma \norm{B_r x - B_s y}^2 \right) .$$
That is, applying Proposition~\ref{prop:evgeniou1}, since $E^{-1} = B^\intercal B$, we can incorporate the task-regularizer information into the Gaussian kernel using that
\begin{align*}
    \norm{B_r x - B_s y}^2
     & = x^\intercal B_r^\intercal B_r x + y^\intercal B_s^\intercal B_s y - 2 x_r^\intercal B_r^\intercal B_s y_s \\
     & = x^\intercal (E^{-1})_{rr} x + y^\intercal (E^{-1})_{ss} y - 2 x_r^\intercal (E^{-1})_{rs} y_s ,
\end{align*}
where $(E^{-1})_{rs}$ is the $s$-th column of the $r$-th row of matrix $E^{-1}$.
That is, we use the task information in the original space and then apply the non-linear transformation implicitly using the kernel trick.
%
This approach has some limitations, because the task information is applied before the non-linear transformation, and it is not clear how this task information is transformed in the kernel space.
We will see in the next section how we can use an alternative kernel extension in which the task information, and the kernel that defines the similarity between features are decoupled.
% In this chapter we will focus on this approach, named \acrshort{gl}, we will develop it using linear models and extend it to the kernelized case, using our tensor product of \acrshort{rkhss} framework.


% However, there is another alternative for the kernel extension: first apply the non-linear transformation and then use the task information in the augmented space.
% To do that we need to extend the notions of matrices to operators in (potentially infinite-dimensional) Hilbert spaces. Extending the general \acrshort{mtl} formulation of~\eqref{eq:mtl_general_formulation}
% \begin{equation}
%     \label{eq:mtl_general_formulation_nonlinear}
%     \begin{aligned}
%         &R(\myvec{u}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{u_r}{\phi(x_i^r)}) + \mu J(\myvec{u}) ,\\
%     \end{aligned}
% \end{equation}
% where $\phi$ is a non-linear transformation
% $\phi: \Xspace \to \mathcal{Y}$ where $\mathcal{Y}$
%  is a Hilbert space and 
% $ J(\myvec{u}) = \dotp{u}{\mymat{E} u} = \dotp{u \mymat{E}}{u}$
% where $E$ is semi-positive definite linear operator in $\mathcal{Y}$.
% Consider also the non-linear extension of~\eqref{eq:mtl_as_stl}:
% \begin{equation}
%     \label{eq:mtl_as_stl_nonlinear}
%     \begin{aligned}
%         &S(\myvec{w}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{w}}{\mymat{B}_r \phi(x_i^r)}) + \mu \dotp{\myvec{w}}{\myvec{w}} ,\\
%     \end{aligned}
% \end{equation}
% where $\phi$ is the same non-linear transformation and $B_r$ is a linear operator
% $$B_r: \mathcal{Y} \to \mathcal{Y} .$$
% \comm{Puede ser otro conjunto de llegada?, $w \in \mathcal{Z}$}
% The goal now is to define under which conditions we can state an analogous result to that of Proposition~\ref{prop:evgeniou1}. To do that, we replicate the proof of~\cite{EvgeniouMP05} replacing the matrix arguments for operator ones.
% \begin{proposition}\label{prop:evgeniou1}
%     If the linear operator $\mymat{B}$ is injective and we define the operator $\mymat{E}$ in equation~\eqref{eq:mtl_general_formulation_nonlinear} as to be $\mymat{E} = (\mymat{B}^* \mymat{B})^{-1}$ then we have that
%     \begin{equation}
%         \nonumber
%         S(\myvec{w}) = R(B^* \myvec{w}).
%     \end{equation}
%     and therefore $\optim{\myvec{u}} = B^* \optim{\myvec{w}}$.
% \end{proposition}

% If B is injective then B* is injective
% If B, B* are injective, B*B is injective
% We have two options:
%   1. We prove the same for surjective
%   2. We restrict the domain of B*B
% This allows us to define E properly

% For the other direction we have a problem because we need to decompose the operator E = T T* which is not trivial

%



% Here, the multi-task kernel is defined as 
% \begin{equation}
%     \nonumber
%     \hat{k}(x_i^r, x_j^s) = \dotp{B_r}{B_s} k(x_i^r, x_j^s) = \left(\mymat{L}^{+}\right)_{rs} k(x_i^r, x_j^s) .
% \end{equation}
% Observe that the pseudoinverse is used because the Laplacian matrices are positive semidefinite. 

%\subsection{Task-Specific Kernels for Kernel Methods}

% On learning vector-valued functions 2004

% Kernels for \acrshort{mtl} 2004

% Learning multiple tasks with kernel methods? 2005

% Multi-output learning via spectral filtering 2012

% Kernels for vector-valued functions: A review 2012

% Bounds for vector-valued function estimation 2016

% Operator-valued Kernels for Learning from Functional Response Data 2016



% \subsection{Connection with \acrshort{svm}+}
% % Cai and Cherkassky
% Another extension of the \emph{regularized \acrshort{mtl}} \acrshort{svm} model can be found in 
% the multi-task problem described in~\cite{LiangC08} for classification, also adapted for regression problems in~\cite{CaiC09}. Using the unified formulation
% \begin{equation}\label{eq:mtlsvm_primal_unif}
%     \nonumber
%     \begin{aligned}
%         & \argmin_{w, b, v_r, b_r, \xi_i^r}
%         & & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \xi_i^r + \frac{1}{2} \dotp{w}{w} + \frac{\mu}{2} \sum_{r=1}^\ntasks \dotp{v_r}{v_r} \\
%         & \text{s.t.}
%         & & y_{i} ( \dotp{w}{\phi(x_{i}^r)} + b + \dotp{v_r}{\phi_r(x_{i}^r)} + b_r) \geq p_i^r - \xi_i^r ,\\
%         & & &\xi_i^r \geq 0, \\
%         & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
%     \end{aligned}
% \end{equation}
% Comparing~\eqref{eq:regmtlsvm_primal} and~\eqref{eq:mtlsvm_primal_unif} we observe that the subyacent idea is the same, but there exists some differences. In first place, note that~\eqref{eq:regmtlsvm_primal} is described as a linear model, while in~\eqref{eq:mtlsvm_primal_unif} not only non-linear transformations of the data are used, but different transformations can be selected $\phi, \phi_r$ for the common part and for each task-specific term, respectively. Moreover, it is relevant to note the incorporation of the bias terms in~\eqref{eq:mtlsvm_primal_unif}.
% The dual form of~\eqref{eq:mtlsvm_primal_unif} is
% \begin{equation}\label{eq:mtlsvm_dual_unif}
%     \begin{aligned}
%         & \argmin_{\alpha_i} 
%         & & \frac{1}{2} \sum_{r, s=1}^\ntasks \sum_{i, j=1}^{\npertask_r} y_i^r y_j^s \alpha_i^r \alpha_j^s \left[k(x_i^r, x_j^s) + \delta_{rs} k_r(x^r_i, x^s_j) \right] - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} p_i^r \alpha_i^r \\
%         & \text{s.t.}
%         & & 0 \leq \alpha_i^r \leq C \\
%         & & & \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r} = 0, \\
%         & \text{for } & & r=1, \ldots, \ntasks; \; i=1, \ldots, \npertask_r.
%         \end{aligned}
% \end{equation}
% In~\eqref{eq:mtlsvm_dual_unif}, $\ntasks$ equality constraints that are not present in~\eqref{eq:regmtlsvm_dual} have been added. This a direct consequence of the incorporation of bias terms in the primal formulation. Since the original SMO algorithm does not account for multiple equality constraints, in~\cite{CaiC12} a generalized SMO algorithm is developed.
% Also, it is important to observe the use of different kernel spaces through the functions $k$ and $k_r$.
% % Connection with \acrshort{lupi}!
% This has connections with the \acrshort{lupi} paradigm~\cite{VapnikI15a} and the proposed \acrshort{svm}+ that embodies this paradigm, as described in Subsection~\ref{subsec:ch3_lupi}. The kernel space for the common part is named the decision space, and the spaces corresponding to the kernel functions $k_r$ are the correcting spaces. That is, each task can independently correct the similarity defined by the common decision space.

% When is there a representer theorem? Vector versus matrix regularizers.

% Multi-task least-squares support vector machines. Shuo

% Multi-task Gaussian process prediction. Bonilla

% Sparse coding for multitask and transfer learning
































































\section{Graph Laplacian Multi-Task Learning with Kernel Methods}\label{sec:graphlap}

%\comm{TODO: Motivar kernel extension, es dificil y usamos tensores}
Although the \acrshort{gl} approach had already been proposed in~\citet{EvgeniouMP05}, it is restricted to linear models, or to a kernel extension where the task information of the Laplacian is included before the kernel-related transformation.
Here, we use the framework of tensor product of \acrshort{rkhss} to extend the \acrshort{gl} formulation and consider to incorporate the Laplacian information in the Hilbert space, after the kernel-related transformation has been applied.

We will first develop the linear case of the \acrshort{svm}-based \acrshort{gl} \acrshort{mtl} formulation. After this, we will present a framework based on tensor products to extend the \acrshort{gl} formulation to the kernel case.

\subsection{Linear Case}
We start from the simplest scenario, a linear approach, that is, given a $\dimx$-dimensional input space $\Xspace$, e.g. $\reals^\dimx$, we consider models of the form
$$ h_r(\cdot) = \dotp{w_r}{\cdot} + b_r,\;  w_r \in \reals^\dimx .$$
First, observe that we can express the Laplacian regularization as
\begin{equation}
    \nonumber
    \begin{aligned}
        \Omega(w_1, \ldots, w_\ntasks) &= \sum_{r=1}^T \sum_{s=1}^T (A)_{rs} \norm{w_r - w_s}^2 \\
        &=  \sum_{r=1}^T \sum_{s=1}^T (A)_{rs} \left\{ \norm{w_r}^2 + \norm{w_s}^2 - 2 \langle w_r, w_s \rangle \right\}.
    \end{aligned}
\end{equation}
Here, only the distance between model parameters is penalized, and in the extreme case where all the tasks can use the same model, i.e. $w_r = w$, the regularization for such model $w$ would be $0$.
This regularization can be combined with the individual model regularizers for each task.
To illustrate this, we first consider a linear L1-SVM, whose primal problem, using the unified formulation for regression and classification, is
\begin{equation}\label{eq:linear_gl_primal}
    \begin{aligned}
         & \argmin_{\fv{w}, \fv{b}, \fv{\xi}}
         &                                    & {C \sum_{r=1}^T \sum_{i=1}^m{\xi_{i}^r} + \frac{\nu}{4} \sum_{r=1}^T \sum_{s=1}^T (A)_{rs} \norm{w_r - w_s}^2 + \frac{1}{2} \sum_r \norm{w_r}^2 }                                                               \\
         & \text{s.t.}
         &                                    & y_{i}^r ( w_r \cdot x_{i}^r + b_r) \geq p_{i}^r - \xi_{i}^r , \;  i=1,\ldots,m_r; \;  r=1,\ldots,T,                                                                                                           \\
         &                                    &                                                                                                                                                 & \xi_{i}^r \geq 0, \;  i=1,\ldots,m_r; \;  r=1,\ldots,T \; ,
    \end{aligned}
\end{equation}
where we are using the vectors
\begin{equation}
    \nonumber
    \fv{w}^\intercal = (w_1^\intercal, \ldots, w_\ntasks^\intercal), \; \fv{b} = (b_1, \ldots, b_\ntasks), \; \fv{\xi} = (\xi_{1}^1, \ldots, \xi_{\npertask_\ntasks}^\ntasks).
\end{equation}
The corresponding Lagrangian is
\begin{equation}\label{eq:svmmtl_lagr_GL_addreg}
    \begin{aligned}
        \mathcal{L} & (\fv{w}, \fv{b}, \fv{\xi}, \fv{\alpha}, \fv{\beta}) = C \sum_{r=1}^T \sum_{i=1}^{m_r}{\xi_{i}^r} + \frac{\nu}{2} \sum_{r=1}^T \sum_{s=1}^T (A)_{rs} \norm{w_r - w_s}^2 + \frac{1}{2} \sum_r \norm{w_r}^2 \\
                    & - \sum_{r=1}^T \sum_{i=1}^{m_r}{ \alpha_i^r \left( y_{i}^r (w_r \cdot x_{i}^r + b_r) - p_{i}^r + \xi_{i}^r \right)  } - \sum_{r=1}^T \sum_{i=1}^{m_r}{ \beta_i^r \xi_i^r } \; ,
    \end{aligned}
\end{equation}
with $\fv{\alpha} = (\alpha_{1}^1, \ldots, \alpha_{\npertask_\ntasks}^\ntasks)$ and $\fv{\beta} = (\beta_{1}^1, \ldots, \beta_{\npertask_\ntasks}^\ntasks)$;
then, taking the derivatives of the Lagrangian with respect to the primal variables and equating them to $0$, we get
\begin{align}
     & \frac{\partial \mathcal{L}}{\partial w_r} = 0 \implies  w_r + \frac{\nu}{2} \sum_{s=1}^T ((A)_{rs} + (A)_{sr}) (w_r - w_s)= \sum_{i=1}^{m_r}{\alpha_i^r y_i^r x_i^r} \label{eq:partial_w_r_addreg} \; , \\
     & \frac{\partial \mathcal{L}}{\partial b_r} = 0 \implies  \sum_{i=1}^{m_r}{\alpha_i^r y_i^r } = 0 \label{eq:partial_b_r_addreg} \; ,                                                                        \\
     & \frac{\partial \mathcal{L}}{\partial \xi_i^r} = 0 \implies C_r - \alpha_i^r - \beta_i^r = 0 \; \label{eq:partial_xi_addreg}\; .
\end{align}
Consider
% the following vectors definitions
% \begin{equation}
%     \nonumber
%     \underset{1 \times Td}{\fv{w}^\intercal} = (w_1^\intercal \ldots w_T^\intercal)
%     , \; 
%     \underset{1 \times (\sum_r m_r)}{\fv{\alpha}^T} = (\fv{\alpha}_1^\intercal \ldots \fv{\alpha}_T^\intercal)
%     , \; 
%     \underset{1 \times m_r}{\fv{\alpha}_r^\intercal} =  (\alpha_1^r \ldots \alpha_{m_r}^r) ;
% \end{equation}
% and consider also
the following matrices: %and corresponding dimensions
\begin{equation*}
    \underset{\ntasks \times \ntasks}{L} =
    \begin{bmatrix}
        \sum_{s \neq 1} (A)_{1s} & - (A)_{12}      & \ldots & - (A)_{1T}                            \\
        \vdots                 & \vdots        & \ddots & \vdots                              \\
        (A)_{\ntasks 1}          & (A)_{\ntasks 2} & \ldots & \sum_{s \neq \ntasks} (A)_{\ntasks s}
    \end{bmatrix}, \; 
    \underset{\ntasks \times \ntasks}{E} = \left\lbrace I_T + \nu L \right\rbrace, \;
    \underset{\ntasks \dimx \times \ntasks \dimx}{E_\otimes} = E \otimes I_d, 
\end{equation*}
where $\otimes$ here is the Kronecker product of matrices, such that
\begin{equation}
    \nonumber
    \begin{bmatrix}
        \fm{E}_{11} & \fm{E}_{12} & \ldots & \fm{E}_{1 \ntasks} \\
        \vdots & \vdots & \ddots & \vdots \\
        \fm{E}_{\ntasks 1} & \fm{E}_{12} & \ldots & \fm{E}_{\ntasks \ntasks} \\
    \end{bmatrix} \otimes I_d
    = 
    \begin{bmatrix}
        \fm{E}_{11} I_d& \fm{E}_{12}  I_d& \ldots & \fm{E}_{1 \ntasks} I_d \\
        \vdots & \vdots & \ddots & \vdots \\
        \fm{E}_{\ntasks 1} I_d & \fm{E}_{12} I_d & \ldots & \fm{E}_{\ntasks \ntasks} I_d \\
    \end{bmatrix} .
\end{equation}
We also define the matrix
\begin{equation*}
    \underset{(\sum_r m_r) \times Td}{\Phi} =
    \begin{bmatrix}
        Y_1 X_1    & 0      & \ldots & 0      \\
        0      & Y_2 X_2    & \ldots & 0      \\
        \vdots & \vdots & \ddots & \vdots \\
        0      & 0      & \ldots & Y_\ntasks X_\ntasks
    \end{bmatrix} ,
\end{equation*}
where $\fm{X}_r$ is the data matrix with the examples from task $r$ and 
\begin{equation}
    \nonumber
    Y_r = 
    \begin{bmatrix}
        y_1^r & 0 & \ldots & 0 \\
        0 & y_2^r & \ldots & 0 \\
        \vdots & \vdots & \ddots & 0 \\
        0 & 0 & \ldots & y_{\npertask_r}^r 
    \end{bmatrix}.
\end{equation}
Then, we can write~\eqref{eq:partial_w_r_addreg} in a matrix formulation as
\begin{equation}
    \nonumber
    E_\otimes \fv{w} = \Phi^\intercal \fv{\alpha} \implies \fv{w} = (E_\otimes)^{-1} \Phi^\intercal \fv{\alpha} .
\end{equation}
With these definitions and replacing~\eqref{eq:partial_b_r_addreg} and~\eqref{eq:partial_xi_addreg} in the Lagrangian, the result is
\begin{align*}
    \mathcal{L}(\fv{\alpha}) & = \frac{1}{2} \fv{w}^\intercal E_\otimes \fv{w} - \fv{\alpha}^\intercal \Phi \fv{w} + p^\intercal \fv{\alpha}                                                                                                                  \\
                                     & = \frac{1}{2} ((E_\otimes)^{-1} \Phi^\intercal \fv{\alpha})^\intercal E_\otimes (E_\otimes)^{-1} \Phi^\intercal \fv{\alpha} - \fv{\alpha}^\intercal \Phi (E_\otimes)^{-1} \Phi^\intercal \fv{\alpha} + p^\intercal \fv{\alpha} \\
                                     & = \frac{1}{2} \fv{\alpha}^\intercal \Phi ({E_\otimes^\intercal})^{-1} \Phi^\intercal \fv{\alpha}  - \fv{\alpha}^\intercal \Phi (E_\otimes)^{-1} \Phi^\intercal \fv{\alpha} + p^\intercal \fv{\alpha}                           \\
                                     & = -\frac{1}{2}  \fv{\alpha}^\intercal \Phi (E_\otimes)^{-1} \Phi^\intercal \fv{\alpha} + p^\intercal \fv{\alpha}.
\end{align*}
Here, the block structure of $\Phi$ and $E_\otimes$ makes it possible to write $\Phi (E_\otimes)^{-1} \Phi^\intercal$ as
\begin{equation}
    \label{eq:laplacian_block_linear}
    \begin{bmatrix}
        \left(E^{-1}\right)_{11} X_1 Y_1 Y_1 X_1^\intercal             & \left(E^{-1}\right)_{12} X_1 Y_1 Y_2 X_2^\intercal              & \ldots & \left(E^{-1}\right)_{1\ntasks} X_1 Y_1 Y_\ntasks X_\ntasks^\intercal              \\
        \left(E^{-1}\right)_{21} X_2 Y_2 Y_1  X_1^\intercal             & \left(E^{-1}\right)_{22} X_2 Y_2 Y_2 X_2^\intercal              & \ldots & \left(E^{-1}\right)_{2\ntasks} X_2 Y_2 Y_\ntasks X_\ntasks^\intercal              \\
        \vdots                                                 & \vdots                                                  & \ddots & \vdots                                                              \\
        \left(E^{-1}\right)_{\ntasks1} X_\ntasks Y_\ntasks Y_1 X_1^\intercal & \left(E^{-1}\right)_{\ntasks 2} X_\ntasks Y_\ntasks Y_2 X_2^\intercal & \ldots & \left(E^{-1}\right)_{\ntasks \ntasks} X_\ntasks Y_\ntasks Y_\ntasks X_\ntasks^\intercal \\
    \end{bmatrix} .
\end{equation}
Here, we note that $E = I_\ntasks + \nu L$ is invertible because it is a strictly diagonally dominant matrix. Observe that $L$ is diagonally dominant, because it is a Laplacian matrix, but not strictly so. However, with the addition of the identity matrix $I_\ntasks$, this property becomes strict. The identity matrix has its origin in the individual regularization of the primal problem; thus, this additional regularization ultimately makes the solution of the problem more numerically stable.

Now we can define the kernel matrix $\widetilde{Q} = \Phi (E_\otimes)^{-1} \Phi^\intercal$, such that its corresponding the kernel function is
\begin{equation}
    \label{eq:kernelfun_gl}
    \widetilde{k}(x_i^r, x_j^s) =  \left((I_T + \nu L)^{-1}\right)_{rs} \dotp{x_i^r}{x_j^s},
\end{equation}
so the block $Q[{rs}]$, corresponding to the $r$-th and $s$-th task, is defined as
$$ Q[{rs}]_{ij} = y_i^r y_j^s \widetilde{k}(x_i^r, x_j^s) .$$
Using this kernel matrix, the corresponding dual problem can be expressed as
\begin{equation}\label{eq:linear_gl_dual}
    \begin{aligned}
         & \argmin_{\fv{\alpha}}
         &                       & \Theta(\fv{\alpha}) = \frac{1}{2} \fv{\alpha}^t \widetilde{Q} \fv{\alpha} - p \fv{\alpha}                                                             \\
         & \text{s.t.}
         &                       & 0 \leq \alpha_i^r \leq C, \;  i=1,\ldots,m_r; r=1,\ldots,T ,                                                                                          \\
         &                       &                                                                                           & \sum_{i=1}^{n_r}{\alpha_i^r y_i^r} = 0, \; r=1,\ldots,T .
    \end{aligned}
\end{equation}

\subsection{Kernel Extension}
%After the definition of the linear \acrshort{gl} \acrshort{mtl} \acrshort{svm}, we present the extension to the kernel case.

%\subsubsection*{A Priori Kernel Extension of \acrshort{mtl} Models}
The kernel extension presented in~\cite{EvgeniouMP05} proposes using a mapping in the original finite space to incorporate the task information and then apply the kernel trick over the new mapped features. However, these results do not allow to perform the, possibly infinite dimensional, mapping corresponding to a kernel and then incorporate the task information in the new space.

Here we propose another approach using tensor product of \acrshort{rkhss} and \acrshort{mtl} kernels as given in Definition~\ref{def:mtl_kernel}. 
Consider the \acrshort{rkhs} $\hilbertspace$ and the functional
\begin{equation}
    \label{eq:mtl_kernel_altext_original}
    \begin{aligned}
         & R({u_1, \ldots, u_T}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{u_r}{\phi(x_i^r)}) + \mu \sum_r \sum_s (E)_{rs} \dotp{u_r}{u_s} , \\
    \end{aligned}
\end{equation}
where $\phi$ is a feature transformation, $u_1, \ldots, u_T \in \hilbertspace$, and $E$ is a $\ntasks \times \ntasks$ symmetric, positive definite matrix.
%
Recall that for the linear case of the \acrshort{gl} \acrshort{mtl} formulation, shown above, we have used the vector
\begin{equation}
    \nonumber
    \fv{w} = 
    \begin{bmatrix}
        w_1 \\
        \vdots \\
        w_\ntasks 
    \end{bmatrix}
    = \sum_{t=1}^T e_r \otimes w_r ,
\end{equation}
where $w_r \in \reals^\dimx$ and $\set{e_1, \ldots, e_\ntasks}$ is an orthonormal basis of $\reals^\ntasks$.
To use a similar approach in the infinite-dimensional scenario, we define
\begin{equation}
    \nonumber
    \myvec{u} = \sum_{t=1}^T e_r \otimes u_r ,
\end{equation}
where we are using the tensor product as defined in the previous section, such that $\myvec{u} \in \reals^T \otimes \hilbertspace$. Then, we can reformulate~\eqref{eq:mtl_kernel_altext_original} with a tensor product formulation as the problem
\begin{equation}
    \label{eq:mtl_kernel_altext_tensor}
    \begin{aligned}
            & R(\myvec{u}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{u}}{e_r \otimes \phi(x_i^r)}) + \mu \left(  \myvec{u}^\intercal (E \otimes I) \myvec{u} \right). \\
    \end{aligned}
\end{equation}
The following lemma illustrates how to minimize this functional as a single task problem.
\begin{lemma}\label{lemma:regproblems_kernel}
    The
    %predictions $\dotp{u_r^*}{\phi(x)}$ of the 
    solutions $u_1^*, \ldots, u_\ntasks^*$, from the \acrshort{mtl} problem~\eqref{eq:mtl_kernel_altext_original}, and equivalently the solution $\fv{u}$ from the problem~\eqref{eq:mtl_kernel_altext_tensor},    
    can be obtained solving the problem
    \begin{equation}
        \label{eq:mtl_kernel_tensor}
        \begin{aligned}
             & S(\myvec{w}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{w}}{(B_r \otimes \phi(x_i^r))}) + \mu  \myvec{w}^\intercal \myvec{w} , \\
        \end{aligned}
    \end{equation}
    where $\bm{w} \in \reals^p \otimes \hilbertspace$ with $p \geq \ntasks$ and $B_r$ are the columns of a full rank matrix $B \in \reals^{p \times \ntasks}$ such that $\mymat{E}^{-1} = \mymat{B}^\intercal \mymat{B}$.
\end{lemma}

\begin{proof}
    Replicating the idea of~\cite{EvgeniouMP05}, since ${E} \in \reals^{\ntasks \times \ntasks}$ is symmetric and positive definite, we can find $B \in \reals^{p \times \ntasks}, p \geq \ntasks$ and $\rank{B} = \ntasks$ such that $E^{-1} =
            {B^\intercal} {B}$, using for example the SVD.
    % In the case that ${E} \in \reals^{\ntasks \times \ntasks}$ is a semipositive definite matrix with rank $r$ we can find $B \in \reals^{p \times r}, p \geq \ntasks$ and $\rank{B} = r$ such that $E^{+} = 
    % {B^\intercal} {B}$.
    % For the rest of the analysis we will consider a positive definite matrix $\mymat{E}$ but the results are also valid for positive semidefinite matrices.
    % 
    Then, with the properties of the tensor product of linear maps,  $$ E^{-1} \otimes I = (B^\intercal B) \otimes I = (B^\intercal \otimes I) (B \otimes I) . $$
    %
    Consider the change of variable $\myvec{u} = (B^\intercal \otimes I)\myvec{w}$, where $\myvec{w} \in \reals^p \otimes \hilbertspace$. % Observe that this can always be done because %the columns of $B$ generates $\reals^T$, so we can always find $\hat{w}$ such that $e_r = B w$   .The condition of full rank for $B$ is necessary in this step. $B$ is full rank. That is, $B$ has linearly independent columns, and, thus, we can write its right inverse as $B^+$

    Rewriting \eqref{eq:mtl_kernel_altext_tensor} using $\myvec{w}$, we obtain
    \begin{equation}
        \nonumber
        \begin{aligned}
            R(\myvec{w}) & = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{(B^\intercal \otimes I) \myvec{w}}{ (e_r \otimes \phi(x_i^r))}) + \mu  \myvec{w}^\intercal (B^\intercal \otimes I)^\intercal (E \otimes I) (B^\intercal \otimes I)\myvec{w} \\
                         & = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{w}}{(B \otimes I) (e_r \otimes \phi(x_i^r))}) + \mu  \myvec{w}^\intercal \myvec{w} ,                                                                                 \\
        \end{aligned}
    \end{equation}
    which is equivalent to problem~\eqref{eq:mtl_kernel_altext_tensor}.
    % \begin{equation}
    %     \nonumber
    %     \begin{aligned}
    %          & S(\myvec{w}) = \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{w}}{(B_r \otimes \phi(x_i^r))}) + \mu  \myvec{w}^\intercal \myvec{w} . \\
    %     \end{aligned}
    % \end{equation}

    We are thus considering a regularized functional $S(\myvec{w})$ where we seek the minimum over functions $\fv{w}$ in the Hilbert space $\reals^p \otimes \hilbertspace$. Note that in this space the inner product is:
    \begin{equation}
        \nonumber
        \begin{aligned}
            \dotp{}{}: & (\reals^p \otimes \hilbertspace) & \times &  & (\reals^p \otimes \hilbertspace) & \to &                 & \reals      & \\
                       & (z_1, \phi(x_1))                 &        &  & (z_2, \phi(x_2))                 & \to & \dotp{z_1}{z_2} & k(x_1, x_2) &
        \end{aligned},
    \end{equation}
    where $k(\cdot, \cdot)$ is the reproducing kernel of the space of functions $\phi(\cdot)$.
    However, in the minimization problem we only have values $z = B e_r = B_r$ for some $r=1, \ldots, T$; then $\dotp{B_r}{B_s} = \left(E^{-1}\right)_{rs}.$
    Since the regularizer is clearly increasing in $\norm{w}^2$, we can apply the Representer theorem, which states that the minimizer of $S(\myvec{w})$ has the form
    \begin{equation}
        \nonumber
        %\label{eq:representer_tensor}
        \optim{\myvec{w}} = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r (B_r \otimes \phi(x_i^r)) .
    \end{equation}
    Using the correspondence between $\optim{\myvec{u}}$ and $\optim{\myvec{w}}$, we have
    \begin{equation}
        \nonumber
        \optim{\myvec{u}} = (B^\intercal \otimes I) \optim{\myvec{w}} =  \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r (\vect{\dotp{B_1}{B_r}, \ldots, \dotp{B_\ntasks}{B_r}} \otimes \phi(x_i^r)),
    \end{equation}
    where we define $\vect{a_1, \ldots, a_\ntasks}$ as
    $$ \vect{a_1, \ldots, a_\ntasks} = 
        \begin{bmatrix}
            a_1 \\
            \vdots \\
            a_\ntasks \\
        \end{bmatrix}.
    $$
    Then, we can recover the predictions corresponding to the solutions $u_r^*$ as
    \begin{equation}
        \nonumber
        \begin{aligned}
            \dotp{u_s}{\phi(\hat{x}^s)} & = \dotp{\myvec{u}}{e_s \otimes \phi(\hat{x}^s)}                                                                                                                      \\
                                        & = \dotp{\sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r (\vect{\dotp{B_1}{B_r}, \ldots, \dotp{B_\ntasks}{B_r}} \otimes \phi(x_i^r))}{e_s \otimes \phi(\hat{x}^s)} \\
                                        & = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r  \dotp{B_s}{B_r} \dotp{\phi(x_i^r)}{\phi(\hat{x}_s)}                                                                  \\
                                        & = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r  \left(E^{-1}\right)_{rs} k(x_i^r, \hat{x}^s).
        \end{aligned}
    \end{equation}
\end{proof}
Observe that, applying the corresponding feature map $\mymat{B} \otimes I$, the predictions can be obtained equivalently using the common $\myvec{w}$ as
\begin{equation}
    \nonumber
    \begin{aligned}
        \dotp{\myvec{w}}{(B \otimes I) (e_s \otimes \phi(\hat{x}^s))}
         & = \dotp{\myvec{w}}{ (B_s \otimes \phi(\hat{x}^s))}                                                               \\
         & = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r \dotp{B_r \otimes \phi(x_i^r)}{B_s \otimes \phi(\hat{x}^s)} \\
         & = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r \dotp{B_r}{B_s} \dotp{\phi(x_i^r)}{\phi(\hat{x}^s)}         \\
         & = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r \left(E^{-1}\right)_{rs} k(x_i^r, \hat{x}^s).
    \end{aligned}
\end{equation}
That is, we have expressed the \acrshort{mtl} problem as a \acrshort{stl} problem with the \acrshort{mtl} kernel being
\begin{equation}
    \nonumber
    \hat{k}(x_i^r, x_j^s) = \left(E^{-1}\right)_{rs} k(x_i^r, x_j^s),
\end{equation}
as defined in Definition~\ref{def:mtl_kernel}.
%
Note that the kernels obtained in this way, unlike those of Lemma~\ref{eq:evgeniou_lemma2}, split the inter-task relations and the similarity between data points. That is, we can implicitly send our data into another, possibly infinite-dimensional, space and apply the task information after this transformation.
These are separable kernels~\citep{AlvarezRL12}, but, to the best of our knowledge, this is the first time they are constructed using tensor products.



Now we can use Lemma~\ref{lemma:regproblems_kernel} to solve the kernelized \acrshort{gl} \acrshort{mtl} problem 
\begin{equation}\label{eq:kernel_gl_primal}
    \begin{aligned}
         & \argmin_{\fv{w}, \fv{b}, \fv{\xi}}
         &                                    & {C \sum_{r=1}^T \sum_{i=1}^m{\xi_{i}^r} + \frac{\nu}{4} \sum_{r=1}^T \sum_{s=1}^T (A)_{rs} \norm{w_r - w_s}^2 + \frac{1}{2} \sum_r \norm{w_r}^2 }                                                               \\
         & \text{s.t.}
         &                                    & y_{i}^r ( \dotp{w_r}{\phi(x_{i}^r)} + b_r) \geq p_{i}^r - \xi_{i}^r , \;  i=1,\ldots,m_r; \;  r=1,\ldots,T,                                                                                                           \\
         &                                    &                                                                                                                                                 & \xi_{i}^r \geq 0, \;  i=1,\ldots,m_r; \;  r=1,\ldots,T \; .
    \end{aligned}
\end{equation}
When we use an implicit kernel transformation, the result~\eqref{eq:laplacian_block_linear} is not direct. However, we can take the following approach. First, we define $\fv{v}$ as
$$ \myvec{v} = \sum_{t=1}^T e_r \otimes v_r ,$$
where $\set{e_1, \ldots, e_\ntasks}$ is the canonical basis of $\reals^\ntasks$.
Then, we can observe that
\begin{align*}
    \myvec{v}^\intercal (I_T \otimes I_d) \myvec{v} & = \sum_{r=1}^T \norm{v_r}^2 ,                                       \\
    \myvec{v}^\intercal (L \otimes I_d) \myvec{v}   & = \frac{1}{2} \sum_{r=1}^T \sum_{s=1}^T (A)_{rs} \norm{v_r - v_s}^2 .
\end{align*}
To check the second equation, see
\begin{align*}
    \myvec{v}^\intercal (\mymat{L} \otimes \mymat{I}_d) \myvec{v} & = \myvec{v}^\intercal (\mymat{D} \otimes \mymat{I}_d) \myvec{v} - \myvec{v}^\intercal (\mymat{A} \otimes \mymat{I}_d) \myvec{v}   \\
                                                                  & = \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks D_{rs} v_r^\intercal v_s - \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} v_r^\intercal v_s \\
                                                                  & = \sum_{r=1}^\ntasks D_{rr} v_r^\intercal v_r - \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} v_r^\intercal v_s                    \\
                                                                  & = \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} v_r^\intercal v_r - \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} v_r^\intercal v_s \\
                                                                  & = \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} (v_r^\intercal v_r - v_r^\intercal v_s) ,
\end{align*}
which can be expressed as
\begin{align*}
     & \sum_{r=1}^\ntasks \sum_{s=r}^\ntasks \lbrace (A)_{rs}  (v_r^\intercal v_r - v_r^\intercal v_s) + (A)_{sr} (v_s^\intercal v_s - v_s^\intercal v_r) \rbrace \\
     & =
    \sum_{r=1}^\ntasks \sum_{s=r}^\ntasks \lbrace ((A)_{rs} + (A)_{sr})  (v_r^\intercal v_r + v_s^\intercal v_s - 2v_r^\intercal v_s) \rbrace                     \\
     & =
    \sum_{r=1}^\ntasks \sum_{s=r}^\ntasks \lbrace ((A)_{rs} + (A)_{sr})  \norm{v_r - v_s}^2 \rbrace                                                               \\
    % &=
    % \frac{1}{2} \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks \lbrace ((A)_{rs} + (A)_{sr})  \norm{v_r - v_s}^2 \rbrace \\
     & =
    \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks \lbrace (A)_{rs}  \norm{v_r - v_s}^2 \rbrace .                                                                      
\end{align*}
Therefore, we can express the kernelized problem~\eqref{eq:kernel_gl_primal} as
\begin{equation}
    \begin{aligned}
         & R(\myvec{v}) = C \sum_{r=1}^{\ntasks} \sum_{i=1}^\npertask \lossf(y_i^r, \dotp{\myvec{v}}{e_r \otimes \phi(x_i^r)}) + \left(  \myvec{v}^\intercal ((\nu L + I_\ntasks ) \otimes I) \myvec{v} \right), \\
    \end{aligned}
\end{equation}
where $\lossf$ is the hinge loss.
Note now that this is the optimization problem~\eqref{eq:mtl_kernel_altext_tensor} with the matrix $E =  \left(\nu \fm{L} + \fm{I}_\ntasks\right)$; then, as shown in Lemma~\ref{lemma:regproblems_kernel}, this is equivalent to solving a dual problem where the kernel function is
\begin{equation}
    \label{eq:kernelfun_gl_kernel}
    \widetilde{k}(x_i^r, x_j^s) = \left( \left(\nu \fm{L} + \fm{I}_\ntasks\right)^{-1} \right)_{rs} k(x_i^r, x_j^s) ,
\end{equation}
where $k(\cdot, \cdot)$ is the reproducing kernel induced by the implicit transformation $\phi(\cdot)$.

Thus, the dual problem corresponding to problem~\eqref{eq:kernel_gl_primal} is the problem presented in~\eqref{eq:linear_gl_dual}, but where the kernel matrix $\widetilde{Q}$ is now defined using the kernel function~\eqref{eq:kernelfun_gl_kernel}.
% If $A$ is symmetric, that is $(A)_{rs} = (A)_{sr}$, then
% \begin{align*}
%     \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} (v_r^\intercal v_r - v_r^\intercal v_s) &=
%     \sum_{r=1}^\ntasks \sum_{s=r}^\ntasks \lbrace (A)_{rs}  (v_r^\intercal v_r - v_r^\intercal v_s) + (A)_{sr} (v_s^\intercal v_s - v_s^\intercal v_r) \rbrace \\
%     &=
%     \sum_{r=1}^\ntasks \sum_{s=r}^\ntasks \lbrace ((A)_{rs} + (A)_{sr})  (v_r^\intercal v_r + v_s^\intercal v_s - 2v_r^\intercal v_s) \rbrace \\
%     &=
%     \sum_{r=1}^\ntasks \sum_{s=r}^\ntasks \lbrace ((A)_{rs} + (A)_{sr})  \norm{w_r - w_s}^2 \rbrace \\
%     &=
%     \frac{1}{2} \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks \lbrace ((A)_{rs} + (A)_{sr})  \norm{w_r - w_s}^2 \rbrace \\
%     &=
%      \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks \lbrace (A)_{rs}  \norm{w_r - w_s}^2 \rbrace .\\
% \end{align*}



\section{Convex Graph Laplacian Multi-Task Learning}
\label{sec:convexgl}


In~\cite{RuizAD20} we proposed a convex formulation for the Graph Laplacian \acrshort{mtl} \acrshort{svm}, which includes a convex combination of a common part and task-specific parts that are coupled through a Laplacian regularization.
That is, the models for each task are
\begin{equation}
    \nonumber
    h_r({\cdot}) = \lambda_r \left\lbrace \dotp{w}{\phi(\cdot)} + b  \right\rbrace + (1 - \lambda_r) \left\lbrace \dotp{{v}_r}{\psi(\cdot)} + d_r \right\rbrace ,
\end{equation}
where $\phi(\cdot)$ and $\psi(\cdot)$ are the implicit transformations for the common and task-specific parts, and can be possibly distinct to try to capture different properties of the data.
That is, $\phi: \Xspace \to \hilbertspace_\phi$ and $\psi: \Xspace \to \hilbertspace_\psi$, where $\hilbertspace_\phi$ and $\hilbertspace_\psi$ are RKHS's with reproducing kernels $k_\phi(\cdot, \cdot)$ and $k_\psi(\cdot, \cdot)$, respectively.
Unlike the model definition for Convex \acrshort{mtl} in~\eqref{eq:convexmtl_modeldef}, where each task-specific part can use a different Hilbert space, here all tasks must use the same two transformations: $\phi$ and $\psi$. The common $\phi(\cdot)$ must be obviously equal for all tasks, but also the specific one $\psi(\cdot)$ must be the same for all tasks because to impose a Laplacian regularization, all the parameters $v_r$ have to be elements from the same \acrshort{rkhs}.
Again, as with the convex \acrshort{mtl} approach, the hyperparameters $\lambda_r \in [0, 1]$ define how relevant is the common part for each task. When $\lambda_r=1$, only the common part is present, while $\lambda_r=0$ results in task-specific models that, now, are coupled through the Laplacian regularization.
%
Therefore, the $\lambda_r$ values are hyperparameters that must be selected for each specific problem.

\subsection{General Result for Kernel Methods}
%\paragraph*{A general result for Convex Graph Laplacian \acrshort{mtl}.\\}
In general, we can apply Lemma~\ref{lemma:regproblems_kernel} to find the solution of kernel methods problems that use an \acrshort{gl} regularization.
Here, for the convex formulation, we follow an analogous approach as the one used above; with ${e_1, \ldots, e_\ntasks}$ being the canonical basis of $\reals^\ntasks$, we define
\begin{equation}
    \label{eq:vtensor_def}
    \fv{v} = \sum_{t=1}^T e_r \otimes v_r \in \reals^\ntasks \otimes \hilbertspace_\psi,
\end{equation}
such that
$\dotp{\fv{v}}{e_r \otimes \psi(x_i^r)} = \dotp{v_r}{\psi(x_i^r)}$.
Consider then the product space of the \acrshort{rkhs} $\hilbertspace_\phi$ and the tensor product of spaces $(\reals^\ntasks \otimes \hilbertspace_\psi)$: $\hilbertspace_\phi \times (\reals^\ntasks \otimes \hilbertspace_\psi)$. In this space, the norm of $(w, \fv{v}) \in \hilbertspace_\phi \times (\reals^\ntasks \otimes \hilbertspace_\psi)$ is
$$\dotp{(w, \fv{v})}{(w, \fv{v})} =  \dotp{w}{w} + \dotp{\fv{v}}{\fv{v}} = \norm{w}^2 + \norm{\fv{v}^2}; $$
then the general kernelized problem for convex \acrshort{gl} \acrshort{mtl} can be expressed as
\begin{equation}\label{eq:cvxgl_general_problem_tensor}
    \begin{aligned}
        \argmin_{(w, \fv{v}) \in \hilbertspace_\phi \times (\reals^\ntasks \otimes \hilbertspace_\psi)} R((w, \fv{v})) & = \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \ell(y_i^r, (\dotp{(w, \fv{v})}{(\lambda_r \phi(x_i^r), (1 - \lambda_r) (e_r \otimes \psi({x}_i^r)))})) \\
                                                                                                                       & \quad + \dotp{(w, \fv{v})}{(I_{\rkhs_\phi} \times (E \otimes I_{\rkhs_\psi})) (w, \fv{v})}  ,
    \end{aligned}
\end{equation}
where $\ell$ is a loss function, $I_{\rkhs_\phi}$ in our case is the identity operator in $\rkhs_\phi$, and $E$ is a symmetric, positive definite operator in $\reals^\ntasks$, which for our \acrshort{gl} formulation is
$$E = (\nu \fm{L} + I),$$
with $L$ being a \acrshort{gl} matrix.
% With both $A$ and $E$ are positive definite, the function $ \dotp{(w, \fv{v})}{(A \times E) (w, \fv{v})}$ is strictly increasing in $\norm{(w, \fv{v})}$; then, we can apply the Representer Theorem to problem~\eqref{eq:cvxgl_general_problem_tensor}, that is, the solutions of this problem can be expressed as
% \begin{equation}
%     \nonumber
%     (w^*, \fv{v}^*) = \sum_{r=1}^\ntasks \sum_{i=1}^\npertask \alpha_i^r y_i^r (\lambda_r \phi(x_i^r), (1 - \lambda_r) (e_r \otimes \psi({x}_i^r)))
% \end{equation}
Using a modification of Lemma~\ref{lemma:regproblems_kernel}, we can state the following result.
\begin{lemma}\label{lemma:regproblems_kernel_convex}
    The
    %predictions $\dotp{(w^*, \fv{v}^*)}{(\phi(x_i^r), e_r \otimes \psi(x_i^r))}$ of the 
    solution $(w^*, \fv{v}^*)$ from the Multi-Task optimization problem~\eqref{eq:cvxgl_general_problem_tensor} can be obtained solving the minimization problem
    % \begin{equation}
    %     \label{eq:cvxgl_general_problem_tensor_alt}
    %     \begin{aligned}
    %         &S((w, \fv{u})) = \sum_{r=1}^{\ntasks} \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{(w, \fv{u})}{(\lambda_r \phi(x_i^r), (1 - \lambda_r) (B_r \otimes \psi(x_i^r)))}) + \mu \left( \dotp{w}{w} + \dotp{\fv{u}}{\fv{u}}\right),\\
    %     \end{aligned}
    % \end{equation}
    \begin{equation}\label{eq:cvxgl_general_problem_tensor_alt}
        \begin{aligned}
            \argmin_{(w, \fv{u}) \in \hilbertspace_\phi \times (\reals^\ntasks \otimes \hilbertspace_\psi)} S((w, \fv{u})) & = \sum_{r=1}^{\ntasks} \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{(w, \fv{u})}{(\lambda_r \phi(x_i^r), (1 - \lambda_r) (B_r \otimes \psi(x_i^r)))}) \\
                                                                                                                           & \quad + \mu \left( \dotp{w}{w} + \dotp{\fv{u}}{\fv{u}}\right)  ,
        \end{aligned}
    \end{equation}
    where $w \in \rkhs_\phi$ and $\fv{u} \in \reals^p \otimes \hilbertspace_\psi$ with $p \geq \ntasks$, and $B_r$ are the columns of a full rank matrix $B \in \reals^{p \times \ntasks}$ such that $\mymat{E}^{-1} = \mymat{B}^\intercal \mymat{B}$.
\end{lemma}
\begin{proof}
    Since ${E} \in \reals^{\ntasks \times \ntasks}$ is positive definite, we can find $B \in \reals^{p \times \ntasks}, p \geq \ntasks$ and $\rank{B} = \ntasks$ such that $E^{-1} =
            {B^\intercal} {B}$, using for example the SVD;
    % In the case that ${E} \in \reals^{\ntasks \times \ntasks}$ is a semipositive definite matrix with rank $r$ we can find $B \in \reals^{p \times r}, p \geq \ntasks$ and $\rank{B} = r$ such that $E^{+} = 
    % {B^\intercal} {B}$.
    % For the rest of the analysis we will consider a positive definite matrix $\mymat{E}$ but the results are also valid for positive semidefinite matrices.
    % 
    then, we can express the inverse of the operator $E \otimes I_{\rkhs_\psi}$ from~\eqref{eq:cvxgl_general_problem_tensor} as
    $$ \left(E \otimes I_{\rkhs_\psi}\right)^{-1} = E^{-1} \otimes I_{\rkhs_\psi} = (B^\intercal B) \otimes I_{\rkhs_\psi} = (B^\intercal \otimes I_{\rkhs_\psi}) (B \otimes I_{\rkhs_\psi}).$$
    %
    Consider the change of variable $\fv{v} = (B^\intercal \otimes I_{\rkhs_\psi}) \fv{u}$, where $\fv{u} \in \reals^p \otimes \hilbertspace$. Then we can write
    %, which can always be done because
    %the columns of $B$ generates $\reals^T$, so we can always find $\hat{w}$ such that $e_r = B w$   .The condition of full rank for $B$ is necessary in this step.
    % $B$ is full rank; then
    \begin{equation}
        \nonumber
        \begin{aligned}
            R(w, \fv{u}) & = \sum_{r=1}^{\ntasks} \sum_{i=1}^{\npertask_r} \lossf(y_i^r, \dotp{(w, (B^\intercal \otimes I_{\rkhs_\psi}) \fv{u})}{ (\lambda_r \phi(x_i^r), (1 - \lambda_r) (e_r \otimes \psi(x_i^r)))}) \\
                         & \quad + \mu \dotp{(w, (B^\intercal \otimes I_{\rkhs_\psi}) \fv{u})}{(I_{\rkhs_\phi} \times (E \otimes I_{\rkhs_\psi})) (w, (B^\intercal \otimes I_{\rkhs_\psi}) \fv{u}) }                    .
        \end{aligned}
    \end{equation}
    This is equivalent to problem~\eqref{eq:cvxgl_general_problem_tensor_alt}, where the regularizer is increasing in $\norm{(w, \fv{u})}^2$, so we can apply the representer theorem~\citep{ScholkopfHS01}, which states that the minimizer $\opt{\omega}$ of any empirical regularized risk
    \begin{equation}
        \nonumber
        \sum_{i=1}^\nsamples \ell(\dotp{\omega}{\phi(x_i)}, y_i) + \Omega(\norm{f}),
    \end{equation}
    where $\Omega$ is a strictly increasing function,
    admits a representation of the form $\opt{\omega} = \sum_{i=1}^\nsamples \alpha_i \phi(x_i)$.
    Thus, the minimizer of $S(w, \fv{u})$ in~\eqref{eq:cvxgl_general_problem_tensor_alt} has the form
    \begin{equation}
        \label{eq:repr_th_convexgl_u}
        %\label{eq:representer_tensor}
        (w^*, \fv{u}^*) = \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r (\lambda_r \phi(x_i^r), (1-\lambda_r) (B_r \otimes \psi(x_i^r))) .
    \end{equation}
    Applying the correspondence between $\fv{u}^*$ and $\fv{v}^*$, namely, $\opt{\fv{v}} = (B^\intercal \otimes I_{\rkhs_\psi}) \opt{\fv{u}}$, we get
    \begin{equation}
        \label{eq:repr_th_convexgl_v}
        \begin{aligned}
            (w^*, \fv{v}^*)
             & = (w^*, (B^\intercal \otimes I_{\rkhs_\psi}) \fv{u}^*)                                                                                                                       \\
             & =  \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r (\lambda_r \phi(x_i^r), (1 - \lambda_r)\vect{\dotp{B_1}{B_r}, \ldots, \dotp{B_\ntasks}{B_r}} \otimes \psi(x_i^r)) \\
             & =  \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r (\lambda_r \phi(x_i^r), (1 - \lambda_r)\vect{(E^{-1})_{1r}, \ldots, (E^{-1})_{\ntasks r}} \otimes \psi(x_i^r)),
        \end{aligned}
    \end{equation}
    where $\vect{a_1, \ldots, a_l}$ is the vector whose elements are the scalars $a_1, \ldots, a_l$.
    Now, we can recover the predictions corresponding to the solutions $(w^*, \fv{v}^*)$ as
    \begin{equation}
        \nonumber
        \begin{aligned}
             & \dotp{(w^*, \fv{v}^*)}{\left(\lambda_t \phi(\hat{x}^t), (1 - \lambda_t) e_t \otimes \psi(\hat{x}^t) \right)}                                                                                           \\
             & = \biggl\langle \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r (\lambda_r \phi(x_i^r), (1 - \lambda_r) \vect{(E^{-1})_{1r}, \ldots, (E^{-1})_{\ntasks r}} \otimes \phi(x_i^r) ), \\
             &\qquad \left(\lambda_t \phi(\hat{x}^t), (1 - \lambda_t) e_t \otimes \psi(\hat{x}^t) \right) \biggr\rangle    \\
             %& = \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r  (\lambda_r \lambda_t \dotp{\phi(x_i^r)}{\phi(x_i^r)} + (1-\lambda_r) (1 - \lambda_t) \dotp{B_s}{B_r} \dotp{\psi(x_i^r)}{\psi(\hat{x}^t)}) \\
             & = \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r  (\lambda_r \lambda_t \dotp{\phi(x_i^r)}{\phi(x_i^r)} + (1-\lambda_r) (1 - \lambda_t) (E^{-1})_{rs} \dotp{\psi(x_i^r)}{\psi(\hat{x}^t)}) .
        \end{aligned}
    \end{equation}
\end{proof}
As with Lemma~\ref{lemma:regproblems_kernel}, this is a result that gives an easy way of finding the solutions because it shows that the \acrshort{mtl} problem~\eqref{eq:cvxgl_general_problem_tensor} can be expressed as a \acrshort{ctl} problem with the \acrshort{mtl} kernel function
\begin{equation}
    \label{eq:dual_cvxgl_kernel_function}
    \bar{{k}}(x_i^r, x_j^s) = \lambda_r \lambda_s k_\phi(x_i^r, x_j^s) + (1 - \lambda_r) (1 - \lambda_s) \left((\nu \fm{L} + \fm{I}_\ntasks)^{-1}\right)_{rs} k_\psi(x_i^r, x_j^s) ,
\end{equation}
where $k_\phi(\cdot, \cdot)$ and $k_\psi(\cdot, \cdot)$ are the reproducing kernels corresponding to the transformations $\phi$ and $\psi$, respectively. Thus, we can use standard \acrshort{ctl} kernel methods with a modified kernel to solve \acrshort{gl} \acrshort{mtl} problems.
%

We derive next the Convex \acrshort{gl} \acrshort{mtl} formulations for the L1, L2 and LS-\acrshort{svms} by applying this result.


\subsection{Convex Graph Laplacian L1-SVM}
% Anyway, this result is general for any kernel method, but it is also interesting to see how this procedure can be applied in the specific cases of L1, L2 and LS-\acrshort{svms}.
%
The primal problem for the linear L1-\acrshort{svm} with the convex \acrshort{gl} approach, that we have presented in~\citep*{RuizAD20}, is the following one
%
\begin{equation}\label{eq:primal_cvxgl_l1_linear}
    \begin{aligned}
         & \argmin_{\substack{v_1, \ldots, v_\ntasks ;                                                                                                                                                                                                                                                                                          \\ \substack{b_1, \ldots, b_\ntasks } ; \\ \substack{\fv{\xi}}, w; }}
         &                                             & { C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} {\xi_i^r}  + \frac{\nu}{2} \sum_{r=1}^\ntasks \sum_{s=1}^T (A)_{rs} {\| {v}_r - {v}_s \|}^2 + \frac{1}{2} \sum_r \norm{{v}_r}^2 + \frac{1}{2} \norm{{w}}^2}                                                                              \\
         & \text{s.t.}
         &                                             & y_i^r (\lambda_r ({w} \cdot {x}_i^r) + (1 - \lambda_r) ({v}_r \cdot {x}_i^r) + b_r) \geq p_i^r - \xi_i^r  ,                                                                                                                                                                            \\
         &                                             &                                                                                                                                                                                                           & \xi_i^r \geq 0,  \;  i = 1, \dotsc, \npertask_r, \; r=1, \dotsc, \ntasks .
    \end{aligned}
\end{equation}
%
Note that with $\nu=0$, this problem is equivalent to our proposed convex \acrshort{mtl} formulation shown in~\eqref{eq:svmmtl_primal_convex}. Recall here that we are using the unifying formulation, such that, for every task $r=1, \ldots, \ntasks$, when $p_i^r = 1$ for $i=1, \ldots, \npertask_r$, it is equivalent to the \acrshort{svm} for classification; but if we double the number of instances and select $y_i^r = 1$ for $i=1, \ldots, \npertask_r$, $y_i= -1$ for $i = \npertask_r+1, \ldots, 2 \npertask_r$, and $p_i^r$ as the target values, we get the \acrshort{svm} problem for regression.
%
The extension to the kernel case requires using a different formulation of~\eqref{eq:primal_cvxgl_l1_linear}, that is
\begin{equation}\label{eq:primal_cvxgl_l1_kernel}
    \begin{aligned}
         & \argmin_{\substack{\fv{v} ;                                                                                                                                                                                                                                                                                                               \\ \substack{b_1, \ldots, b_\ntasks } ; \\ \substack{\fv{\xi}}, w; }}
         &                             & { C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} {\xi_i^r}  + \frac{\nu}{2} \fv{v}^\intercal (\fm{L} \otimes I_\rkhs) \fv{v} + \frac{1}{2} \fv{v}^\intercal (\fm{I}_\ntasks \otimes I_\rkhs) \fv{v} + \frac{1}{2} \norm{{w}}^2}                                                                              \\
         & \text{s.t.}
         &                             & y_i^r (\lambda_r \dotp{w}{\phi(x_i^r)} + (1 - \lambda_r) (\dotp{\fv{v}}{e_r \otimes \psi({x}_i^r)}) + b_r) \geq p_i^r - \xi_i^r  ,                                                                                                                                                                          \\
         &                             &                                                                                                                                                                                                                                & \xi_i^r \geq 0,  \;  i = 1, \dotsc, \npertask_r, \; r=1, \dotsc, \ntasks .
    \end{aligned}
\end{equation}
%
Although the result from Lemma~\ref{lemma:regproblems_kernel_convex} can be applied for this problem, for illustration purposes we will develop the entire procedure for this L1-SVM case.
%
The Lagrangian corresponding to~\eqref{eq:primal_cvxgl_l1_kernel} is
\begin{equation}\label{eq:lagr_cvxgl_l1_kernel}
    \begin{aligned}
        \mathcal{L} & ({w}, \fv{v}, \fv{b}, \fv{\xi}, {\fv{\alpha}}, \fv{\beta})                                                                                                                                                                  \\
                    & = C \sum_{r=1}^T \sum_{i=1}^{\npertask_r}{\xi_{i}^r} + \frac{\nu}{2} \fv{v}^\intercal (\fm{L} \otimes I_\rkhs) \fv{v} + \frac{1}{2} \fv{v}^\intercal (\fm{I}_\ntasks \otimes I_\rkhs) \fv{v} + \frac{1}{2} \norm{{w}}^2
        \\ &\quad  - \sum_{r=1}^T \sum_{i=1}^{\npertask_r}{ \alpha_i^r [y_{i}^r (\lambda_r \dotp{w}{\phi(x_i^r)} + (1 - \lambda_r) (\dotp{\fv{v}}{e_r \otimes \psi({x}_i^r)}) + b_r) - p_{i}^r + \xi_{i}^r]   } \\
                    & \quad - \sum_{r=1}^T \sum_{i=1}^{\npertask_r}{ \beta_i^r \xi_i^r },
    \end{aligned}
\end{equation}
where $\alpha_i^r, \beta_i^r \geq 0$.
Taking derivatives and making them $0$, we get
% \begin{align*}
%     & \frac{\partial \mathcal{L}}{\partial {w}} = 0 \implies {w} = \sum_{r=1}^\ntasks \lambda_r \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r \phi(x_i^r)}  \; , \\
%     & \frac{\partial \mathcal{L}}{\partial {v}_r} = 0 \implies \left((\fm{I}_\ntasks + \nu \fm{L}) \otimes I_\rkhs \right) \fv{v} = \sum_{r=1}^\ntasks (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r (e_r \otimes \psi({x}_i^r))}  \; , \\
%     & \frac{\partial \mathcal{L}}{\partial b_r} = 0 \implies  \sum_{i=1}^{m_r}{\alpha_i^r y_i^r } = 0  \; ,\\
%     & \frac{\partial \mathcal{L}}{\partial \xi_i^r} = 0 \implies C - \alpha_i^r - \beta_i^r = 0 \; .
% \end{align*}
\begin{align}
    \grad_{{w}} \lagr = 0     & \implies \optim{{w}} = \sum_{r= 1}^\ntasks \lambda_r \sum_{i=1}^{m_r} {\alpha_i^r} \left\lbrace y_i^r \phi(x_i^r) \right\rbrace , \label{eq:common_repr_cvxgl_l1}                                                           \\
    \grad_{\fv{v}} \lagr = 0  & \implies  \left((\fm{I}_\ntasks + \nu \fm{L}) \otimes I_\rkhs \right) \fv{v} = \sum_{r=1}^\ntasks (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r (e_r \otimes \psi({x}_i^r))}, \label{eq:specific_repr_cvxgl_l1} \\
    \grad_{{b}_r} \lagr = 0   & \implies \sum_{i=1}^{m_r} {\alpha_i^r} y_i^r = 0 , \label{eq:specific_eqconstr_cvxgl_l1}                                                                                                                                    \\
    \grad_{\xi_i^r} \lagr = 0 & \implies C - \alpha_i^r - \beta_i^r = 0 . \label{eq:xi_feas_cvxgl_l1}
\end{align}
Here we have
\begin{equation}
    \label{eq:expression_E}
    E =  (\fm{I}_\ntasks + \nu \fm{L}),\; E_\otimes =  \left(E \otimes I_\rkhs \right).
\end{equation}
Substituting these results in the Lagrangian, we get
\begin{equation}\nonumber
    \begin{aligned}
        \mathcal{L} & ({w}, \fv{v}, \fv{b}, \fv{\xi}, {\fv{\alpha}}, \fv{\beta})      \\
        %& = \frac{1}{2} \dotp{\fv{v}}{E_\otimes \fv{v}} + \frac{1}{2} \dotp{w}{w}    \\ 
        %&\quad  - \sum_{r=1}^T \sum_{i=1}^{\npertask_r}{ \alpha_i^r [y_{i}^r (\lambda_r \dotp{w}{\phi(x_i^r)} + (1 - \lambda_r) (\dotp{\fv{v}}{e_r \otimes \psi({x}_i^r)}) + b_r) - p_{i}^r]   } \\
        % &= \frac{1}{2} \dotp{(E_\otimes)^{-1} \sum_{r=1}^\ntasks (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r (e_r \otimes \psi({x}_i^r))}}{E_\otimes (E_\otimes)^{-1} \sum_{r=1}^\ntasks (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r (e_r \otimes \psi({x}_i^r))}} \\ 
        % &\quad  +\frac{1}{2} \dotp{\sum_{r=1}^\ntasks \lambda_r \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r \phi(x_i^r)}}{\sum_{r=1}^\ntasks \lambda_r \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r \phi(x_i^r)}} \\
        % &\quad - \sum_{r=1}^T (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{ \alpha_i^r y_{i}^r \left\lbrace  \dotp{(E_\otimes)^{-1} \sum_{s=1}^\ntasks (1 - \lambda_s) \sum_{j=1}^{\npertask_s}{\alpha_j^s y_j^s (e_s \otimes \psi({x}_j^s))}}{e_r \otimes \psi(x_i^r)} \right\rbrace   } \\
        % &\quad - \sum_{r=1}^T \lambda_r \sum_{i=1}^{\npertask_r}{ \alpha_i^r y_{i}^r \left\lbrace  \dotp{\sum_{s=1}^\ntasks \lambda_s \sum_{j=1}^{\npertask_s}{\alpha_j^s y_j^s \phi(x_j^s)}}{\phi(x_i^r)} \right\rbrace   } - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r p_i^r \\
                    % & = \frac{1}{2} \sum_{r, s=1}^\ntasks (1-\lambda_r)(1-\lambda_s) \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \dotp{(e_s \otimes \psi(x_j^s))}{(E_\otimes)^{-1} (e_r \otimes \psi(x_i^r))}       \\
                    % & \quad +\frac{1}{2} \sum_{r,s=1}^\ntasks \lambda_r \lambda_s \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \dotp{\phi(x_j^s)}{\phi(x_i^r)}                                                       \\
                    & = - \frac{1}{2} \sum_{r, s=1}^\ntasks (1-\lambda_r)(1-\lambda_s) \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \dotp{(e_s \otimes \psi(x_j^s))}{(E_\otimes)^{-1} (e_r \otimes \psi(x_i^r))}             \\
                    & \quad - \frac{1}{2} \sum_{r,s=1}^\ntasks \lambda_r \lambda_s \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \dotp{\phi(x_j^s)}{\phi(x_i^r)} - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r p_i^r . \\
        % &= -\frac{1}{2} \sum_{r, s=1}^\ntasks (1-\lambda_r)(1-\lambda_s) \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \left((\nu \fm{L} + \fm{I}_\ntasks)^{-1}\right)_{rs} \dotp{\psi(x_j^s)}{\psi(x_i^r)} \\ 
        % &\quad -\frac{1}{2} \sum_{r,s=1}^\ntasks \lambda_r \lambda_s \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \dotp{\phi(x_j^s)}{\phi(x_i^r)} - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r p_i^r .
    \end{aligned}
\end{equation}
Due to~\eqref{eq:xi_feas_cvxgl_l1} and $\alpha_i^r, \beta_i^r \geq 0$, we have the box constraints $0 \leq \alpha_i^r \leq C$; then, the dual problem is
\begin{equation}\label{eq:dual_cvxgl_l1_kernel}
    \begin{aligned}
         & \argmin_{\fv{\alpha}}
         &                       & \Theta(\fv{\alpha}) = \frac{1}{2} \fv{\alpha}^t \left( \Lambda \fm{Q} \Lambda + \left(\fm{I}_{\nsamples} - \Lambda \right) \fm{\widetilde{Q}} \left(\fm{I}_{\nsamples} - \Lambda \right) \right) \fv{\alpha} - \fv{p} \fv{\alpha}                                                             \\
         & \text{s.t.}
         &                       & 0 \leq \alpha_i^r \leq C, \;  i=1,\ldots,m_r; r=1,\ldots,T ,                                                                                                                                                                                                                                  \\
         &                       &                                                                                                                                                                                                                                   & \sum_{i=1}^{n_r}{\alpha_i^r y_i^r} = 0, \; r=1,\ldots,T .
    \end{aligned}
\end{equation}
Here, we use the matrix
\begin{equation}\label{eq:lambdamatrix_def_chapgl}
    \Lambda = \Diag(\overbrace{\lambda_1, \ldots, \lambda_1}^{\npertask_1}, \ldots, \overbrace{\lambda_\ntasks, \ldots, \lambda_\ntasks}^{\npertask_\ntasks}) ,
\end{equation}
$\fm{I}_{\nsamples}$ is the $\nsamples \times \nsamples$ identity matrix, with $\nsamples = \sum_{r=1}^\ntasks \npertask_r,$
%
$Q$ is the common, standard, kernel matrix, and $\widetilde{Q}$ is the kernel matrix with the \acrshort{gl} information. We can define now the kernel matrix
\begin{equation}
    \label{eq:dual_cvxgl_kernel_matrix}
    \bar{{\fm{Q}}} = \Lambda \fm{Q} \Lambda + \left(\fm{I}_{\nsamples} - \Lambda \right) \widetilde{\fm{Q}} \left(\fm{I}_{\nsamples} - \Lambda \right),
\end{equation}
as the matrix that is generated using the kernel function~\eqref{eq:dual_cvxgl_kernel_function}. That is, in each block $\bar{Q}[{rs}]$, corresponding to the $r$-th and $s$-th tasks, we have that 
\begin{equation}
    \nonumber
    \bar{Q}[{rs}]_{ij} = y_i^r y_j^s \bar{k}(x_i^r, x_j^s),
\end{equation}
so the entire matrix $\bar{Q}$ is defined as
\begin{equation}
    \nonumber
    \bar{Q} = 
    \begin{bmatrix}
        \bar{Q}[{11}] & \bar{Q}[{12}] & \ldots & \bar{Q}[{1 \ntasks}] \\
        \vdots & \vdots & \ddots & \vdots \\
        \bar{Q}[{\ntasks 1}] & \bar{Q}[{\ntasks 2}] & \ldots & \bar{Q}[{\ntasks \ntasks}] \\
    \end{bmatrix} .
\end{equation}

\subsection{Convex Graph Laplacian L2-SVM}
%\paragraph*{Convex Graph Laplacian L2-SVM.\\}
The primal problem for convex \acrshort{gl} \acrshort{mtl} based on the linear L2-SVM, with the unifying formulation for regression and classification, is
\begin{equation}\label{eq:primal_cvxgl_l2_linear}
    \begin{aligned}
         & \argmin_{\substack{v_1, \ldots, v_\ntasks ;                                                                                                                                                                                                                                                                                          \\ \substack{b_1, \ldots, b_\ntasks } ; \\ \substack{\fv{\xi}}, w; }}
         &                                             & { C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} {(\xi_i^r)^2}  + \frac{\nu}{2} \sum_{r=1}^\ntasks \sum_{s=1}^T (A)_{rs} {\| {v}_r - {v}_s \|}^2 + \frac{1}{2} \sum_r \norm{{v}_r}^2 + \frac{1}{2} \norm{{w}}^2}                                                                              \\
         & \text{s.t.}
         &                                             & y_i^r (\lambda_r ({w} \cdot {x}_i^r) + (1 - \lambda_r) ({v}_r \cdot {x}_i^r) + b_r) \geq p_i^r - \xi_i^r  ;
    \end{aligned}
\end{equation}
and with the tensor product formulation we can express the kernelized version of problem~\eqref{eq:primal_cvxgl_l2_linear} as
\begin{equation}\label{eq:primal_cvxgl_l2_kernel}
    \begin{aligned}
         & \argmin_{\substack{\fv{v} ;                                                                                                                                                                                                                                      \\ \substack{b_1, \ldots, b_\ntasks } ; \\ \substack{\fv{\xi}}, w; }}
         &                             & { C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} {(\xi_i^r)^2}  + \frac{\nu}{2} \fv{v}^\intercal (\fm{L} \otimes I_\rkhs) \fv{v} + \frac{1}{2} \fv{v}^\intercal (\fm{I}_\ntasks \otimes I_\rkhs) \fv{v} + \frac{1}{2} \norm{{w}}^2} \\
         & \text{s.t.}
         &                             & y_i^r (\lambda_r \dotp{w}{\phi(x_i^r)} + (1 - \lambda_r) (\dotp{\fv{v}}{e_r \otimes \psi({x}_i^r)}) + b_r) \geq p_i^r - \xi_i^r  .                                                                                                 \\
    \end{aligned}
\end{equation}
The corresponding Lagrangian it then the following one:
\begin{equation}\label{eq:lagr_cvxgl_l2_kernel}
    \begin{aligned}
        \mathcal{L} & ({w}, \fv{v}, \fv{b}, \fv{\xi}, {\fv{\alpha}})                                                                                                                                                                                  \\
                    & = C \sum_{r=1}^T \sum_{i=1}^{\npertask_r}{(\xi_{i}^r)^2} + \frac{\nu}{2} \fv{v}^\intercal (\fm{L} \otimes I_\rkhs) \fv{v} + \frac{1}{2} \fv{v}^\intercal (\fm{I}_\ntasks \otimes I_\rkhs) \fv{v} + \frac{1}{2} \norm{{w}}^2
        \\ &\quad  - \sum_{r=1}^T \sum_{i=1}^{\npertask_r}{ \alpha_i^r [y_{i}^r (\lambda_r \dotp{w}{\phi(x_i^r)} + (1 - \lambda_r) (\dotp{\fv{v}}{e_r \otimes \psi({x}_i^r)}) + b_r) - p_{i}^r + \xi_{i}^r]   } ,
    \end{aligned}
\end{equation}
where $\alpha_i^r \geq 0$ are the Lagrange multipliers.
Computing the gradients with respect to the primal variables and making them $0$, we obtain
\begin{align}
    \grad_{{w}} \lagr = 0     & \implies {w} = \sum_{r= 1}^\ntasks \lambda_r \sum_{i=1}^{m_r} {\alpha_i^r} \left\lbrace y_i^r \phi(x_i^r) \right\rbrace , \label{eq:common_repr_cvxgl_l2}                                                           \\
    \grad_{\fv{v}} \lagr = 0  & \implies  \left((\fm{I}_\ntasks + \nu \fm{L}) \otimes I_\rkhs \right) \fv{v} = \sum_{r=1}^\ntasks (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r (e_r \otimes \psi({x}_i^r))}, \label{eq:specific_repr_cvxgl_l2} \\
    \grad_{{b}_r} \lagr = 0   & \implies \sum_{i=1}^{m_r} {\alpha_i^r} y_i^r = 0 , \label{eq:specific_eqconstr_cvxgl_l2}                                                                                                                                    \\
    \grad_{\xi_i^r} \lagr = 0 & \implies C \xi_i^r - {\alpha_i^r} = 0 . \label{eq:xi_feas_cvxgl_l2}
\end{align}
With $E_\otimes$ as defined in~\eqref{eq:expression_E}, applying~\eqref{eq:common_repr_cvxgl_l2},~\eqref{eq:specific_repr_cvxgl_l2} to substitute $w$ and $\fv{v}$ and~\eqref{eq:xi_feas_cvxgl_l2} to replace $\xi_i^r$ in the Lagrangian, we get
\begin{equation}\nonumber
    \begin{aligned}
        \mathcal{L} & ({w}, \fv{v}, \fv{b}, \fv{\xi}, {\fv{\alpha}}, \fv{\beta})                                                                                                                                                                    \\
                    & = \frac{1}{2C} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} (\alpha_i^r)^2 - \frac{1}{C} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} (\alpha_i^r)^2 + \frac{1}{2} \dotp{\fv{v}}{E_\otimes\fv{v}} + \frac{1}{2} \dotp{w}{w}
        \\ &\quad  - \sum_{r=1}^T \sum_{i=1}^{\npertask_r}{ \alpha_i^r [y_{i}^r (\lambda_r \dotp{w}{\phi(x_i^r)} + (1 - \lambda_r) (\dotp{\fv{v}}{e_r \otimes \psi({x}_i^r)}) + b_r) - p_{i}^r]   } ,\\
        % &= \frac{1}{2C} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} (\alpha_i^r)^2 - \frac{1}{C} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} (\alpha_i^r)^2 \\
        % &\quad + \frac{1}{2} \dotp{(E_\otimes)^{-1} \sum_{r=1}^\ntasks (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r (e_r \otimes \psi({x}_i^r))}}{E_\otimes (E_\otimes)^{-1} \sum_{r=1}^\ntasks (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r (e_r \otimes \psi({x}_i^r))}} \\ 
        % &\quad  +\frac{1}{2} \dotp{\sum_{r=1}^\ntasks \lambda_r \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r \phi(x_i^r)}}{\sum_{r=1}^\ntasks \lambda_r \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r \phi(x_i^r)}} \\
        % &\quad - \sum_{r=1}^T (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{ \alpha_i^r y_{i}^r \left\lbrace  \dotp{(E_\otimes)^{-1} \sum_{s=1}^\ntasks (1 - \lambda_s) \sum_{j=1}^{\npertask_s}{\alpha_j^s y_j^s (e_s \otimes \psi({x}_j^s))}}{e_r \otimes \psi(x_i^r)} \right\rbrace   } \\
        % &\quad - \sum_{r=1}^T \lambda_r \sum_{i=1}^{\npertask_r}{ \alpha_i^r y_{i}^r \left\lbrace  \dotp{\sum_{s=1}^\ntasks \lambda_s \sum_{j=1}^{\npertask_s}{\alpha_j^s y_j^s \phi(x_j^s)}}{\phi(x_i^r)} \right\rbrace   } - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r p_i^r \\
    \end{aligned}
\end{equation}
which is equal to
\begin{equation}
    \nonumber
    \begin{aligned}
         & \frac{1}{2C} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} (\alpha_i^r)^2 - \frac{1}{C} \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} (\alpha_i^r)^2                                                                             \\
        %  & \quad+ \frac{1}{2} \sum_{r, s=1}^\ntasks (1-\lambda_r)(1-\lambda_s) \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \dotp{(e_s \otimes \psi(x_j^s))}{(E_\otimes)^{-1} (e_r \otimes \psi(x_i^r))}  \\
        %  & \quad +\frac{1}{2} \sum_{r,s=1}^\ntasks \lambda_r \lambda_s \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \dotp{\phi(x_j^s)}{\phi(x_i^r)}                                                       \\
         & \quad - \frac{1}{2} \sum_{r, s=1}^\ntasks (1-\lambda_r)(1-\lambda_s) \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \dotp{(e_s \otimes \psi(x_j^s))}{(E_\otimes)^{-1} (e_r \otimes \psi(x_i^r))}             \\
         & \quad - \frac{1}{2} \sum_{r,s=1}^\ntasks \lambda_r \lambda_s \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \dotp{\phi(x_j^s)}{\phi(x_i^r)} - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r p_i^r . \\
        % &= -\frac{1}{2} \sum_{r, s=1}^\ntasks (1-\lambda_r)(1-\lambda_s) \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \left((\nu \fm{L} + \fm{I}_\ntasks)^{-1}\right)_{rs} \dotp{\psi(x_j^s)}{\psi(x_i^r)} \\ 
        % &\quad -\frac{1}{2} \sum_{r,s=1}^\ntasks \lambda_r \lambda_s \sum_{i, j=1}^{\npertask_r, \npertask_s} \alpha_j^s \alpha_i^r y_j^s y_i^r \dotp{\phi(x_j^s)}{\phi(x_i^r)} - \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \alpha_i^r p_i^r .
    \end{aligned}
\end{equation}
Thus, alongside~\eqref{eq:specific_eqconstr_cvxgl_l2}, the dual problem for the L2-SVM based formulation is
\begin{equation}\label{eq:dual_cvxgl_l2_kernel}
    \begin{aligned}
         & \argmin_{\fv{\alpha}}
         &                       & \Theta(\fv{\alpha}) = \frac{1}{2} \fv{\alpha}^t \left\lbrace  \left( \Lambda \fm{Q} \Lambda + \left(\fm{I}_{\nsamples} - \Lambda \right) \fm{\widetilde{Q}} \left(\fm{I}_{\nsamples} - \Lambda \right) \right) + \frac{1}{C} \fm{I}_\nsamples \right\rbrace \fv{\alpha} - \fv{p} \fv{\alpha}                                                            \\
         & \text{s.t.}
         &                       & 0 \leq \alpha_i^r, \;  i=1,\ldots,m_r; r=1,\ldots,T ,                                                                                                                                                                                                                                                                                                   \\
         &                       &                                                                                                                                                                                                                                                                                              & \sum_{i=1}^{n_r}{\alpha_i^r y_i^r} = 0, \; r=1,\ldots,T.
    \end{aligned}
\end{equation}
Here, again we use the matrix $\Lambda$ defined in~\eqref{eq:lambdamatrix_def_chapgl}.
Now, we have the same differences that we find in the standard L1 and L2-\acrshort{svm}; there is no upper bound for $\alpha_i^r$, but an additional diagonal term, the identity matrix $\frac{1}{C} I_\nsamples$, appears, which can be interpreted as a soft constraint for the dual coefficients $\alpha_i^r$.
%
Again, we end up with the kernel matrix $\bar{Q}$ as defined in~\eqref{eq:dual_cvxgl_kernel_matrix}, which combines the common matrix $Q$ and the matrix $\widetilde{Q}$ with the \acrshort{gl} information.


\subsection{Convex Graph Laplacian LS-SVM}
%\paragraph*{Convex Graph Laplacian LS-SVM.\\}
Recall that in the LS-SVM~\citep{SuykensV99} the inequalities in the constraints are substituted for equalities, which lead to a simpler dual solution.
The primal problem for the convex \acrshort{gl} approach, based on the linear LS-SVM is the following one:
\begin{equation}\label{eq:primal_cvxgl_ls_linear}
    \begin{aligned}
         & \argmin_{\substack{v_1, \ldots, v_\ntasks ;                                                                                                                                                                                                                                                                                          \\ \substack{b_1, \ldots, b_\ntasks } ; \\ \substack{\fv{\xi}}, w; }}
         &                                             & { C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} {(\xi_i^r)^2}  + \frac{\nu}{2} \sum_{r=1}^\ntasks \sum_{s=1}^T (A)_{rs} {\| {v}_r - {v}_s \|}^2 + \frac{1}{2} \sum_r \norm{{v}_r}^2 + \frac{1}{2} \norm{{w}}^2}                                                                              \\
         & \text{s.t.}
         &                                             & y_i^r (\lambda_r ({w} \cdot {x}_i^r) + (1 - \lambda_r) ({v}_r \cdot {x}_i^r) + b_r) \geq p_i^r - \xi_i^r  .
    \end{aligned}
\end{equation}
Here, we are using the unifying formulation for the LS-SVM. Problem~\eqref{eq:primal_cvxgl_ls_linear} is equivalent to the classification problem when $p_i^r=1$ and $y_i^r \in \set{-1, 1}$ are the class labels ; and it is equivalent to the regression problem when $y_i^r=1$ and $p_i^r$ are the target values, for all $i=1, \ldots, \npertask_r$ and $r=1, \ldots, \ntasks$.
%
We can extend this problem to the non-linear case using the tensor product formulation, which is then expressed as:
\begin{equation}\label{eq:primal_cvxgl_ls_kernel}
    \begin{aligned}
         & \argmin_{\substack{\fv{v} ;                                                                                                                                                                                                                                      \\ \substack{b_1, \ldots, b_\ntasks } ; \\ \substack{\fv{\xi}}, w; }}
         &                             & { C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} {(\xi_i^r)^2}  + \frac{\nu}{2} \fv{v}^\intercal (\fm{L} \otimes I_\rkhs) \fv{v} + \frac{1}{2} \fv{v}^\intercal (\fm{I}_\ntasks \otimes I_\rkhs) \fv{v} + \frac{1}{2} \norm{{w}}^2} \\
         & \text{s.t.}
         &                             & y_i^r (\lambda_r \dotp{w}{\phi(x_i^r)} + (1 - \lambda_r) (\dotp{\fv{v}}{e_r \otimes \psi({x}_i^r)}) + b_r) = p_i^r - \xi_i^r  .                                                                                                    \\
    \end{aligned}
\end{equation}
The Lagrangian corresponding to this optimization problem is
\begin{equation}\label{eq:lagr_cvxgl_ls_kernel}
    \begin{aligned}
        \mathcal{L} & ({w}, \fv{v}, \fv{b}, \fv{\xi}, {\fv{\alpha}})                                                                                                                                                                                  \\
                    & = C \sum_{r=1}^T \sum_{i=1}^{\npertask_r}{(\xi_{i}^r)^2} + \frac{\nu}{2} \fv{v}^\intercal (\fm{L} \otimes I_\rkhs) \fv{v} + \frac{1}{2} \fv{v}^\intercal (\fm{I}_\ntasks \otimes I_\rkhs) \fv{v} + \frac{1}{2} \norm{{w}}^2
        \\ &\quad  - \sum_{r=1}^T \sum_{i=1}^{\npertask_r}{ \alpha_i^r [y_{i}^r (\lambda_r \dotp{w}{\phi(x_i^r)} + (1 - \lambda_r) (\dotp{\fv{v}}{e_r \otimes \psi({x}_i^r)}) + b_r) - p_{i}^r + \xi_{i}^r]   } ,
    \end{aligned}
\end{equation}
where now, unlike in the L1 and L2-SVM cases, there are no restrictions for the Lagrange multipliers $\alpha_i^r$.
The KKT conditions for this problem are then
\begin{align}
    \grad_{{w}} \lagr = 0        & \implies \optim{{w}} = \sum_{r= 1}^\ntasks \lambda_r \sum_{i=1}^{m_r} {\alpha_i^r} \left\lbrace y_i^r \phi(x_i^r) \right\rbrace , \label{eq:common_repr_cvxgl_ls}                                                           \\
    \grad_{\fv{v}} \lagr = 0     & \implies  \left((\fm{I}_\ntasks + \nu \fm{L}) \otimes I_\rkhs \right) \fv{v} = \sum_{r=1}^\ntasks (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r (e_r \otimes \psi({x}_i^r))}, \label{eq:specific_repr_cvxgl_ls} \\
    \grad_{{b}_r} \lagr = 0      & \implies \sum_{i=1}^{m_r} {\alpha_i^r} y_i^r = 0 , \label{eq:specific_eqconstr_cvxgl_ls}                                                                                                                                    \\
    \grad_{\xi_i^r} \lagr = 0    & \implies C \xi_i^r - {\alpha_i^r} = 0 , \label{eq:xi_feas_cvxgl_ls}                                                                                                                                                         \\
    \grad_{\alpha_i^r} \lagr = 0 & \implies y_{i}^r (\lambda_r \dotp{w}{\phi(x_i^r)} + (1 - \lambda_r) (\dotp{\fv{v}}{e_r \otimes \psi({x}_i^r)}) + b_r) + \xi_{i}^r = p_i^r . \label{eq:alpha_feas_cvxgl_ls}
\end{align}
Applying~\eqref{eq:common_repr_cvxgl_ls},~\eqref{eq:specific_repr_cvxgl_ls} and~\eqref{eq:xi_feas_cvxgl_ls} to replace $w, \fv{v}$ and ${\xi_i^r}$ in~\eqref{eq:alpha_feas_cvxgl_ls}, with $E_\otimes$ as defined in~\eqref{eq:expression_E}, we get
\begin{equation}
    \nonumber
    \begin{aligned}
         & y_{i}^r (\lambda_r \dotp{w}{\phi(x_i^r)} + (1 - \lambda_r) (\dotp{\fv{v}}{e_r \otimes \psi({x}_i^r)}) + b_r) + \xi_{i}^r = p_i^r                                                                                                           \\
         & \implies  \lambda_r \dotp{\sum_{s= 1}^\ntasks \lambda_s \sum_{j=1}^{m_s} {\alpha_i^s} \left\lbrace y_i^s \phi(x_j^s) \right\rbrace}{y_{i}^r \phi(x_i^r)}                                                                                   \\
         & \quad + (1 - \lambda_r) \dotp{\left(\fm{E_\otimes}\right)^{-1} \sum_{s=1}^\ntasks (1 - \lambda_s) \sum_{j=1}^{\npertask_s}{\alpha_i^s y_i^s (e_s \otimes \psi({x}_i^s))}}{y_{i}^r (e_r \otimes \psi({x}_i^r))}   + b_r + \frac{\alpha_i^r}{C} = p_i^r . \\
    \end{aligned}
\end{equation}
For each coefficient $\alpha_i^r$ we get an equality like this one, which, all together and alongside~\eqref{eq:specific_eqconstr_cvxgl_ls} form a system of equations that can be expressed as
\begin{equation}\label{eq:dual_cvxgl_ls_kernel}
    \begin{aligned}
        \left[
            \begin{array}{c|c}
                \fm{0}_{\ntasks \times \ntasks} & \fm{A}^\intercal \fm{y}                                                                                                                                                         \\
                \hline
                \fm{y} \fm{A}                   & \left( \Lambda \fm{Q} \Lambda + \left(\fm{I}_{\nsamples} - \Lambda \right) \fm{\widetilde{Q}} \left(\fm{I}_{\nsamples} - \Lambda \right) \right) + \frac{1}{C} \fm{I}_\nsamples
            \end{array}
            \right]
        \begin{bmatrix}
            b_1       \\
            \vdots    \\
            b_\ntasks \\
            \fv{\alpha}
        \end{bmatrix}
        =
        \begin{bmatrix}
            \fv{0}_\ntasks \\
            \fv{p}
        \end{bmatrix}.
    \end{aligned}
\end{equation}
Again, $\Lambda$ is the matrix defined in~\eqref{eq:lambdamatrix_def_chapgl},
and we have kernel matrix $\bar{Q}$, defined as
$$ \bar{Q} = \Lambda \fm{Q} \Lambda + \left(\fm{I}_{\nsamples} - \Lambda \right) \fm{\widetilde{Q}} \left(\fm{I}_{\nsamples} - \Lambda \right), $$
appears again in the dual problem. This matrix is computed with the kernel function~\eqref{eq:dual_cvxgl_kernel_function} and combines the common information, in matrix $Q$, and that from tasks-specific parts coupled through the \acrshort{gl} regularization, in matrix $\widetilde{Q}$.

%\subsection{Experiments}

\section{Adaptive Graph Laplacian Algorithm}
\label{sec:adapconvexgl}

Until now, in this chapter we have seen \acrshort{gl} formulations for kernel methods, where we assume that there exists a graph whose nodes represent the tasks, and the weights of the edges determine the pairwise relations between tasks.
%
If $A$ is the graph adjacency matrix containing these weights, we use the Laplacian regularizer
presented in~\eqref{eq:gl_regularization} to enforce a coupling between tasks according to the graph information.
To do this, an adjacency matrix $A$ must be chosen a priori, and it defines the optimization problem.
However, these weights of $A$ are not known in real-world problems. Even if we have an expert knowledge of the problem at hand, manually selecting the weight between each pair of tasks seems unfeasible, even for a few tasks.
It is more sensible to use a data-driven approach and automatically learn the matrix $A$.
In this section, we propose one method to learn $A$ from data, which we presented in~\citep{RuizAD21_hais}, we discuss the procedure and its computational cost, and also show how the distances can be computed in kernel spaces.

\subsection{Motivation and Interpretation}
%\paragraph*{Entropy-based Interpretation.\\}
To explain our data-driven procedure for selecting the adjacency weights, first we would like the matrix $A$ to meet the following requirements:
\begin{itemize}
    \item $A$ is symmetric.
    \item All the weights $(A)_{rs}$ are positive, for $r, s=1, \ldots, \ntasks$.
    \item The rows of $A$ add up to 1.
\end{itemize}
The first one is a necessary condition to express the regularizer of~\eqref{eq:gl_regularization} using the Laplacian matrix $L$.
The second requirement is a natural one, and the third is meant to offer a better interpretability of the matrix $A$, since we can view each row as a probability distribution.
%
Then, we can interpret the entropy of the rows as a measure of how sparse or concentrated are its connection with other tasks. That is, let $\frow{a}^r$ be the row for task $r$; its entropy can be computed as
\begin{equation}
    \nonumber
    H(\frow{a}^r) = -\sum_{s=1}^\ntasks a^r_s \log(a^r_s).
\end{equation}
Observe that this is a non-negative quantity since $a_{r}^s \in [0, 1]$ due the conditions stated before, and reaches its maximum when the distribution is uniform.
If the weight $a^r_r = (A)_{rr}$ is $1$ and the rest, $a_r^s,\; s\neq r$, are $0$, then the $r$-th task is not connected to any other task and the entropy is minimal. In the other extreme case, when $\frow{a}^r = \frac{1}{\ntasks} \fv{1}_\ntasks^\intercal$, the task is equally connected to all the other tasks and the entropy is maximal.
%

With these considerations, there are two trivial options when choosing the adjacency matrix: using a diagonal matrix $A$, i.e. $A = \fm{I}_\ntasks$, where there are no connections between different tasks and the entropy of the rows is minimal, and using an agnostic view, with the constant matrix $A = \frac{1}{\ntasks} \fv{1}_\ntasks \fv{1}_\ntasks^\intercal$ where the degree of relation is the same among all tasks and the entropy of the rows is maximal.

For a better understanding of these situations we look at the following simplified formulation for the convex \acrshort{gl} \acrshort{mtl} problem,
\begin{equation}
    \label{eq:simplified_cvxgl_primal}
    \begin{aligned}
         & \argmin_{w, \fv{v}, \fv{b}}
         &                             & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} {\lossf(\lambda_r \dotp{w}{\phi({x}_i^r)} + (1 - \lambda_r) \dotp{{v}_r}{\psi({x}_i^r)} + b_r, y_i^r)}                                                                                                                                          \\
         &                             &                                                                                                                                                      & \quad + \nu \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} \norm{{v}_r - v_{s}}^2 +  \sum_{r=1}^\ntasks \norm{{v}_r}^2 + \norm{{w}}^2  . \\
    \end{aligned}
\end{equation}
%
When we use the minimal entropy matrix $A = \fm{I}_\ntasks$, it is equivalent to the problem
\begin{equation}\nonumber
    \begin{aligned}
         & \argmin_{w, \fv{v}, \fv{b}}
         &                             & { C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} {\lossf(\lambda_r \dotp{w}{\phi({x}_i^r)} + (1 - \lambda_r) \dotp{{v}_r}{\psi({x}_i^r)} + b_r, y_i^r)}  +  \sum_{r=1}^\ntasks \norm{{v}_r}^2 + \norm{{w}}^2    } , \\
    \end{aligned}
\end{equation}
which is a convex \acrshort{mtl} approach, without Laplacian information. Here, the task-specific models are completely independent, with no coupling between them.
In the case that we use the constant matrix $A = \frac{1}{\ntasks} \fv{1}_\ntasks \fv{1}_\ntasks^\intercal$, if $\nu$ is large enough,~\eqref{eq:simplified_cvxgl_primal}, it tends to
\begin{equation}\nonumber
    \begin{aligned}
         & \argmin_{w, \fv{v}, \fv{b}}
         &                             & { C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} {\lossf(\lambda_r \dotp{w}{\phi({x}_i^r)} + (1 - \lambda_r) \dotp{{v}}{\psi({x}_i^r)} + b_r, y_i^r)}  + {T} \norm{{v}}^2 +  \norm{{w}}^2} ,
    \end{aligned}
\end{equation}
where a convex combination of two common models is used. In the linear case, or when $\phi = \psi$, it is almost equivalent to a \acrshort{ctl} approach, since we would use a single common model for all tasks but with task-specific biases.

Between these two extremes of minimal and maximal entropy, there exists an infinite range of matrices whose rows have intermediate entropies. Our goal is then to find an adjacency matrix $\fm{A}$ that reflects the underlying tasks relations and, therefore, helps to improve the learning process.
%\paragraph*{Optimization Procedure.\\}
To find such an adjacency matrix, we rely on the distances computed between the task parameters; if two tasks parameters are close, those tasks should be strongly related. Consider the Laplacian term
$$  \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} \norm{{v}_r - v_{s}}^2 ;$$
then, the matrix that minimizes this quantity is the diagonal matrix $\fm{A}= \fm{I}_\ntasks$ with minimal entropy rows. The interpretation of this solution is that each task is isolated; however, this is a trivial choice of matrix $A$, because it does not give any information about the underlying tasks relations. To avoid falling in such a trivial solution, we add to the objective function the negative entropies of the rows of $\fm{A}$. Thus, the optimization problem to be solved is
\begin{equation}\label{eq:adapcvxgl_general_problem}
    \begin{aligned}
         & \argmin_{\substack{w, \fv{v}, \fv{b};                                                                                                                                                                                                                                                                                                                              \\  A \in {(\reals_{\geq 0})}^\ntasks \times {(\reals_{\geq 0})}^\ntasks,  \\ \fm{A} \fv{1}_\ntasks = \fv{1}_\ntasks}}
         &                                       & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} {\lossf(\lambda_r \dotp{w}{\phi({x}_i^r)} + (1 - \lambda_r) \dotp{{v}_r}{\psi({x}_i^r)} + b_r, y_i^r)}                                                                                                                                                                       \\
         &                                       &                                                                                                                                                      & \quad + \frac{\nu}{2} \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} \norm{{v}_r - v_{s}}^2 + \frac{1}{2} \sum_{r=1}^\ntasks \norm{{v}_r}^2 + \frac{1}{2}\norm{{w}}^2 \\
         &                                       &                                                                                                                                                      & \quad- \mu \sum_{r=1}^\ntasks H(\fv{a}^r) ,
    \end{aligned}
\end{equation}
where $\fv{v}^\intercal = (v_1^\intercal, \ldots, v_\ntasks^\intercal)$ and $\fv{b} = (b_1, \ldots, b_\ntasks)$.
Also, $\reals_{\geq 0}$ are the non-negative real numbers, and $(\reals_{\geq 0})^\ntasks \times {(\reals_{\geq 0})}^\ntasks$ are the $\ntasks \times \ntasks$ real matrices with non-negative entries. The condition $\fm{A} \fv{1}_\ntasks = \fv{1}_\ntasks$ is to enforce that the rows add up to $1$.
The parameter $\mu$ regulates how much the entropy is promoted; the bigger $\mu$ is, the closer the solution for $A$ should be to the constant matrix, which has rows of maximal entropy.
%
Using the tensor product formulation of~\eqref{eq:cvxgl_general_problem_tensor}, this problem can be expressed as
\begin{equation}\label{eq:adapcvxgl_general_problem_tensor}
    \begin{aligned}
        \argmin_{\substack{(w, \fv{v}) \in \hilbertspace_\phi \times (\reals^\ntasks \otimes \hilbertspace_\psi) , \fv{b};                   \\ A \in {(\reals_{\geq 0})}^\ntasks \times {(\reals_{\geq 0})}^\ntasks,  \\ \fm{A} \fv{1}_\ntasks = \fv{1}_\ntasks}} & \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} \ell(y_i^r, \dotp{(w, \fv{v})}{(\lambda_r \phi(x_i^r), (1 - \lambda_r) (e_r \otimes \psi({x}_i^r)))} + b_r)\\
         & \quad + \dotp{(w, \fv{v})}{(I_{\rkhs_\phi} \times (E \otimes I_{\rkhs_\psi})) (w, \fv{v})}  - \mu \sum_{r=1}^\ntasks H(\fv{a}^r),
    \end{aligned}
\end{equation}
where $E = (\nu L + I_\ntasks)$, $L = \Diag(\fm{A} \fv{1}_\ntasks) - \fm{A}$, and $\fv{v}$ as defined in~\eqref{eq:vtensor_def}.

\subsection{Algorithm and Analysis}
To find the solution of the optimization problem~\eqref{eq:adapcvxgl_general_problem}, or equivalently~\eqref{eq:adapcvxgl_general_problem_tensor}, we will use a alternate descent minimization algorithm: we first fix $A$ and find optimal values for $w, \fv{v}, \fv{b}$; then we fix $w, \fv{v}, \fv{b}$ and find the optimal matrix $A$.
%
Given a convex loss function $\lossf$, the optimization problem of~\eqref{eq:adapcvxgl_general_problem} is convex in the parameters $(w, \fv{v}, \fv{b})$ and in $A$, but not jointly convex in $(w, \fv{v}, b, \fm{A})$.
Then, an iterated procedure where a alternate optimization is done, as our proposal, ensures that a local minimum is found, albeit, not a global one.
%

More concretely, in the first step, we fix the matrix $A$ and find the optimal task parameters $w, \fv{v}, \fv{b}$ that are the solutions to the problem
\begin{equation}\label{eq:cvxgl_general_problem}
    \begin{aligned}
         & \argmin_{w, \fv{v}, \fv{b}}
         &                             & C \sum_{r=1}^\ntasks \sum_{i=1}^{\npertask_r} {\lossf(\lambda_r \dotp{w}{\phi({x}_i^r)} + (1 - \lambda_r) \dotp{{v}_r}{\psi({x}_i^r)} + b_r, y_i^r)}                                                                                                                                                                          \\
         &                             &                                                                                                                                                      & \quad + \frac{\nu}{2} \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} \norm{{v}_r - v_{s}}^2 + \frac{1}{2} \sum_{r=1}^\ntasks \norm{{v}_r}^2 + \frac{1}{2}\norm{{w}}^2  .
    \end{aligned}
\end{equation}
To solve this problem, its corresponding dual problem is actually minimized, and optimal dual coefficients $\fv{\alpha}^*$ are obtained. In the case of the L1-SVM-based model, the problem~\eqref{eq:dual_cvxgl_l1_kernel} is used, while for the L2, and LS-SVM variants, we have~\eqref{eq:dual_cvxgl_l2_kernel} and~\eqref{eq:dual_cvxgl_ls_kernel}, respectively.
%

In the second step, we fix $w, \fv{v}, \fv{b}$ and find the optimal matrix $A$ that is the solution to the problem
\begin{equation}\label{eq:adapgl_optimA}
    \begin{aligned}
        \argmin_{\substack{A \in {(\reals_{\geq 0})}^\ntasks \times {(\reals_{\geq 0})}^\ntasks, \\ \fm{A} \fv{1}_\ntasks = \fv{1}_\ntasks}}
        \frac{\nu}{2} \sum_{r=1}^\ntasks \sum_{s=1}^\ntasks (A)_{rs} \norm{{v}_r - v_{s}}^2 - \mu \sum_{r=1}^\ntasks H(\fv{a}^r) .
    \end{aligned}
\end{equation}
Here we see the role of the entropy term since without it the trivial solution would be $(A)_{rs} = \delta_{rs}$, the identity matrix, which corresponds to the minimum-entropy solution $\fm{A} = \fm{I}_\ntasks$; however, using the entropy term, different, more informative solutions for $A$ can be obtained.
%
This is a separable problem for each row of $\fv{A}$ and
by taking derivatives of the objective function in~\eqref{eq:adapgl_optimA}, we have
$$ \frac{\partial}{\partial (A)_{rs}} J({A})= \frac{1}{2} \left( \nu \norm{{v_r}- {v_s}}^2 + \mu \log{(A)_{rs}} + \mu \right) , $$
and setting it to zero we get that $(A)_{rs} \propto \exp{-\frac{\nu}{\mu} \norm{{v_r}- {v_s}}^2}$; then, since $\sum_s (A)_{rs} = 1$, the solution is
\begin{equation}\label{eq:update_A}
    (A)_{rs} = \frac{\exp{-\frac{\nu}{\mu} \norm{{v}_r - {v}_s}^2 } }{\sum_t \exp{-\frac{\nu}{\mu}  \norm{{v}_r - {v}_t}^2} } .
\end{equation}


%\paragraph*{Distance Computation.\\}
In the second step of our proposed algorithm we need the distances $\norm{v_r - v_s}^2$ to define the problem~\eqref{eq:adapgl_optimA} and find the optimal adjacency matrix; however, computing such distances when $\fv{v}_r$ are elements of the \acrshort{rkhs} $\hilbertspace_\psi$ is not trivial. Next, we show how we can use the dual solution to get them.
%
Recall that the first step requires solving the dual problem corresponding to problems~\eqref{eq:primal_cvxgl_l1_kernel},~\eqref{eq:primal_cvxgl_l2_kernel} or~\eqref{eq:primal_cvxgl_ls_kernel} for L1, L2 or LS-\acrshort{svm}, respectively. Observe that all these convex \acrshort{gl} primal problems are particular cases of the problem~\eqref{eq:cvxgl_general_problem_tensor} when $E = (I_\ntasks + \nu L)$. Therefore, we can apply Lemma~\ref{lemma:regproblems_kernel_convex} and express the solution $\fv{v}$ as in~\eqref{eq:repr_th_convexgl_v}, that is:
\begin{equation}
    \nonumber
    \begin{aligned}
        \fv{v} &=  \sum_{r=1}^\ntasks (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r \left( \vect{(E^{-1})_{1r}, \ldots, (E^{-1})_{\ntasks r}} \otimes \psi(x_i^r) \right)} \\
        &= \left((\fm{I}_\ntasks + \nu \fm{L}) \otimes I_\rkhs \right)^{-1}  \sum_{r=1}^\ntasks (1 - \lambda_r) \sum_{i=1}^{\npertask_r}{\alpha_i^r y_i^r (e_r \otimes \psi({x}_i^r))},
    \end{aligned}
\end{equation}
where $\alpha_i^r$ are the dual solutions, and recall that we have different dual problems for the L1, L2 and LS-\acrshort{svm}.
Then, since the distances can be expressed as
$$ \norm{v_r - v_s}^2 = \dotp{v_r}{v_r} + \dotp{v_s}{v_s} - 2 \dotp{v_r}{v_s}, $$
we are interested in computing the dot products $\dotp{v_r}{v_s}$ for $r, s=1, \ldots, \ntasks$. These inner products, with our formulation for $\fv{v}$ as given in~\eqref{eq:vtensor_def}, are
\begin{equation}
    \nonumber
    \dotp{v_r}{v_s} = \dotp{(e_r^\intercal \otimes I_\rkhs)\fv{v}}{(e_s^\intercal \otimes I_\rkhs)\fv{v}} ,
\end{equation}
where\begin{equation}
    \nonumber
    \begin{aligned}
        \left(e_r^\intercal \otimes I_\rkhs \right)\fv{v}
         & =  \left(e_r^\intercal \otimes I_\rkhs \right) \left( \left((\fm{I}_\ntasks + \nu \fm{L}) \otimes I_\rkhs \right)^{-1}  \sum_{t=1}^\ntasks (1 - \lambda_t) \sum_{i=1}^{\npertask_t}{\alpha_i^t y_i^t (e_t \otimes \psi({x}_i^t))} \right) \\
         & = \sum_{t=1}^\ntasks (1 - \lambda_t) \sum_{i=1}^{\npertask_t} \alpha_i^t y_i^t \left( (e_r^\intercal (\fm{I}_\ntasks + \nu \fm{L})^{-1}  e_t) \otimes \psi(x_i^t) \right)                                                  \\
         & = \sum_{t=1}^\ntasks (1 - \lambda_t) \sum_{i=1}^{\npertask_t} \alpha_i^t y_i^t \left( ((\fm{I}_\ntasks + \nu \fm{L})^{-1}_{rt} ) \otimes \psi(x_i^t) \right) .
    \end{aligned}
\end{equation}
Then, using $E = I_\ntasks + \nu L$, as defined in~\eqref{eq:expression_E}, the inner product $\dotp{v_r}{v_s}$ is computed as
\begin{equation}
    \nonumber
    \begin{aligned}                                           
         & \dotp{(e_r^\intercal \otimes I_\rkhs)\fv{v}}{(e_s^\intercal \otimes I_\rkhs)\fv{v}}                                                                                                                                                                                                                              \\
         & = \dotp{\sum_{t=1}^\ntasks (1 - \lambda_t) \sum_{i=1}^{\npertask_t} \alpha_i^t y_i^t \left( \left(E^{-1}\right)_{rt}  \otimes \psi(x_i^t) \right)}{\sum_{\tau=1}^\ntasks (1 - \lambda_\tau) \sum_{i=1}^{\npertask_t} \alpha_i^\tau y_i^\tau \left( \left(E^{-1}\right)_{st}  \otimes \psi(x_i^\tau) \right)}       \\
         & = \sum_{t=1}^\ntasks \sum_{\tau=1}^\ntasks (1 - \lambda_t) (1 - \lambda_\tau) \sum_{i=1}^{\npertask_t}   \sum_{i=1}^{\npertask_t} \alpha_i^t y_i^t \alpha_i^\tau y_i^\tau \dotp{  \left( \left(E^{-1}\right)_{rt}  \otimes \psi(x_i^t) \right)}{ \left( \left(E^{-1}\right)_{st}  \otimes \psi(x_i^\tau) \right)},
    \end{aligned}
\end{equation}
which, using a matrix formulation, can be expressed as
\begin{equation}\label{eq:dot_computation_matrix}
    \dotp{v_r}{v_s} = \fv{\alpha}^\intercal \left(\fm{I}_{\nsamples} - \Lambda \right) \widetilde{{\fm{Q}^{rs}}} \left(\fm{I}_{\nsamples} - \Lambda \right) \fv{\alpha} ,
\end{equation}
where $\widetilde{{\fm{Q}^{rs}}}$ is the kernel matrix computed using the kernel function
\begin{equation}
    \label{eq:dot_computation_kernel}
    \widetilde{{k^{rs}}}(x_i^t, x_j^\tau) = (\fm{I}_\ntasks + \nu \fm{L})^{-1}_{rt} (\fm{I}_\ntasks + \nu \fm{L})^{-1}_{s\tau} k_\psi(x_i^t, x_j^\tau) .
\end{equation}
Observe here that for each pairwise distance we need 
%
Therefore, the distances are computed as
\begin{equation}\label{eq:distance_computation_matrix}
    \norm{v_r - v_s}^2 = \fv{\alpha}^\intercal \left(\fm{I}_{\nsamples} - \Lambda \right) (\widetilde{{\fm{Q}^{rr}}} + \widetilde{{\fm{Q}^{ss}}} - 2\widetilde{{\fm{Q}^{rs}}}) \left(\fm{I}_{\nsamples} - \Lambda \right) \fv{\alpha}.
\end{equation}
Observe here that for each pairwise distance $\norm{v_r - v_s}^2$ we need to compute three $\nsamples \times \nsamples$ matrices, where $\nsamples = \sum_{r=1}^\ntasks \npertask_r$, and perform the matrices product defined in~\eqref{eq:distance_computation_matrix}.

%\paragraph*{Analysis and Computational Cost.\\}

\begin{algorithm}[!t]
    \DontPrintSemicolon

    \KwInput{$(X, y) = \set{(x_i^r, y_i^r), i=1, \ldots, \npertask_r; r=1, \ldots, \ntasks}$ \tcp*{Data}}
    \KwOutput{$\fv{\alpha}^*$ \tcp*{Optimal dual coefficients}}
    \KwOutput{$\fm{A}^*$ \tcp*{Optimal adjacency matrix}}
    \KwData{params = $\set{C, \lambda, \nu, \mu, \sigma_\phi, \sigma_\psi (, \epsilon)}$ \tcp*{Hyperparameters}}
    %   $Q_\phi$ = ComputeKernelMatrix($(X, y)$, $\sigma_\phi$) \\
    %   $Q_\psi$ = ComputeKernelMatrix($(X, y)$, $\sigma_\psi$) \\
    $o^\text{old}$ = $\infty$ \\
    $A = A_0$ \tcp*{Constant matrix}
    \While{True}{
        $L_\text{inv}$ $\gets$ getInvLaplacian($\fm{A}$) \tcp*{Step 0}
        $\alpha_\text{opt}$ $\gets$ solveDualProblem($(X, y)$, $L_\text{inv}$, params) \tcp*{Step 1}
        $o$ $\gets$ computeObjectiveValue($(X, y)$, $L_\text{inv}$, $\alpha_\text{opt}$) \tcp*{Objective function value}
        \If{$o^{old} - o \leq \delta_\text{tol}$}{break \tcp*{Exit condition}}
        $o^{old} \gets o$ \\
        % \If{$J_\text{obj}^old - J_\text{obj} \geq \delta_\text{\tol}$}{
        %     break \\
        % }
        $D$ $\gets$ computeDistances($(X, y)$, $L_\text{inv}$, $\alpha_\text{opt}$) \tcp*{Step 2}
        $A$ $\gets$ updateAdjMatrix($D$, params) \tcp*{Step 3}
    }
    \Return{$\alpha_\text{opt}, A$}
    \caption{Adaptive \acrshort{gl} algorithm.}
    \label{alg:adapgl}
\end{algorithm}



%
For a better understanding, we now describe the entire procedure. 
%
We start from an agnostic point of view, with a fixed maximal entropy matrix $A^0 = \frac{1}{T} \fv{1}_\ntasks \fv{1}_\ntasks^\intercal$, where all tasks are equally related among them.
%
Note that the inverse of $(I_\ntasks + \nu L)$ is needed for the definition of the dual problem, and also for the distance computations.
%
Thus, at the beginning of each iteration, we compute the inverse of the matrix $(I_\ntasks + \nu L^0)$, where $L^\tau$ is the Laplacian matrix corresponding to the adjacency matrix $A^\tau$ in the $\tau$-th iteration.
%
After this, we find the solutions of the problem~\eqref{eq:cvxgl_general_problem}; we actually solve the corresponding dual problem and obtain the dual solution $\fv{\alpha}^*$, which has a correspondence with the optimal primal variables $w, \fv{v}, \fv{b}$.
Finally, we can compute the distances between each pair of parameters $v_r, v_s$ as shown in~\eqref{eq:distance_computation_matrix} for $r, s=1, \ldots, \ntasks$, and use these distances to obtain an adjacency matrix $A^1$ using~\eqref{eq:update_A}.
%
%
This in an iterated algorithm and all these steps are repeated with the new adjacency matrix, until convergence of the value of the objective function defined in~\eqref{eq:adapcvxgl_general_problem} or a maximum number of iterations is reached. 
%
To sum it up, this is Algorithm~\ref{alg:adapgl}, which consists on an iterated procedure where the following steps are repeated until convergence:
\begin{itemize}
    \item Step 0: invert the matrix $(I_\ntasks + \nu L)$.
    \item Step 1: minimize the dual problem to obtain $\alpha^*$.
    \item Step 2: compute the distances $\norm{v_r - v_s}^2$ between task parameters.
    \item Step 3: update the adjacency matrix $A$ using~\eqref{eq:update_A}.
\end{itemize}
% \begin{itemize}
%     \item Solve the convex GL \acrshort{mtl} problem, as shown in~\eqref{eq:cvxgl_general_problem_tensor}, with fixed matrix $A$
%     \item Compute the distances between task parameters $v_r$
%     \item Update the matrix $A$ according to these distances using~\eqref{eq:update_A}
% \end{itemize}


To study the computational cost of our algorithm we consider the cost of each step, where, for a simpler notation, we will use $\nsamples = \sum_{r=1}^\ntasks \npertask_r$.

In step 0, the inverse of the $\ntasks \times \ntasks$ Laplacian matrix, which is used in the first and second steps, has a cost of $C_0 = O(\ntasks^3)$.
%
Step 1, where the dual problem is solved, has the standard cost corresponding to these SVM variants, which all have a complexity $C_1 = O(\nsamples^{2+\epsilon})$ with $O(\nsamples^2) \leq C_1$.
%
The step 2, the distance computations, involves the inner products $\dotp{v_r}{v_s}$ shown in~\eqref{eq:dot_computation_matrix} for $r,s=1, \ldots, \ntasks$, each with a cost $\nsamples_{\text{sv}}^2$ with $\nsamples_{\text{sv}}$ being the number of support vectors; then, the complexity of the second step is $C_2 = O(\ntasks^2 \nsamples_\text{sv}^2)$.
%
Finally, in step 3, involving the update of the adjacency matrix $A$, we perform the computation of the $\ntasks \times \ntasks$ elements of $A$ using equation~\eqref{eq:update_A}, which has then a total cost of $C_3 = O(\ntasks^2)$.
%
Therefore, the total computational cost of each iteration is
$$ C_0 + C_1 + C_2 + C_3 = O(\ntasks^3 + \nsamples^{2+\epsilon} + \ntasks^2 \nsamples_\text{sv}^2 + \ntasks^2) .$$
%
Assuming a standard situation, where $\nsamples$ is much larger than $\ntasks$, steps 1 and 2 have clearly the greater cost; however, it is difficult to determine what steps are more computationally challenging, since it depends on the specific problem being solved. Anyway, step 2, unlike step 1, can be easily parallelized, computing each distance at the same time, which would result in a cost of $O(\nsamples_\text{sv}^2)$ for that step.

%\subsection{Experiments}


%\section{Experiments}

\section{Conclusions}\label{sec-conclusions-4}

In this chapter, we have focused on the \acrshort{gl} approach for \acrshort{mtl}, which penalizes the distances between task models; we consider that the tasks can be interpreted as nodes in a graph, and the corresponding adjacency matrix values are used to weight the sum of pairwise distances.
%
Since this method relies on distance computation, it is trivial to apply it in the linear case, but it is more challenging when non-linear transformations, related to reproducing kernels, are applied.

To overcome this issue, in Section~\ref{sec:mtl_kernelmethods} we present an \acrshort{mtl} framework based on tensor product of \acrshort{rkhss}. We describe some relevant concepts, and we develop the necessary elements to define the kernel of the tensor product space as the product of the kernels in each space. Then, we show how they can be applied in the context of \acrshort{mtl}: we define a kernel between tasks using a fixed tasks relation matrix and, given a kernel defined over the feature space, the \acrshort{mtl} kernel can be defined as the product of these two kernels.

Next, in Section~\ref{sec:graphlap} we describe the \acrshort{gl} \acrshort{mtl} formulation for kernel methods, and specifically for the L1-\acrshort{svm}. We first present the linear case, and then, with the tensor product of \acrshort{rkhss} definitions, we present the extension to the kernelized case.

We combine this \acrshort{gl} approach with the convex combination of Chapter~\ref{Chapter4} in Section~\ref{sec:convexgl}. Again, for the kernel case we need to use the concepts related to tensor product of \acrshort{rkhss}. We describe the convex \acrshort{gl} \acrshort{mtl} for the L1, L2 and LS-\acrshort{svm}.

Finally, in Section~\ref{sec:adapconvexgl} we propose an algorithm to automatically learn the adjacency weights between tasks from data. We motivate this method using the entropy interpretation of the rows of the adjacency matrix, we describe the algorithm and discuss its computational complexity.




