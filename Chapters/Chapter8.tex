% Chapter 8

\chapter{Conclusions And Future Work} % Write in your own chapter title
\label{Chapter8}
\lhead{Chapter \ref{Chapter8}. \emph{Conclusions And Future Work}} 
Bayesian optimization is used to optimize black-box functions, \textit{i.e.}, potentially noise expensive functions with unknown analytical expressions. In particular, examples of black-box optimization include hyper-parameter tuning of machine learning algorithms to optimize an estimation of the generalization error, industry problems such as the optimization of the configuration of a robot or even curious applications such as the optimization of the cooking recipe of a cookie. Concretely, the standard BO scenario concerns the optimization of a single black-box function. However, this thesis has introduced methods that extend the applicability of BO to broader scenarios such as constrained multi-objective scenarios (Chapter \ref{Chapter4}), parallel BO (Chapter \ref{Chapter5}) or dealing with integer and categorical-valued variable (Chapter \ref{Chapter6}). We also include a real application regarding the optimization of a hybrid grouping genetic algorithm applied to an extreme learning machine for robust prediction of ocean wave features in Chapter \ref{Chapter7}. In those chapters, we have illustrated the usefulness of the proposed methods with toy, synthetic, benchmark or real problems. Finally, in this section, we illustrate the main conclusions of this thesis.

\section{Conclusions}
We provide a list with the main conclusions of this thesis regarding the work that has been shown in the previous chapters.
\begin{itemize}
    \item First, we developed a method that performs Bayesian optimization in constrained multi-objective scenarios. This method is called Predictive Entropy Search for multi-objective optimization with constraints (PESMOC). Concretely, this method optimizes conflicting black-boxes under the presence of several constraints. In particular, solutions that do not fulfill the constraints are not considered valid. Most importantly, PESMOC iteratively suggests the recommendation that minimizes the expected reduction on the entropy of the Pareto set. In Chapter \ref{Chapter4} we showed the usefulness of PESMOC for optimizing several objectives and constraints regarding ensembles and the implementation of deep neural networks in hardware. In those experiments, PESMOC outperforms other methods tackling the constrained multi-objective scenario such as a generalization of the expected improvement acquisition function, that is expected to be greedy, or a random search, that only performs pure exploration.
    \item As we have already seen, Bayesian optimization suggests a single recommendation point in every iteration of the algorithm. Nevertheless, there are settings where we may have a cluster of nodes available to process suggestions. Unfortunately, standard Bayesian optimization leaves them idle as it can only provides a single suggestion at a time. In order to solve this issue, we generalized the previous constrained multi-objective approach to make it suggest a batch of points for every iteration. In particular, this extension of PESMOC is called Parallel Predictive entropy search for multi-objective optimization with constraints (PPESMOC). In this case, the acquisition function now suggests a batch of points that minimize the expected reduction on the entropy search of the Pareto set. As in the case of PESMOC, we tested PPESMOC on the same experiments of PESMOC but comparing it with greedy versions of PPESMOC where we iteratively create a batch of points using the PESMOC acquisition function. Critically, PPESMOC outperforms or performs similarly to these methods on the proposed experiments. Moreover, it scales the batch size better than the proposed baselines.
    \item We have studied that BO uses Gaussian processes to model the black-box. GPs assume continuous input real variables. When the problem involves other variables, such as integer-valued or categorical variables, it is common to perform a one hot encoding approximation for categorical variables or to round the integer-valued variables to the closest integer. In this work, we show how such procedures incur in a bad performance of the BO method. To circumvent this issue, we propose a transformation for the input space variables that alleviates the issues that arise in previous procedures. In particular, we include empirical evidence that shows how our transformation outperforms previous approaches dealing with integer-valued and categorical variables.
    \item Finally, we propose the Bayesian optimization of a hybrid grouping genetic algorithm for attribute selection combined with an extreme learning machine (GGA-ELM) approach for robust prediction of ocean wave features. The contribution of the thesis regarding this work was the design and implementation of the experiments. In Chapter \ref{Chapter7}, we perform two sets of experiments regarding the prediction of the wave energy flux and wave height optimization. Critically, we showed how BO outperforms the configuration suggested by experts on the field and the performance delivered by random search.
\end{itemize}

\section{Future Work}
Our proposed methods show how BO can be effectively adapted to cover a wide range of different scenarios with an excellent performance. Moreover, there are a plethora of scenarios that BO can cover in addition to the described settings that could be targeted by future work. Precisely, to conclude this document, we include some ideas as further work that can be studied by future research.
\begin{itemize}
    \item PESMOC assumes that the black-boxes to be optimized are independent. But this is not necessarily true in real-world problems. The black-boxes can have dependencies. For example, consider the optimization of the number of workers of the company and the benefit. In this example, the benefit is dependent on the hired number of employees. We hypothesize that, by modelling these dependencies, we can improve the performance of the PESMOC method \citep{shah2016pareto}. This work will need to use a multi-output GP to model these dependencies and to propose a new acquisition function, for example extending PESMOC, that takes into account the information provided by this model \citep{moreno2018heterogeneous}.
    \item We have used Bayesian optimization with Gaussian processes. Gaussian processes are flexible priors over functions, but may have difficulties modelling non-stationary functions. An extension of GPs, deep Gaussian processes, are designed to be priors over non-stationary functions \citep{damianou2013deep, bui2016deep}. Deep GPs consist of multiple GP mappings organized in several layers. The input of every layer is the output of the previous layer, that consist of several sparse GP. The nodes are sparse GPs to make the deep GP scale to more observations, typically being used from $500$ to $5000000$ observations. However, Bayesian optimization scenarios do not usually consider more than $300$ observations. Hence, if deep GPs are used for BO, they need to be modified to include GPs in their layers as the complexity in the number of observations is not a problem in the BO setting but if we use sparse GPs the performance will suffer. A future line of research is to design a deep GP that can perform well for BO scenarios. In order to do so, we will need to perform hyper-parameter sampling, that is not usually done for deep GPs on regression problems.
    \item The acquisition function of Bayesian optimization can easily be optimized in a low number of dimensions with a grid search procedure and a local optimizer method such as L-BFGS. In most cases, it is also possible to compute the gradients of the acquisition function. In particular, the number of points of the grid search is implemented to scale linearly with the number of dimensions. However, due to the curse of dimensionality, as we increment the number of dimensions or for parallel acquisition functions such as PPESMOC (whose dimensional complexity is a function of the size of the batch) and the number of dimensions of the input space this methodology will deliver worse and worse results. In order to circumvent this issue, it would be interesting to test evolutionary strategies such as NSGA-II or other metaheuristic strategies to optimize the acquisition function in a high number of dimensions to enhance the results obtained by the proposed methods \citep{deb2002fast}.
    \item An interesting application where we can apply constrained multi-objective BO is to deploy fair ML algorithms \citep{perrone2020fair}. Fairness strategies ensure ML algorithms not to incur in discriminations such as racism, machism or ageism. These strategies may be conflicting with the optimization of the estimation of the generalization error. Moreover, they can be considered as black-boxes. Additionally, if we want that these strategies can invalidate certain solutions, we can also implement them as constraints. Therefore, the methods proposed, and more precisely PESMOC, could be used to address the problem described. Further research may analyze the utility of PESMOC for finding fair machine learning models.
\end{itemize}
