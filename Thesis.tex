%% ----------------------------------------------------------------
%% Thesis.tex -- MAIN FILE (the one that you compile with LaTeX)
%% ---------------------------------------------------------------- 

% Set up the document
\documentclass[a4paper, 11pt]{Thesis}  % Use the "Thesis" style, based on the ECS Thesis style by Steve Gunn
%\documentclass[b5paper,9pt]{Thesis}  % Use the "Thesis" style, based on the ECS Thesis style by Steve Gunn
\graphicspath{{Figures/}}  % Location of the graphics files (set up for graphics to be in PDF format)

% Include any extra LaTeX packages required
\usepackage[round, authoryear, semicolon, sort&compress]{natbib}  % Use the "Natbib" style for the references in the Bibliography
\usepackage{verbatim}  % Needed for the "comment" environment to make LaTeX comments
\usepackage{vector}  % Allows "\bvec{}" and "\buvec{}" for "blackboard" style bold vectors in maths
\usepackage[Lenny]{fncychap}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{psfrag}
\usepackage{lettrine}
\usepackage{lscape}
\usepackage[ruled]{algorithm2e}
\usepackage{multirow}
\usepackage{float}
\usepackage{dsfont}
\usepackage{bm}
%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amssymb}
\usepackage{amsmath}

% My packages
\usepackage{mathtools}
\usepackage{siunitx}       % typesetting values with units
\usepackage{graphics}      % scalebox
\usepackage{longtable}      % multi-page tables
\usepackage{tikz}
\usepackage[automake,toc]{glossaries}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{textcomp}   % to use the permyriad symbol
\usepackage{bibentry} 

\makeglossaries

% mathtools
\mathtoolsset{showonlyrefs}

% algorithm
\makeatletter
\renewcommand{\@algocf@capt@plain}{above}% formerly {bottom}
\makeatother
%\SetAlFnt{\footnotesize}
%\SetKwSty{\footnotesize}
%\SetKwSty{\small}

% % subcaption
% \renewcommand\thesubfigure{(\alph{subfigure})}

%algorithm2e
%%% Coloring the comment as blue
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\SetKwInput{KwInput}{\small{Input}}                % Set the Input
\SetKwInput{KwOutput}{\small{Output}}              % set the Output
\SetKwInput{KwData}{\small{Data}}              % set the Output

%tikz
\usetikzlibrary{arrows,positioning} 
\tikzset{
    %Define standard arrow tip
    >=stealth',
    %Define style for boxes
    punkt/.style={
           rectangle,
           rounded corners,
           draw=black, very thick,
           text width=6.5em,
           minimum height=2em,
           text centered},
    % Define arrow style
    pil/.style={
           ->,
           thick,
           shorten <=2pt,
           shorten >=2pt,}
}
\usetikzlibrary{calc}
%Tikz picture
% Input layer neurons'number
\newcommand{\inputnum}{2} 
 
% Hidden layer neurons'number
\newcommand{\hiddennum}{2}  

% Hidden layer neurons'number hard sharing
\newcommand{\hiddennumhs}{4}  

% Hidden layer neurons'number hard sharing
\newcommand{\hiddennumsp}{3}  
 
% Output layer neurons'number
\newcommand{\outputnum}{2} 

% Min Node size
\newcommand{\minnodesize}{5mm} 


\hypersetup{urlcolor=blue, colorlinks=true}  % Colours hyperlinks in blue, but this can be distracting if there are many links.

% gls
\newacronym{erm}{ERM}{Empirical Risk Minimization}
\newacronym{srm}{SRM}{Structural Risk Minimization}
\newacronym{sgd}{SGD}{Stochastic Gradient Descent}
\newacronym{iid}{iid}{independent and identically distributed}

\newacronym{mtl}{MTL}{Multi-Task Learning}
\newacronym{gl}{GL}{Graph Laplacian}

\newacronym{stl}{STL}{Single-Task Learning}
\newacronym{itl}{ITL}{Independent-Task Learning}
\newacronym{ctl}{CTL}{Common-Task Learning}
\newacronym{ltl}{LTL}{Learning to Learn}
\newacronym{lupi}{LUPI}{Learning Using Privileged Information}

\newacronym{kkt}{KKT}{Karush-Kuhn-Tucker}
\newacronym{svd}{SVD}{Singular Value Decomposition}


\newacronym{ml}{ML}{Machine Learning}
\newacronym{mt}{MT}{Multi-Task}
\newacronym{gp}{GP}{Gaussian Process}
\newacronym{gps}{GPs}{Gaussian Processes}
\newacronym{svm}{SVM}{Support Vector Machine}
\newacronym{svr}{SVR}{Support Vector Regressor}
\newacronym{svc}{SVC}{Support Vector Classifier}

\newacronym{smo}{SMO}{Sequential Minimal Optimization}
\newacronym{svms}{SVMs}{Support Vector Machines}
\newacronym{nn}{NN}{Neural Network}
\newacronym{nns}{NNs}{Neural Networks}
\newacronym{dnn}{DNN}{Neural Network}
\newacronym{dnns}{DNNs}{Deep Neural Networks}
\newacronym{cv}{CV}{Cross Validation}
\newacronym{vc}{VC}{Vapnik-Chervonenkis}


\newacronym{rkhs}{RKHS}{Reproducing Kernel Hilbert Space}
\newacronym{rkhss}{RKHSs}{Reproducing Kernel Hilbert Spaces}

\newacronym{mae}{MAE}{Mean Absolute Error}
\newacronym{mse}{MSE}{Mean Squared Error}



\newacronym{gs}{GS}{Grid Search}
\newacronym{nwp}{NWP}{Numerical Weather Prediction}
\newacronym{ecmwf}{ECMWF}{European Centre for Medium-Range Weather Forecasts}
\newacronym{pv}{PV}{Photovoltaic}


% My definitions

  % Math Operators
\DeclareMathOperator*{\argmin}{arg\min}
\DeclareMathOperator*{\argmax}{arg\max}
\DeclareMathOperator*{\arginf}{arg\inf}
\DeclareMathOperator*{\argsup}{arg\sup}
\DeclareMathOperator{\Tr}{tr}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Diag}{diag}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\expect}{E}
\DeclareMathOperator{\VCdim}{VCdim}

\DeclareMathOperator{\mn}{\mathcal{M_N}}
\DeclareMathOperator{\tn}{\mathcal{T_N}}



\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Vector}{vec}

\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\equivclass{\lbrack}{\rbrack}



\newcommand{\comm}[1]{{\color{red} #1}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\mymax}[1]{\max\left(#1\right)}
\newcommand{\mymin}[1]{\min\left(#1\right)}
\newcommand{\pospart}[1]{\left[#1\right]_{+}}
\newcommand{\epsins}[1]{\left[#1\right]_{\epsilon}}

\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\cardinal}[1]{\left|#1\right|}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\hypf}{h}
\newcommand{\hypfun}[1]{\hypf\left(#1\right)}
\newcommand{\hyp}[2]{\hypf\left(#1, #2\right)}
\newcommand{\fun}[1]{f\left(#1\right)}
\newcommand{\den}[1]{p\left( #1 \right)}
\newcommand{\opt}[1]{{#1}^*}
\newcommand{\cond}[2]{P\left( #1 \;\middle\vert\; #2 \right)}
\newcommand{\normal}[1]{N \left(#1\right)}
\newcommand{\multinormal}[1]{\mn \left(#1\right)}
\newcommand{\tensornormal}[1]{\tn \left(#1\right)}
\newcommand{\tendsto}[2]{\xrightarrow[#1]{} #2}
\newcommand{\diffp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\optim}[1]{{#1}^*}
\newcommand{\priv}[1]{{#1}^{\diamond}}

\newcommand{\adj}[1]{{#1}^{\#}}


\newcommand{\crefrangeconjunction}{--}
\newcommand{\svset}{\mathcal{I}}




\newcommand{\upper}[1]{\expandafter\MakeUppercase\expandafter{#1}}
\newcommand{\mymat}[1]{\upper{#1}}
\newcommand{\myvec}[1]{\bm{#1}}
\newcommand{\fv}[1]{\myvec{#1}}
\newcommand{\fm}[1]{\mymat{#1}}
\newcommand{\frow}[1]{\fv{#1}}
\newcommand{\vect}[1]{\bm{\text{vect}}\left(#1\right)}

\newcommand{\trace}[1]{\Tr{\left(#1\right)}}
\newcommand{\dotp}[2]{\bm{\left\langle} #1, #2 \bm{\right\rangle}}
\newcommand{\ydotp}[2]{\left[ #1, #2 \right]_\mathcal{Y}}

\newcommand{\ind}{\bm{1}}

\newcommand{\domain}{\mathcal{D}}
\newcommand{\cvxset}{X}
\newcommand{\vecspace}{\reals^\dimx}

\newcommand{\nsamples}{n}
\newcommand{\dimx}{d}
\newcommand{\vcdim}[1]{\VCdim\left(#1\right)}
\newcommand{\vc}{d}
\newcommand{\hplane}{w}
\newcommand{\bias}{b}

\newcommand{\ntasks}{T}
\newcommand{\nclusters}{C}

\newcommand{\npertask}{m}
\newcommand{\idotsn}{i=1, \ldots, \nsamples}
\newcommand{\hypspace}{\mathcal{H}}
\newcommand{\hypspacef}{\mathbb{H}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\param}{\alpha}
\newcommand{\paramspace}{A}
\newcommand{\fparam}{\beta}
\newcommand{\fparamspace}{B}
\newcommand{\hilbertspace}{\mathcal{H}}
\newcommand{\lagr}{\mathcal{L}}
\newcommand{\grad}{\nabla}



\newcommand{\lossf}{\ell}
\newcommand{\loss}[2]{\lossf\left( #1, #2\right)}

\newcommand{\distf}{P}
\newcommand{\sample}{D}
\newcommand{\samplen}{D_{\nsamples}}

\newcommand{\err}{e}
\newcommand{\risk}{R}
\newcommand{\emprisk}{\hat{\risk}_{\sample}}
\newcommand{\empriskn}{\hat{\risk}_{\samplen}}

\newcommand{\regrisk}{\hat{\risk}_{\sample, \lambda}}
\newcommand{\exprisk}{\risk_\distf}
\newcommand{\hypemp}{\opt{\hypf}_\nsamples}
\newcommand{\hypexp}{\opt{\hypf}_\distf}

% bias learning

\newcommand{\bprobspace}{\mathcal{P}}
\newcommand{\bdistf}{Q}
\newcommand{\bprobseq}{\bm{P}}
\newcommand{\bsample}{\bm{\sample}}
\newcommand{\bemprisk}{\hat{\risk}_{\bsample}}
\newcommand{\bexprisk}{\risk_\bdistf}
\newcommand{\bsetsample}{\hypspacef^{\ntasks}}
\newcommand{\bsetdist}{\hypspacef^{*}}
\newcommand{\capf}{C}
\newcommand{\capacity}[2]{\capf\left(#1, #2\right)}
\newcommand{\dist}[1]{d_{#1}}

\newcommand{\bigO}[1]{O\left( #1 \right)}

\newcommand{\Xspace}{\mathcal{X}}
\newcommand{\Tspace}{\mathcal{T}}
\newcommand{\Yspace}{\mathcal{Y}}
\newcommand{\Fspace}{\mathcal{V}}
\newcommand{\Vspace}{\mathcal{V}}


\newcommand{\toprob}{\xrightarrow{P}}

\newcommand{\powerset}[1]{\wp\left( #1 \right)}

\newcommand{\frelf}{f}
\newcommand{\frel}[1]{\frelf\left[ #1 \right]}
\newcommand{\frelset}{\mathcal{F}}

\newcommand{\rkhs}{\mathcal{H}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}


% Tables
\newcommand{\fcode}[1]{\texttt{#1}}
\newcommand{\fheadmulti}[2]{\multicolumn{#1}{c}{\boldmath\textbf{#2}}}
\newcommand{\fhead}[1]{\fheadmulti{1}{#1}}
\newcommand{\fmax}[1]{{\num[detect-weight=true,math-rm=\mathbf]{#1}}}
\newcommand{\fmaxn}[1]{\textbf{#1}}
\newcommand{\fdata}[1]{\textsf{#1}}
\newcommand{\fmod}[1]{\textsf{#1}}
\newcommand{\ftt}[1]{\text{#1}}

% energies
\newcommand{\fmodt}[2]{\fmod{(#1)}\_\fmod{#2}}

% Units
\DeclareSIUnit\utc{UTC}
\newcommand{\utc}[1]{\SI[parse-numbers=false]{#1}{\utc}}
\newcommand{\mwh}[1]{\SI{#1}{\mega{\watt\hour}}}
\newcommand{\mw}[1]{\SI{#1}{\mega\watt}}
\newcommand{\mwhu}{\si{\mega{\watt\hour}}}
\newcommand{\km}[1]{\SI{#1}{\kilo\metre}}

% Macros
\newcommand{\diagrams}{../Diagrams}



% Theorems
\newtheorem{prop}{Proposition}





%% ----------------------------------------------------------------
\setlength{\headheight}{26pt}
\begin{document}


\frontmatter	  % Begin Roman style (i, ii, iii, iv...) page numbering

% Set up the Title Page
\title  {Advanced Kernel Methods for Multi-Task Learning}
%\title  {\uppercase{Prediction Based on Averages over Automatically Induced Learners: Ensemble Methods and Bayesian Techniques}}
\authors  {\texorpdfstring
            {{Carlos Ruiz Pastor}}
            {Carlos Ruiz Pastor}
          }
\addresses  {\groupname\\\deptname\\\univname}  
% Do not change this here, instead these must be set in the "Thesis.cls" file, please look through it instead
\date       {\today}
\subject    {}
\keywords   {}

\maketitle
%% ----------------------------------------------------------------

%\setstretch{1.3}  % It is better to have smaller font and larger line spacing than the other way round

% Define the page headers using the FancyHdr package and set up for one-sided printing
\fancyhead{}  % Clears all page headers and footers
\rhead{\thepage}  % Sets the right side header to show the page number
\lhead{}  % Clears the left side page header

\pagestyle{fancy}  % Finally, use the "fancy" page style to implement the FancyHdr headers

%% ----------------------------------------------------------------
% Declaration Page required for the Thesis, your institution may give you a different text to place here
%\Declaration{

%\addtocontents{toc}{\vspace{1em}}  % Add a gap in the Contents, for aesthetics
%\addtocontents{toc}{}  % Add a gap in the Contents, for aesthetics

%I, AUTHOR NAME, declare that this thesis titled, `THESIS TITLE' and the work presented in it are my own. I confirm that:

%\begin{itemize} 
%\item[\tiny{$\blacksquare$}] This work was done wholly or mainly while in candidature for a research degree at this University.
 
%\item[\tiny{$\blacksquare$}] Where any part of this thesis has previously been submitted for a degree or any other qualification at this University or any other institution, this has been clearly stated.
 
%\item[\tiny{$\blacksquare$}] Where I have consulted the published work of others, this is always clearly attributed.
 
%\item[\tiny{$\blacksquare$}] Where I have quoted from the work of others, the source is always given. With the exception of such quotations, this thesis is entirely my own work.
 
%\item[\tiny{$\blacksquare$}] I have acknowledged all main sources of help.
 
%\item[\tiny{$\blacksquare$}] Where the thesis is based on work done by myself jointly with others, I have made clear exactly what was done by others and what I have contributed myself.
%\\
%\end{itemize}
 
 
%Signed:\\
%\rule[1em]{25em}{0.5pt}  % This prints a line for the signature
 
%Date:\\
%\rule[1em]{25em}{0.5pt}  % This prints a line to write the date
%}
%\clearpage  % Declaration ended, now start a new page

%% ----------------------------------------------------------------
% The "Funny Quote Page"
\pagestyle{empty}  % No headers or footers for the following pages

\null\vfill
% Now comes the "Funny Quote", written in italics
\textit{What is the essence of life? To serve others and to do good.} 
%\textit{``Write a quote here.''}

\begin{flushright}
Aristotle.
\end{flushright}

\vfill\vfill\vfill\vfill\vfill\vfill\null
\clearpage  % Funny Quote page ended, start a new page
%% ----------------------------------------------------------------

% The Abstract Page
\addtotoc{Abstract}  % Add the "Abstract" page entry to the Contents
\abstract{
%\addtocontents{toc}{\vspace{1em}}  % Add a gap in the Contents, for aesthetics
\addtocontents{toc}{}  % Add a gap in the Contents, for aesthetics

\small{
% ml es importante 
% mtl intenta juntar varias tareas como los humanos
Machine Learning (ML), whose goal is to automatize the process of learning, has a great influence in our current society.
The ML algorithms try to infer general patterns from data, which can then be applicable to unseen data. 
%Gathering data, with the ML algorithms it is possible to inductively infer general patterns. 
These algorithms, such as the Support Vector Machine (SVM) or Neural Network (NN), are present in many daily situations and have strongly impacted multiple areas such as engineering or advertising, among many others.
Multi-Task Learning (MTL) is a field of ML that considers learning different tasks jointly to improve the learning process. This is the natural way of learning for humans: we do not learn each task in an isolated manner, but there exist related tasks are better learned together. The goal in MTL is to develop strategies to mimic this behaviour, where learning various tasks jointly offers an advantage.


This thesis begins with a presentation of some basic concepts and definitions that we will use in the rest of this work.
% first, a brief description of some of the basic concepts and definitions that we will use

After that, a theoretical motivation for MTL is given, and some of the most relevant works in this area are reviewed. A taxonomy for these methods is proposed, where three categories are considered: feature-based, parameter-based and combination-based strategies. Different ML algorithms, depending on their characteristics, are more suitable for one of these strategies than for others. The feature-based strategies are more natural for NNs, while the kernel methods, such as the SVM, present a more rigid framework, and the combination-based strategies are better suited for them.
% After that, the motivation of mtl is given
% and a thorough review of related work is given with a categorization

Considering the combination-based strategies, a new convex formulation is proposed: in each task we consider a convex combination of a common and task-specific part as the model. This formulation offers some nice properties, such as better interpretability of the models, or the possibility to dismiss the common or task-specific parts with a particular choice of the hyperparameters.
This approach is applied to kernel methods, in particular, the L1, L2 and LS-SVM with this convex MTL formulation are proposed, and the solutions for the corresponding training problems can be obtained with standard SVM solvers.
One natural alternative, which considers the convex combination of pre-trained common and task-specific models, is also described.
In multiple experiments, it is observed that the kernel methods with a convex MTL formulation obtain better results than those considering just a common model, task-specific ones or the convex combination of pre-trained models.
As a real world application, the prediction of solar and wind energy is also presented using these models, where our proposal outperforms or ties with the described competition.

%
Applying this formulation, a MTL proposal for NNs is made, where a convex combination of a common and a task-specific networks is used. These models can also be trained with standard optimization techniques for NNs.
In experiments with four image datasets, it is shown that the results of this proposal are better than using standard approaches, such as sharing the weights in the hidden layers and task-specific output neurons.

% Considering an alternative combination-based approach, a new convex form ... is proposed
% It is applied to kernel methods
% and to NN
% As a real work application, it is shown (energy)...

% Another strategy, based on a gl approach is proposed.
Another approach is proposed with a Graph Laplacian (GL) regularization, where the tasks are interpreted as nodes in a graph, and the pairwise distances between the task models are penalized. In this approach, the adjacency matrix of the graph defines the weights for the distances. A new formulation, based on tensor product of Reproducing Kernel Hilbert Spaces, to apply this regularization in kernel spaces is developed, and the GL regularization is applied to the L1, L2 and LS-SVM. 
It is exemplified with multiple experiments that this approach can obtain competitive results. 
%
Moreover, an algorithm to automatically learn the adjacency matrix from the data is proposed. Examples of the advantages of using this algorithm are given using experiments with synthetic and real data.

The thesis ends with some general conclusions and presents lines of research for future work. 
%
}

}

\clearpage  % Abstract ended, start a new page
%% ----------------------------------------------------------------

% The Resumen page, for thanking everyone
\resumen{
%\addtocontents{toc}{\vspace{1em}}  % Add a gap in the Contents, for aesthetics
\addtocontents{toc}{}  % Add a gap in the Contents, for aesthetics

\small{
El aprendizaje automático (AA), cuyo objetivo es automatizar el proceso de aprendizaje, tiene una gran influencia en la sociedad actual. Los algoritmos de AA tratan de inferir patrones generales a partir de datos, los cuales pueden después ser aplicados a nuevos datos. Estos algoritmos, como las Máquinas de Vectores Soporte (MVS) o las Redes Neuronales (RNs), están presentes en muchas situaciones cotidianas y han causado un fuerte impacto en múltiples áreas como la ingeniería o la publicidad, entre otras muchas. El Aprendizaje Multitarea (AMT) es un campo del AA que considera el aprendizaje de diferentes tareas de forma conjunta para mejorar el proceso de aprendizaje. Esta es la manera natural de aprender para los humanos: no aprendemos las tareas de manera iaslada, sino que existen tareas que están relacionadas y se aprenden mejor juntas. El objetivo del AMT es desarrollar estrategies que imiten este comportamiento, donde aprender diversas tareas conjuntamente ofrece una ventaja.

Esta tesis comienza presentando algunos conceptos básicos y definiciones que usaremos en el resto de este trabajo.

Después de esto, se ofrece una motivación teórica para el AMT, y se revisan algunos de los trabajos más relevantes en este área. Se propone una taxonomía para estos métodos, donde se consideran tres categorías: estrategias basadas en características, basadas en parámetros y basadas en combinación. Distintos algoritmos de AA, dependiendo de sus características, son más aptos para una u otra de estas estrategias. Las estrategias basadas en características son naturales para las RN, mientras que los métodos de kernel, como las MVS, presentan un esquema más rígido, y las basadas en combinaciones se adaptan mejor a estos.

Dentro de las estrategias basadas en combinaciones, se presenta una nueva formulación convexa: se considera la combinación convexa de una parte común y otra específica como el modelo de cada tarea. Esta formulación ofrece algunas propiedades buenas, como una mejor interpretabilidad o la posibilidad de, con una selección particular de hiperparámetros, eliminar la parte común o la específica del modelo. Este enfoque se aplica a métodos de kernel, en particular se proponen la L1, L2 y LS-MVS convexas para AMT, y las soluciones correspondientes a los problemas de entrenamiento se pueden obtener con técnicas estándares de MVS. También se describe una alternativa natural, que considera la combinación convexa de un modelo común y otros específicos que ya han sido preentrenados. 
Se observa en múltiples experimentos que los métodos de kernel con esta formulación convexa obtienen mejores resultados que considerando únicamente un modelo común, uno específico, o la combinación de estos modelos preentrenados. También se presenta la predicción de energía eólica y solar como una aplicación real de estos modelos, donde nuestra propuesta iguala o supera a la competencia descrita.

Aplicando esta formulación se hace una propuesta de AMT para RNs, donde se considera una combinación convexa de redes comunes y específicas. Estos modelos se puede entrenar también con técnicas estándares para RNs. Se muestra con cuatro conjuntos de imágenes que los resultados de nuestra propuesta son mejores que aquellos obtenidos con enfoques más tradicionales, como compartir las capas ocultas y definir neuronas de salida específicas para cada tarea.

Se propone otro enfoque con una regularización basada en el laplaciano de un grafo, en el que las tareas se interpretan como nodos en un grafo, y las distancias entre los modelos de cada par de tareas es penalizada. En este enfoque, la matriz de adyacencia del grafo define el peso de cada distancia. Se propone una nueva formulación, basada en el producto tensorial de espacios de kernel reproductor, para usar este enfoque, y la regularización laplaciana se aplica a la L1, L2 y LS-MVS. Se ejemplifica con múltiples experimentos que este enfoque puede obtener resultados competitivos. Además se propone un algoritmo para para aprender la matriz de adyacencia de forma automática. Se proporcionan ejemplos de las ventajas de este algoritmo en varios problemas sintéticos y reales.

Esta tesis termina con algunas conclusiones generales y persenta líneas de investigación para un trabajo futuro.
}
}
\clearpage  % End of the resumen

%% ----------------------------------------------------------------


%
%\setstretch{1.3}  % Reset the line-spacing to 1.3 for body text (if it has changed)

% The Acknowledgements page, for thanking everyone
\acknowledgements{
%\addtocontents{toc}{\vspace{1em}}  % Add a gap in the Contents, for aesthetics
\addtocontents{toc}{}  % Add a gap in the Contents, for aesthetics
.
}
\clearpage  % End of the Acknowledgements
%% ----------------------------------------------------------------

\pagestyle{fancy}  %The page style headers have been "empty" all this time, now use the "fancy" headers as defined before to bring them back


%% ----------------------------------------------------------------
\lhead{\emph{Contents}}  % Set the left side page header to "Contents"
\tableofcontents  % Write out the Table of Contents

%% ----------------------------------------------------------------
\lhead{\emph{List of Figures}}  % Set the left side page header to "List if Figures"
\listoffigures  % Write out the List of Figures

%% ----------------------------------------------------------------
\lhead{\emph{List of Tables}}  % Set the left side page header to "List of Tables"
\listoftables  % Write out the List of Tables

%% ----------------------------------------------------------------
%\setstretch{1.5}  % Set the line spacing to 1.5, this makes the following tables easier to read
\clearpage  % Start a new page
%\lhead{\emph{Acronyms}}  % Set the left side page header to "Abbreviations"
%\listofsymbols{\printglossary[type=\acronymtype]}
\printglossary[type=\acronymtype,nonumberlist,title=Abbreviations]

\clearpage
%\setstretch{1.3}  % Return the line spacing back to 1.3

\pagestyle{empty}  % Page style needs to be empty for this page
\dedicatory{To my family}


%\addtocontents{toc}{\vspace{2em}}  % Add a gap in the Contents, for aesthetics
\addtocontents{toc}{}  % Add a gap in the Contents, for aesthetics

\setstretch{1}  % Return the line spacing back to 1

%% ----------------------------------------------------------------
\mainmatter	  % Begin normal, numeric (1,2,3...) page numbering
\pagestyle{fancy}  % Return the page headers back to the "fancy" style

% Include the chapters of the thesis, as separate files
% Just uncomment the lines as you write the chapters

\include{./Chapters/Intro} % Introduction

\include{./Chapters/Chapter2} % Foundations and Concepts

\include{./Chapters/Chapter3} % Multi-Task Learning Overview

\include{./Chapters/Chapter4} % Convex Multi-Task SVM

\include{./Chapters/Chapter5} % Adaptive Graph Laplacian Multi-Task Learning SVM

\include{./Chapters/Chapter6} % Experiments

\include{./Chapters/Conclusions} % Conclusions and Future Work

%% ----------------------------------------------------------------
% Now begin the Appendices, including them as separate files

\addtocontents{toc}{\vspace{2em}} % Add a gap in the Contents, for aesthetics

\appendix % Cue to tell LaTeX that the following 'chapters' are Appendices

\input{./Appendices/AppendixA} % List os Publications

\input{./Appendices/AppendixB} % Appendix for Chapter 4

\input{./Appendices/AppendixC} % Multi-Task Datasets

\addtocontents{toc}{\vspace{2em}}  % Add a gap in the Contents, for aesthetics
\backmatter

%% ----------------------------------------------------------------
\label{Bibliography}
\lhead{\emph{Bibliography}}  % Change the left side page header to "Bibliography"
%\bibliographystyle{unsrtnat}  % Use the "unsrtnat" BibTeX style for formatting the Bibliography
\bibliographystyle{apalike}  % Use the "unsrtnat" BibTeX style for formatting the Bibliography
\bibliography{Bibliography}  % The references (bibliography) information are stored in the file named "Bibliography.bib"

\end{document}  % The End
%% ----------------------------------------------------------------
